"""
Vercel serverless function handler for the Flask app.
This is a standalone version with admin access functionality.
"""
import os
import re
import json
import random
import logging
import traceback
import uuid
import time
import stripe
import sys
import traceback
from datetime import datetime, timedelta, date
from functools import wraps
from dotenv import load_dotenv
from zoneinfo import ZoneInfo
from sqlalchemy import create_engine, Column, Integer, String, Float, ForeignKey, DateTime, Boolean, func, text, and_, or_, cast, Date
from sqlalchemy.pool import NullPool
from flask import Flask, render_template_string, render_template, redirect, url_for, request, session, flash, jsonify, send_from_directory
from flask_login import LoginManager, UserMixin, login_user, logout_user, login_required, current_user
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from flask_session import Session
from werkzeug.security import generate_password_hash, check_password_hash
from werkzeug.middleware.proxy_fix import ProxyFix
from authlib.integrations.flask_client import OAuth
import secrets
import string
import requests

# Load environment variables
load_dotenv()

# Configure structured logging
log_level = os.environ.get('LOG_LEVEL', 'INFO').upper()
log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
logging.basicConfig(
    level=getattr(logging, log_level),
    format=log_format
)

# Create logger
logger = logging.getLogger('apestogether')

# Add handler for Vercel environment
if os.environ.get('VERCEL_ENV') == 'production':
    # In production, log to stderr which Vercel captures
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(handler)
    logger.info("Configured logging for Vercel production environment")
else:
    # In development, log to console
    logger.info("Configured logging for development environment")

# Admin credentials from environment variables
ADMIN_EMAIL = os.environ.get('ADMIN_EMAIL', 'admin@apestogether.ai')
ADMIN_USERNAME = os.environ.get('ADMIN_USERNAME', 'admin')

# =============================================================================
# TIMEZONE CONFIGURATION (Eastern Time for US Stock Market)
# =============================================================================
# Vercel runs in UTC, but we need Eastern Time for market operations
MARKET_TZ = ZoneInfo('America/New_York')

def get_market_time():
    """Get current time in Eastern Time (handles DST automatically)"""
    return datetime.now(MARKET_TZ)

def get_market_date():
    """Get current date in Eastern Time"""
    return get_market_time().date()

def generate_portfolio_slug():
    """Generate a URL-safe unique slug for portfolio sharing (11 chars, like nanoid)"""
    alphabet = string.ascii_letters + string.digits  # a-z, A-Z, 0-9
    return ''.join(secrets.choice(alphabet) for _ in range(11))

def is_market_hours(dt=None):
    """
    Check if current time (or provided datetime) is during market hours
    Market hours: Monday-Friday, 9:30 AM - 4:00 PM ET (excluding holidays)
    
    Args:
        dt: datetime object (with timezone). If None, uses current ET time.
    
    Returns:
        bool: True if during market hours
    """
    if dt is None:
        dt = get_market_time()
    
    # Ensure datetime is in ET
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=MARKET_TZ)
    elif dt.tzinfo != MARKET_TZ:
        dt = dt.astimezone(MARKET_TZ)
    
    # Check if weekend
    if dt.weekday() >= 5:  # Saturday=5, Sunday=6
        return False
    
    # Check if within market hours (9:30 AM - 4:00 PM ET)
    market_open = dt.replace(hour=9, minute=30, second=0, microsecond=0)
    market_close = dt.replace(hour=16, minute=0, second=0, microsecond=0)
    
    return market_open <= dt <= market_close

def is_market_holiday(check_date=None):
    """
    Check if a given date is a US market holiday (NYSE/NASDAQ closed)
    
    Args:
        check_date: date object to check. If None, uses current ET date.
    
    Returns:
        bool: True if market is closed for a holiday
    """
    if check_date is None:
        check_date = get_market_date()
    
    year = check_date.year
    month = check_date.month
    day = check_date.day
    
    # Fixed holidays
    holidays = [
        date(year, 1, 1),   # New Year's Day
        date(year, 7, 4),   # Independence Day
        date(year, 12, 25), # Christmas
    ]
    
    # NOTE: MLK Day is NOT a market holiday - NYSE/NASDAQ are open
    
    # Presidents Day - 3rd Monday in February
    feb_1 = date(year, 2, 1)
    presidents_day = feb_1 + timedelta(days=(7 - feb_1.weekday() + 14))  # 3rd Monday
    holidays.append(presidents_day)
    
    # Good Friday - Friday before Easter (complex calculation)
    # Simplified: Use a lookup table for common years
    good_fridays = {
        2024: date(2024, 3, 29),
        2025: date(2025, 4, 18),
        2026: date(2026, 4, 3),
        2027: date(2027, 3, 26),
        2028: date(2028, 4, 14),
    }
    if year in good_fridays:
        holidays.append(good_fridays[year])
    
    # Memorial Day - Last Monday in May
    may_31 = date(year, 5, 31)
    memorial_day = may_31 - timedelta(days=(may_31.weekday() + 7) % 7)
    holidays.append(memorial_day)
    
    # Juneteenth - June 19 (observed if weekend)
    juneteenth = date(year, 6, 19)
    if juneteenth.weekday() == 5:  # Saturday
        juneteenth = date(year, 6, 18)
    elif juneteenth.weekday() == 6:  # Sunday
        juneteenth = date(year, 6, 20)
    holidays.append(juneteenth)
    
    # Labor Day - 1st Monday in September
    sep_1 = date(year, 9, 1)
    labor_day = sep_1 + timedelta(days=(7 - sep_1.weekday()) % 7)
    holidays.append(labor_day)
    
    # Thanksgiving - 4th Thursday in November
    nov_1 = date(year, 11, 1)
    thanksgiving = nov_1 + timedelta(days=(3 - nov_1.weekday() + 21) % 7 + 21)
    holidays.append(thanksgiving)
    
    # NOTE: Columbus Day is NOT a market holiday - NYSE/NASDAQ are open
    
    # Observed holidays (if holiday falls on weekend, observe on Friday/Monday)
    for holiday in list(holidays):
        if holiday.weekday() == 5:  # Saturday -> observe Friday
            holidays.append(holiday - timedelta(days=1))
        elif holiday.weekday() == 6:  # Sunday -> observe Monday
            holidays.append(holiday + timedelta(days=1))
    
    return check_date in holidays

# =============================================================================

# Initialize Flask app
# Use absolute paths for templates and static files in production
import shutil

# Helper function to add common template variables
def render_template_with_defaults(*args, **kwargs):
    """Wrapper for render_template that adds common variables"""
    # Add the current date/time for use in templates (e.g., copyright year)
    if 'now' not in kwargs:
        kwargs['now'] = datetime.now()
    return render_template(*args, **kwargs)

# Create a Flask app with the appropriate template and static folders
if os.environ.get('VERCEL_ENV') == 'production':
    # For Vercel production environment
    app = Flask(__name__)
    
    # Set absolute paths for templates and static files
    app.template_folder = '/var/task/templates'
    app.static_folder = '/var/task/static'
    
    # Ensure the directories exist
    os.makedirs(app.template_folder, exist_ok=True)
    os.makedirs(app.static_folder, exist_ok=True)
    
    # Copy templates if needed
    source_templates = '/var/task/templates'
    if not os.path.exists(os.path.join(source_templates, 'index.html')):
        try:
            # Try different source paths
            potential_sources = [
                '/var/task/api/../templates',
                '/var/task/templates',
                os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'templates')
            ]
            
            for src_path in potential_sources:
                if os.path.exists(src_path) and os.path.isdir(src_path):
                    print(f"Found templates at: {src_path}")
                    for item in os.listdir(src_path):
                        src = os.path.join(src_path, item)
                        dst = os.path.join(app.template_folder, item)
                        if os.path.isdir(src):
                            shutil.copytree(src, dst, dirs_exist_ok=True)
                        else:
                            shutil.copy2(src, dst)
                    print(f"Templates copied from {src_path} to {app.template_folder}")
                    break
        except Exception as e:
            print(f"Error copying templates: {str(e)}")
    
    # Copy static files if needed
    if not os.listdir(app.static_folder):
        try:
            # Try different source paths
            potential_sources = [
                '/var/task/api/../static',
                '/var/task/static',
                os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'static')
            ]
            
            for src_path in potential_sources:
                if os.path.exists(src_path) and os.path.isdir(src_path):
                    print(f"Found static files at: {src_path}")
                    for item in os.listdir(src_path):
                        src = os.path.join(src_path, item)
                        dst = os.path.join(app.static_folder, item)
                        if os.path.isdir(src):
                            shutil.copytree(src, dst, dirs_exist_ok=True)
                        else:
                            shutil.copy2(src, dst)
                    print(f"Static files copied from {src_path} to {app.static_folder}")
                    break
        except Exception as e:
            print(f"Error copying static files: {str(e)}")
    
    print(f"Final template folder: {app.template_folder}")
    print(f"Final static folder: {app.static_folder}")
else:
    # For local development
    app = Flask(__name__, template_folder='../templates', static_folder='../static')

app.wsgi_app = ProxyFix(app.wsgi_app, x_for=1, x_proto=1, x_host=1, x_port=1)

# Enable jinja2 template features in render_template_string
app.jinja_env.globals.update({
    'len': len,
    'format': format
})

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Get Vercel environment information
VERCEL_ENV = os.environ.get('VERCEL_ENV', 'development')
VERCEL_REGION = os.environ.get('VERCEL_REGION', 'local')
VERCEL_URL = os.environ.get('VERCEL_URL', 'localhost')

# Login required decorator
# Use Flask-Login's login_required decorator instead of our custom one
# This is kept for backward compatibility with existing code
def custom_login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not current_user.is_authenticated:
            flash('Please log in to access this page', 'danger')
            return redirect(url_for('login', next=request.url))
        return f(*args, **kwargs)
    return decorated_function

# For backward compatibility, keep the original name
login_required = custom_login_required

# Get environment variables with fallbacks
# Check for DATABASE_URL first, then fall back to POSTGRES_PRISMA_URL if available
DATABASE_URL = os.environ.get('DATABASE_URL') or os.environ.get('POSTGRES_PRISMA_URL')
SECRET_KEY = os.environ.get('SECRET_KEY', 'dev-key-for-testing')
VERCEL_ENV = os.environ.get('VERCEL_ENV')

# Log environment information
logger.info(f"Starting app with VERCEL_ENV: {VERCEL_ENV}")
logger.info(f"DATABASE_URL present: {'Yes' if DATABASE_URL else 'No'}")
logger.info(f"POSTGRES_PRISMA_URL present: {'Yes' if os.environ.get('POSTGRES_PRISMA_URL') else 'No'}")
logger.info(f"SECRET_KEY present: {'Yes' if SECRET_KEY else 'No'}")

# Configure database with error handling
try:
    # Configure database
    DATABASE_URL = os.environ.get('DATABASE_URL', os.environ.get('POSTGRES_PRISMA_URL'))
    if not DATABASE_URL:
        logger.error("No database URL provided - using SQLite fallback")
        DATABASE_URL = 'sqlite:///portfolio.db'
    elif DATABASE_URL.startswith('postgres://'):
        # Heroku-style postgres:// URL needs to be converted for SQLAlchemy
        DATABASE_URL = DATABASE_URL.replace('postgres://', 'postgresql://', 1)
    
    logger.info(f"Using database type: {DATABASE_URL.split('://')[0]}")

    # Configure Flask app
    app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-key-for-testing')
    app.config['SQLALCHEMY_DATABASE_URI'] = DATABASE_URL
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
    # Serverless-friendly SQLAlchemy engine options to avoid pool exhaustion
    app.config['SQLALCHEMY_ENGINE_OPTIONS'] = {
        'poolclass': NullPool,       # Disable pooling; connections are short-lived on serverless
        'pool_pre_ping': True,       # Validate connections before use
    }
    
    # Configure Flask-Session with SQLAlchemy backend for serverless environment
    app.config['SESSION_TYPE'] = 'sqlalchemy'
    app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(days=7)
    app.config['SESSION_COOKIE_SECURE'] = True
    app.config['SESSION_COOKIE_HTTPONLY'] = True
    app.config['SESSION_COOKIE_SAMESITE'] = 'Lax'
    
    # Debug logging for session configuration
    logger.info(f"Session configuration: TYPE={app.config.get('SESSION_TYPE')}, LIFETIME={app.config.get('PERMANENT_SESSION_LIFETIME')}")
    logger.info(f"Database URL for sessions: [REDACTED]")

    # Initialize Flask-Login
    login_manager = LoginManager()
    login_manager.init_app(app)
    login_manager.login_view = 'login'

    # Add a before_request handler to ensure session and current_user are properly initialized
    @app.before_request
    def handle_before_request():
        # Skip session processing for static files and favicon
        if request.path.startswith('/static/') or request.path == '/favicon.png':
            return  # Skip session processing for static files
        
        try:
            session.permanent = True
            
            if '_user_id' in session and not hasattr(current_user, 'is_authenticated'):
                user_id = session.get('_user_id')
                if user_id:
                    user = load_user(user_id)
                    if user:
                        login_user(user)
                        logger.info(f"User {user_id} loaded from session")
            
            if current_user.is_authenticated:
                session['user_id'] = current_user.id
                session['email'] = getattr(current_user, 'email', None)
                session['username'] = getattr(current_user, 'username', None)
                session.modified = True  # Refresh the session to keep it alive
        except Exception as e:
            # Log but don't fail the request
            logger.error(f"Error in before_request handler: {str(e)}")
            logger.error(traceback.format_exc())
        
        
        # Add request logging for debugging (moved from separate handler)
        if request.path.startswith('/api/portfolio/'):
            logger.info(f"REQUEST: {request.method} {request.path}")
            logger.info(f"User-Agent: {request.headers.get('User-Agent', 'Unknown')}")

    @login_manager.user_loader
    def load_user(user_id):
        try:
            return User.query.get(int(user_id))
        except Exception as e:
            logger.error(f"Error loading user: {str(e)}")
            return None

    # Stripe configuration
    app.config['STRIPE_PUBLIC_KEY'] = os.environ.get('STRIPE_PUBLIC_KEY')
    app.config['STRIPE_SECRET_KEY'] = os.environ.get('STRIPE_SECRET_KEY')
    app.config['STRIPE_WEBHOOK_SECRET'] = os.environ.get('STRIPE_WEBHOOK_SECRET')
    
    # Initialize Stripe
    stripe.api_key = app.config['STRIPE_SECRET_KEY']

    # Initialize OAuth
    oauth = OAuth(app)

    # Configure OAuth providers
    google = oauth.register(
        name='google',
        client_id=os.environ.get('GOOGLE_CLIENT_ID', 'google-client-id'),
        client_secret=os.environ.get('GOOGLE_CLIENT_SECRET', 'google-client-secret'),
        server_metadata_url='https://accounts.google.com/.well-known/openid-configuration',
        client_kwargs={
            'scope': 'openid email profile'
        }
    )

    apple = oauth.register(
        name='apple',
        client_id=os.environ.get('APPLE_CLIENT_ID', 'apple-client-id'),
        client_secret=os.environ.get('APPLE_CLIENT_SECRET', 'apple-client-secret'),
        access_token_url='https://appleid.apple.com/auth/token',
        access_token_params=None,
        authorize_url='https://appleid.apple.com/auth/authorize',
        authorize_params=None,
        api_base_url='https://appleid.apple.com/',
        client_kwargs={'scope': 'name email'},
    )

    # Initialize SQLAlchemy
    db = SQLAlchemy(app)
    migrate = Migrate(app, db)
    
    # Initialize Flask-Session with appropriate backend based on environment
    try:
        # For Vercel serverless environment, use filesystem sessions by default
        # This is more reliable in a serverless environment where database connections may be limited
        if os.environ.get('VERCEL_ENV') == 'production':
            logger.info("Vercel production environment detected, using filesystem session storage")
            app.config['SESSION_TYPE'] = 'filesystem'
            # Create a sessions directory if it doesn't exist
            os.makedirs('/tmp/flask_session', exist_ok=True)
            app.config['SESSION_FILE_DIR'] = '/tmp/flask_session'
            Session(app)
            logger.info("Flask-Session initialized with filesystem backend")
        else:
            # For non-Vercel environments, try SQLAlchemy backend first
            logger.info("Using SQLAlchemy session backend")
            app.config['SESSION_SQLALCHEMY'] = db
            app.config['SESSION_SQLALCHEMY_TABLE'] = 'sessions'
            
            # Create all tables including sessions
            with app.app_context():
                # First create all other tables
                tables_to_create = [table for table in db.metadata.tables.values() 
                                if table.name != 'sessions']
                db.metadata.create_all(bind=db.engine, tables=tables_to_create)
                
                # Explicitly create sessions table with proper schema
                # This ensures the table exists before Flask-Session tries to use it
                db.session.execute(text("""
                CREATE TABLE IF NOT EXISTS sessions (
                    id VARCHAR(255) NOT NULL PRIMARY KEY,
                    session_data BYTEA NOT NULL,
                    expiry TIMESTAMP(6) NOT NULL
                )
                """))
                db.session.commit()
                logger.info("Database tables created successfully, including sessions table")
            
            # Initialize Flask-Session after creating the sessions table
            Session(app)
            logger.info("Flask-Session initialized successfully with SQLAlchemy backend")
    except Exception as e:
        logger.error(f"Failed to initialize database or Flask-Session: {str(e)}", extra={'traceback': traceback.format_exc()})
        # Fall back to filesystem sessions as a last resort
        app.config['SESSION_TYPE'] = 'filesystem'
        os.makedirs('/tmp/flask_session', exist_ok=True)
        app.config['SESSION_FILE_DIR'] = '/tmp/flask_session'
        Session(app)
        logger.warning("Falling back to filesystem sessions due to error")
    logger.info("Database and migrations initialized successfully")
except Exception as e:
    logger.error(f"Failed to initialize database: {str(e)}")
    # Continue without database to allow basic functionality

# Define database models
class User(db.Model, UserMixin):
    __tablename__ = 'user'  # Explicitly set for PostgreSQL compatibility
    
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(80), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    password_hash = db.Column(db.String(200), nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    stripe_customer_id = db.Column(db.String(120), nullable=True)
    oauth_provider = db.Column(db.String(20), nullable=True)
    oauth_id = db.Column(db.String(100), nullable=True)
    stripe_price_id = db.Column(db.String(255), nullable=True)
    subscription_price = db.Column(db.Float, nullable=True)
    
    # Cash tracking
    max_cash_deployed = db.Column(db.Float, default=0.0, nullable=False)
    cash_proceeds = db.Column(db.Float, default=0.0, nullable=False)
    
    # Portfolio sharing & GDPR
    portfolio_slug = db.Column(db.String(20), unique=True, nullable=True)
    deleted_at = db.Column(db.DateTime, nullable=True)
    
    stocks = db.relationship('Stock', backref='user', lazy=True)
    transactions = db.relationship('Transaction', backref='user', lazy=True)
    # We'll use subscriptions_made and subscribers relationships defined in the Subscription model
    
    def set_password(self, password):
        self.password_hash = generate_password_hash(password)
        
    def check_password(self, password):
        return check_password_hash(self.password_hash, password)
        
    @property
    def is_admin(self):
        return self.email == ADMIN_EMAIL or self.username == ADMIN_USERNAME

    def __repr__(self):
        return f'<User {self.username}>'

class Stock(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
    ticker = db.Column(db.String(10), nullable=False)
    quantity = db.Column(db.Float, nullable=False)
    purchase_price = db.Column(db.Float, nullable=False)
    purchase_date = db.Column(db.DateTime, default=datetime.utcnow)

    def __repr__(self):
        return f'<Stock {self.ticker}>'
        
    def current_value(self):
        # Get real stock data from Alpha Vantage
        stock_data = get_stock_data(self.ticker)
        current_price = stock_data.get('price', self.purchase_price)
        return current_price * self.quantity
        
    def profit_loss(self):
        return self.current_value() - (self.purchase_price * self.quantity)

class Transaction(db.Model):
    __tablename__ = 'stock_transaction'  # Use same table name as models.py to avoid conflicts
    
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
    ticker = db.Column(db.String(10), nullable=False)
    quantity = db.Column(db.Float, nullable=False)
    price = db.Column(db.Float, nullable=False)
    transaction_type = db.Column(db.String(4), nullable=False)  # 'buy' or 'sell'
    timestamp = db.Column(db.DateTime, default=datetime.utcnow)
    
    def __repr__(self):
        return f'<Transaction {self.transaction_type} {self.ticker}>'

class Subscription(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    subscriber_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
    subscribed_to_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
    stripe_subscription_id = db.Column(db.String(255), unique=True, nullable=False)
    status = db.Column(db.String(20), nullable=False, default='active')  # 'active', 'canceled', etc.
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    start_date = db.Column(db.DateTime, default=datetime.utcnow)
    end_date = db.Column(db.DateTime, nullable=True)
    
    # Define relationships to get User objects with explicit foreign keys
    # backref creates a 'subscriptions_made' collection on the User model (for the subscriber)
    subscriber = db.relationship(
        'User', 
        foreign_keys=[subscriber_id], 
        backref=db.backref('subscriptions_made', lazy=True),
        lazy=True
    )
    
    # backref creates a 'subscribers' collection on the User model (for the user being subscribed to)
    subscribed_to = db.relationship(
        'User', 
        foreign_keys=[subscribed_to_id], 
        backref=db.backref('subscribers', lazy=True),
        lazy=True
    )

    def __repr__(self):
        return f'<Subscription {self.subscriber_id} to {self.subscribed_to_id} - {self.status}>'

class UserPortfolioChartCache(db.Model):
    """Pre-generated portfolio charts for leaderboard users only"""
    __tablename__ = 'user_portfolio_chart_cache'
    
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
    period = db.Column(db.String(10), nullable=False)  # '1D', '5D', '3M', 'YTD', '1Y', '5Y', 'MAX'
    chart_data = db.Column(db.Text, nullable=False)  # JSON string of chart data
    generated_at = db.Column(db.DateTime, nullable=False)
    
    # Ensure one cache entry per user per period
    __table_args__ = (db.UniqueConstraint('user_id', 'period', name='unique_user_period_chart'),)
    
    def __repr__(self):
        return f"<UserPortfolioChartCache user_id={self.user_id} {self.period} generated at {self.generated_at}>"

# Secret key is already set in app.config

# Check if we're running on Vercel
VERCEL_ENV = os.environ.get('VERCEL_ENV')
if VERCEL_ENV:
    print(f"Running in Vercel environment: {VERCEL_ENV}")

# Admin email for authentication - using environment variables defined above

# Flash message categories
app.config['MESSAGE_CATEGORIES'] = ['success', 'info', 'warning', 'danger']

# Admin authentication check
def admin_required(f):
    """Decorator to check if user is an admin"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # Check if user is authenticated via session
        email = session.get('email', '')
        
        # Allow access for fordutilityapps@gmail.com
        if email == ADMIN_EMAIL:
            return f(*args, **kwargs)
            
        # Show access denied page with login form instead of redirecting
        return render_template_string("""
<!DOCTYPE html>
<html>
<head>
    <title>Admin Access</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 800px; margin: 0 auto; }
        .button { 
            display: inline-block; 
            background: #4CAF50; 
            color: white; 
            padding: 10px 20px; 
            text-decoration: none; 
            border-radius: 5px; 
            margin-top: 20px;
        }
        .error {
            background-color: #ffdddd;
            border-left: 6px solid #f44336;
            margin-bottom: 15px;
            padding: 10px;
            border-radius: 5px;
        }
        .form {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 5px;
        }
        input[type=text] {
            width: 100%;
            padding: 12px 20px;
            margin: 8px 0;
            box-sizing: border-box;
        }
        input[type=submit] {
            background-color: #4CAF50;
            color: white;
            padding: 14px 20px;
            margin: 8px 0;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Admin Access</h1>
        
        <div class="error">
            <h2>Access Denied</h2>
            <p>You must be logged in with the admin email to access this page.</p>
        </div>
        
        <div class="form">
            <h2>Admin Login</h2>
            <form action="/login" method="post">
                <label for="email">Admin Email:</label>
                <input type="text" id="email" name="email" placeholder="Enter admin email">
                <input type="submit" value="Login">
            </form>
        </div>
        
        <a href="/" class="button">Back to Home</a>
    </div>
</body>
</html>
    """)
    return decorated_function

# Simple HTML template for the home page
HOME_HTML = """
<!DOCTYPE html>
<html>
<head>
    <title>ApesTogether</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 800px; margin: 0 auto; }
        .info { margin-top: 20px; background: #f5f5f5; padding: 15px; border-radius: 5px; }
        .button { 
            display: inline-block; 
            background: #4CAF50; 
            color: white; 
            padding: 10px 20px; 
            text-decoration: none; 
            border-radius: 5px; 
            margin-top: 20px;
        }
        .button-secondary {
            background: #2196F3;
        }
        .nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
        }
        .nav-links a {
            margin-left: 15px;
            text-decoration: none;
            color: #333;
        }
        .hero {
            background: #f9f9f9;
            padding: 40px;
            border-radius: 5px;
            text-align: center;
            margin-bottom: 30px;
        }
        .features {
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
            margin-bottom: 30px;
        }
        .feature {
            flex-basis: 30%;
            background: #f9f9f9;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <h2>ApesTogether</h2>
            <div class="nav-links">
                <a href="/">Home</a>
                <a href="/login">Login</a>
                <a href="/register">Register</a>
                <a href="/admin">Admin</a>
            </div>
        </div>
        
        <div class="hero">
            <h1>ApesTogether Stock Portfolio App</h1>
            <p>Track, manage, and optimize your stock investments in one place</p>
            <a href="/register" class="button">Get Started</a>
            <a href="/login" class="button button-secondary">Login</a>
        </div>
        
        <div class="features">
            <div class="feature">
                <h3>Portfolio Tracking</h3>
                <p>Keep track of all your stock investments in one place</p>
            </div>
            <div class="feature">
                <h3>Performance Analysis</h3>
                <p>Analyze your portfolio performance over time</p>
            </div>
            <div class="feature">
                <h3>Stock Comparison</h3>
                <p>Compare different stocks to make better investment decisions</p>
            </div>
        </div>
        
        <div class="info">
            <h2>Admin Access</h2>
            <p>If you are an admin user, you can access the admin panel here:</p>
            <a href="/admin" class="button">Admin Access</a>
        </div>
        
        <div class="info">
            <h2>Environment Info:</h2>
            <p><strong>Time:</strong> {{ current_time }}</p>
            <p><strong>Environment:</strong> {{ environment }}</p>
        </div>
    </div>
</body>
</html>
"""

# Add diagnostic route for troubleshooting
# Subscription and Payment Routes

@app.route('/create-checkout-session/<int:user_id>')
@login_required
def create_checkout_session(user_id):
    """Create a checkout session for Apple Pay and other payment methods"""
    user_to_subscribe_to = User.query.get_or_404(user_id)
    
    try:
        # Get or create a Stripe customer for the current user
        current_user_id = session.get('user_id')
        current_user = User.query.get(current_user_id)
        
        if not current_user.stripe_customer_id:
            customer = stripe.Customer.create(
                email=current_user.email,
                name=current_user.username
            )
            current_user.stripe_customer_id = customer.id
            db.session.commit()
        
        # Create a checkout session
        checkout_session = stripe.checkout.Session.create(
            payment_method_types=['card', 'apple_pay'],
            line_items=[{
                'price': user_to_subscribe_to.stripe_price_id,
                'quantity': 1,
            }],
            mode='subscription',
            success_url=request.host_url + 'subscription-success?session_id={CHECKOUT_SESSION_ID}',
            cancel_url=request.host_url + f'profile/{user_to_subscribe_to.username}',
            customer=current_user.stripe_customer_id,
            metadata={
                'subscriber_id': current_user.id,
                'subscribed_to_id': user_to_subscribe_to.id
            }
        )
        
        return redirect(checkout_session.url)
    except Exception as e:
        flash(f'Error creating checkout session: {str(e)}', 'danger')
        return redirect(url_for('profile', username=user_to_subscribe_to.username))

@app.route('/create-payment-intent', methods=['POST'])
@login_required
def create_payment_intent():
    """Creates a subscription and a Payment Intent for Stripe Elements with Apple Pay support"""
    data = request.get_json()
    user_id = data.get('user_id')
    user_to_subscribe_to = User.query.get_or_404(user_id)
    
    try:
        # Get or create a Stripe customer for the current user
        current_user_id = session.get('user_id')
        current_user = User.query.get(current_user_id)
        
        if not current_user.stripe_customer_id:
            customer = stripe.Customer.create(
                email=current_user.email,
                name=current_user.username
            )
            current_user.stripe_customer_id = customer.id
            db.session.commit()
        
        customer_id = current_user.stripe_customer_id

        # Create a subscription with an incomplete payment
        subscription = stripe.Subscription.create(
            customer=customer_id,
            items=[{'price': user_to_subscribe_to.stripe_price_id}],
            payment_behavior='default_incomplete',
            payment_settings={'save_default_payment_method': 'on_subscription'},
            expand=['latest_invoice.payment_intent'],
            metadata={
                'subscriber_id': current_user.id,
                'subscribed_to_id': user_to_subscribe_to.id
            }
        )
        
        # Add metadata to the payment intent as well for better tracking
        payment_intent = subscription.latest_invoice.payment_intent
        stripe.PaymentIntent.modify(
            payment_intent.id,
            metadata={
                'subscription': subscription.id,
                'subscriber_id': current_user.id,
                'subscribed_to_id': user_to_subscribe_to.id
            }
        )

        return jsonify({
            'clientSecret': payment_intent.client_secret,
            'subscriptionId': subscription.id
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/payment-confirmation')
@login_required
def payment_confirmation():
    """Handle payment confirmation for cases requiring additional authentication"""
    payment_intent_client_secret = request.args.get('payment_intent_client_secret')
    subscription_id = request.args.get('subscription_id')
    user_id = request.args.get('user_id')
    
    if not payment_intent_client_secret or not subscription_id:
        flash('Missing payment information', 'danger')
        return redirect(url_for('dashboard'))
    
    # Get the user to subscribe to
    user_to_subscribe_to = User.query.get_or_404(user_id) if user_id else None
    
    return render_template_with_defaults(
        'payment_confirmation.html',
        stripe_public_key=app.config['STRIPE_PUBLIC_KEY'],
        payment_intent_client_secret=payment_intent_client_secret,
        subscription_id=subscription_id,
        user_to_view=user_to_subscribe_to
    )

@app.route('/subscription-success')
@login_required
def subscription_success():
    """Handle successful subscription from both checkout session and payment intent flows"""
    session_id = request.args.get('session_id')
    subscription_id = request.args.get('subscription_id')
    
    try:
        if session_id:
            # Checkout Session flow
            checkout_session = stripe.checkout.Session.retrieve(session_id)
            
            # Create a new subscription record
            subscription = Subscription(
                subscriber_id=int(checkout_session.metadata.subscriber_id),
                subscribed_to_id=int(checkout_session.metadata.subscribed_to_id),
                stripe_subscription_id=checkout_session.subscription,
                status='active'
            )
            
            db.session.add(subscription)
            db.session.commit()
            
            subscribed_to = User.query.get(subscription.subscribed_to_id)
            flash(f'Successfully subscribed to {subscribed_to.username}\'s portfolio!', 'success')
            return redirect(url_for('profile', username=subscribed_to.username))
            
        elif subscription_id:
            # Payment Intent flow (Apple Pay)
            stripe_subscription = stripe.Subscription.retrieve(subscription_id)
            
            # Check if we already have this subscription recorded
            existing_sub = Subscription.query.filter_by(stripe_subscription_id=subscription_id).first()
            if existing_sub:
                subscribed_to = User.query.get(existing_sub.subscribed_to_id)
                flash(f'Successfully subscribed to {subscribed_to.username}\'s portfolio!', 'success')
                return redirect(url_for('profile', username=subscribed_to.username))
            
            # Create a new subscription record from the metadata
            if 'subscriber_id' in stripe_subscription.metadata and 'subscribed_to_id' in stripe_subscription.metadata:
                subscription = Subscription(
                    subscriber_id=int(stripe_subscription.metadata.subscriber_id),
                    subscribed_to_id=int(stripe_subscription.metadata.subscribed_to_id),
                    stripe_subscription_id=subscription_id,
                    status=stripe_subscription.status
                )
                
                db.session.add(subscription)
                db.session.commit()
                
                subscribed_to = User.query.get(subscription.subscribed_to_id)
                flash(f'Successfully subscribed to {subscribed_to.username}\'s portfolio!', 'success')
                logger.info(f"Login successful for user {subscribed_to.username}, redirecting to dashboard")
                try:
                    # Use a simple redirect to avoid potential template rendering issues
                    return redirect('/')
                except Exception as redirect_error:
                    logger.error(f"Error during redirect after successful login: {str(redirect_error)}")
                    logger.error(traceback.format_exc())
                    # Even if redirect fails, user is already logged in
                    return redirect(url_for('index'))
            else:
                flash('Subscription metadata is missing', 'danger')
                return redirect(url_for('dashboard'))
        else:
            flash('Invalid subscription information', 'danger')
            return redirect(url_for('dashboard'))
            
    except Exception as e:
        flash(f'Error processing subscription: {str(e)}', 'danger')
        return redirect(url_for('dashboard'))

@app.route('/webhook', methods=['POST'])
def webhook():
    """Handle Stripe webhook events"""
    payload = request.get_data(as_text=True)
    sig_header = request.headers.get('Stripe-Signature')
    
    try:
        event = stripe.Webhook.construct_event(
            payload, sig_header, app.config['STRIPE_WEBHOOK_SECRET']
        )
    except ValueError as e:
        # Invalid payload
        return jsonify({'error': str(e)}), 400
    except stripe.error.SignatureVerificationError as e:
        # Invalid signature
        return jsonify({'error': str(e)}), 400
    
    # Handle the event
    if event['type'] == 'invoice.payment_succeeded':
        # Handle successful payment for subscription
        invoice = event['data']['object']
        subscription_id = invoice['subscription']
        
        # Update the subscription status
        stripe_subscription = stripe.Subscription.retrieve(subscription_id)
        
        # Find the corresponding subscription in our database
        subscription = Subscription.query.filter_by(stripe_subscription_id=subscription_id).first()
        if subscription:
            subscription.status = stripe_subscription['status']
            db.session.commit()
        else:
            # This might be a new subscription from Apple Pay that hasn't been recorded yet
            # Create a new subscription record if metadata is available
            try:
                if 'subscriber_id' in stripe_subscription.metadata and 'subscribed_to_id' in stripe_subscription.metadata:
                    new_subscription = Subscription(
                        subscriber_id=int(stripe_subscription.metadata.subscriber_id),
                        subscribed_to_id=int(stripe_subscription.metadata.subscribed_to_id),
                        stripe_subscription_id=subscription_id,
                        status=stripe_subscription.status
                    )
                    db.session.add(new_subscription)
                    db.session.commit()
            except Exception as e:
                # Log the error but don't fail the webhook
                print(f"Error creating subscription from webhook: {str(e)}")
    
    elif event['type'] == 'customer.subscription.deleted':
        # Handle subscription cancellation
        subscription_obj = event['data']['object']
        subscription_id = subscription_obj['id']
        
        # Find and update the subscription in our database
        subscription = Subscription.query.filter_by(stripe_subscription_id=subscription_id).first()
        if subscription:
            subscription.status = 'canceled'
            subscription.end_date = datetime.utcnow()
            db.session.commit()
    
    elif event['type'] == 'payment_intent.succeeded':
        # Handle successful payment intent (could be from Apple Pay)
        payment_intent = event['data']['object']
        
        # If this payment intent is related to a subscription, make sure it's properly recorded
        if 'subscription' in payment_intent.metadata:
            subscription_id = payment_intent.metadata.subscription
            
            # Check if we already have this subscription
            subscription = Subscription.query.filter_by(stripe_subscription_id=subscription_id).first()
            if not subscription and 'subscriber_id' in payment_intent.metadata and 'subscribed_to_id' in payment_intent.metadata:
                # Create the subscription record
                new_subscription = Subscription(
                    subscriber_id=int(payment_intent.metadata.subscriber_id),
                    subscribed_to_id=int(payment_intent.metadata.subscribed_to_id),
                    stripe_subscription_id=subscription_id,
                    status='active'
                )
                db.session.add(new_subscription)
                db.session.commit()
    
    return jsonify({'status': 'success'})

@app.route('/subscriptions')
@login_required
def subscriptions():
    """Display a user's active and canceled subscriptions"""
    try:
        # Verify user is authenticated - this should be redundant with @login_required
        # but we're being extra cautious in the serverless environment
        if not current_user.is_authenticated:
            logger.warning("User not authenticated despite @login_required decorator")
            flash('Please log in to access your subscriptions.', 'warning')
            return redirect(url_for('login'))
            
        # For backward compatibility, ensure session is in sync with Flask-Login
        # and refresh the session to keep it alive in the serverless environment
        if session.get('user_id') != current_user.id:
            session['user_id'] = current_user.id
            session['email'] = current_user.email
            session['username'] = current_user.username
            session.modified = True
            
        # Log successful access for debugging
        logger.info(f"Subscriptions accessed by user: {current_user.username} (ID: {current_user.id})")
            
        current_user_id = current_user.id
        
        # Get active and cancelled (but not expired) subscriptions
        try:
            from models import Subscription
            from datetime import datetime, timedelta
            
            # Get all active subscriptions
            active_subs = Subscription.query.filter_by(
                subscriber_id=current_user_id,
                status='active'
            ).all()
            
            # Get cancelled subscriptions that haven't expired yet
            cancelled_subs = Subscription.query.filter(
                Subscription.subscriber_id == current_user_id,
                Subscription.status == 'cancelled',
                (Subscription.end_date.is_(None) | (Subscription.end_date > datetime.utcnow()))
            ).all()
            
            # Combine active and cancelled (still have access) as "active_subscriptions"
            all_active_subs = active_subs + cancelled_subs
            
            # Clean up expired cancelled subscriptions
            expired_subs = Subscription.query.filter(
                Subscription.subscriber_id == current_user_id,
                Subscription.status == 'cancelled',
                Subscription.end_date <= datetime.utcnow()
            ).all()
            for sub in expired_subs:
                db.session.delete(sub)
            if expired_subs:
                db.session.commit()
            
            # Get truly canceled (past) subscriptions for history
            past_subs = Subscription.query.filter(
                Subscription.subscriber_id == current_user_id,
                Subscription.status.in_(['canceled', 'expired'])
            ).all()
            
            return render_template_with_defaults(
                'subscriptions.html',
                active_subscriptions=all_active_subs,
                canceled_subscriptions=past_subs,
                current_user=current_user
            )
        except Exception as e:
            logger.error(f"Error querying subscriptions: {str(e)}")
            logger.error(traceback.format_exc())
            flash('An error occurred while loading your subscriptions. Please try again.', 'warning')
            return render_template_with_defaults(
                'subscriptions.html',
                active_subscriptions=[],
                canceled_subscriptions=[],
                current_user=current_user
            )
    except Exception as e:
        logger.error(f"Error in subscriptions route: {str(e)}")
        logger.error(traceback.format_exc())
        flash('An error occurred while loading your subscriptions. Please try again.', 'danger')
        return redirect(url_for('index'))
    

@app.route('/cancel-subscription', methods=['POST'])
@login_required
def cancel_subscription():
    """Cancel a user's subscription"""
    subscription_id = request.form.get('subscription_id')
    if not subscription_id:
        flash('Invalid subscription', 'danger')
        return redirect(url_for('subscriptions'))
    
    # Get the subscription
    subscription = Subscription.query.get_or_404(subscription_id)
    
    # Verify ownership
    current_user_id = session.get('user_id')
    if subscription.subscriber_id != current_user_id:
        flash('You do not have permission to cancel this subscription', 'danger')
        return redirect(url_for('subscriptions'))
    
    try:
        # Cancel the subscription in Stripe
        stripe.Subscription.delete(subscription.stripe_subscription_id)
        
        # Update the subscription in our database
        subscription.status = 'canceled'
        subscription.end_date = datetime.utcnow()
        db.session.commit()
        
        flash('Subscription canceled successfully', 'success')
    except Exception as e:
        flash(f'Error canceling subscription: {str(e)}', 'danger')
    
    return redirect(url_for('subscriptions'))

@app.route('/resubscribe', methods=['POST'])
@login_required
def resubscribe():
    """Reactivate a canceled subscription by creating a new one"""
    old_subscription_id = request.form.get('subscription_id')
    if not old_subscription_id:
        flash('Invalid subscription', 'danger')
        return redirect(url_for('subscriptions'))
    
    # Get the old subscription
    old_subscription = Subscription.query.get_or_404(old_subscription_id)
    
    # Verify ownership
    current_user_id = session.get('user_id')
    if old_subscription.subscriber_id != current_user_id:
        flash('You do not have permission to reactivate this subscription', 'danger')
        return redirect(url_for('subscriptions'))
    
    try:
        # Get the user to subscribe to
        user_to_subscribe_to = User.query.get_or_404(old_subscription.subscribed_to_id)
        
        # Get the current user
        current_user = User.query.get_or_404(current_user_id)
        
        # Create a new subscription in Stripe
        stripe_subscription = stripe.Subscription.create(
            customer=current_user.stripe_customer_id,
            items=[
                {'price': app.config['STRIPE_PRICE_ID']},
            ],
            metadata={
                'subscriber_id': current_user_id,
                'subscribed_to_id': user_to_subscribe_to.id
            }
        )
        
        # Create a new subscription record
        new_subscription = Subscription(
            subscriber_id=current_user_id,
            subscribed_to_id=user_to_subscribe_to.id,
            stripe_subscription_id=stripe_subscription.id,
            status='active'
        )
        db.session.add(new_subscription)
        db.session.commit()
        
        flash(f'Successfully resubscribed to {user_to_subscribe_to.username}\'s portfolio!', 'success')
    except Exception as e:
        flash(f'Error reactivating subscription: {str(e)}', 'danger')
    
    return redirect(url_for('subscriptions'))

@app.route('/explore')
@login_required
def explore():
    """Display a list of all other users to subscribe to."""
    current_user_id = session.get('user_id')
    users = User.query.filter(User.id != current_user_id).order_by(User.username).all()
    return render_template_with_defaults('explore.html', users=users)

@app.route('/onboarding')
@login_required
def onboarding():
    """User onboarding page"""
    return render_template_with_defaults('onboarding.html')

@app.route('/dashboard')
@login_required
def dashboard():
    """Display the user's dashboard."""
    # Log dashboard view activity
    try:
        from models import UserActivity
        activity = UserActivity(
            user_id=current_user.id,
            activity_type='view_dashboard',
            ip_address=request.remote_addr,
            user_agent=request.headers.get('User-Agent', '')[:255]
        )
        db.session.add(activity)
        db.session.commit()
        db.session.flush()  # Ensure record is immediately visible
        logger.info(f"Successfully logged dashboard activity for user {current_user.id}")
    except Exception as e:
        logger.error(f"Error logging dashboard activity: {str(e)}")
        logger.error(traceback.format_exc())
        db.session.rollback()
    
    # Get user's portfolio
    portfolio_data = []
    total_portfolio_value = 0
    
    if current_user.is_authenticated:
        from datetime import date, datetime, timedelta
        from models import PortfolioSnapshot
        from models import PortfolioSnapshotIntraday
        
        # Check if it's weekend or after market hours
        today = get_market_date()  # FIX: Use ET not UTC
        is_weekend = today.weekday() >= 5  # Saturday = 5, Sunday = 6
        current_hour = get_market_time().hour  # FIX: Use ET not UTC
        is_after_hours = current_hour < 9 or current_hour >= 16  # Before 9 AM or after 4 PM
        
        # SMART CACHING STRATEGY: Use cached data by default, refresh only when requested during market hours
        force_refresh = request.args.get('refresh') == 'true'
        is_market_hours_weekday = not is_weekend and not is_after_hours
        use_cached_data = not (is_market_hours_weekday and force_refresh)
        
        # Always try cached data first (fastest loading)
        from models import PortfolioSnapshotIntraday
        
        # Get most recent data from either intraday or daily snapshots
        latest_daily_snapshot = PortfolioSnapshot.query.filter_by(user_id=current_user.id)\
            .order_by(PortfolioSnapshot.date.desc()).first()
        
        latest_intraday_snapshot = PortfolioSnapshotIntraday.query.filter_by(user_id=current_user.id)\
            .order_by(PortfolioSnapshotIntraday.timestamp.desc()).first()
        
        # Determine which snapshot is more recent
        use_intraday = False
        latest_snapshot = latest_daily_snapshot
        
        if latest_intraday_snapshot and latest_daily_snapshot:
            # Compare intraday timestamp with daily date
            intraday_date = latest_intraday_snapshot.timestamp.date()
            daily_date = latest_daily_snapshot.date
            
            if intraday_date >= daily_date:
                use_intraday = True
                latest_snapshot = latest_intraday_snapshot
        elif latest_intraday_snapshot:
            use_intraday = True
            latest_snapshot = latest_intraday_snapshot
        
        if use_cached_data and latest_snapshot:
            # Use cached data (fast loading)
            if use_intraday:
                total_portfolio_value = latest_snapshot.total_value
                data_timestamp = latest_snapshot.timestamp
                data_source = f"Intraday snapshot from {data_timestamp.strftime('%H:%M')}"
            else:
                total_portfolio_value = latest_snapshot.total_value
                data_timestamp = datetime.combine(latest_snapshot.date, datetime.min.time())
                data_source = f"Market close from {latest_snapshot.date.strftime('%m/%d/%Y')}"
            
            # Get individual stock data for display
            stocks = Stock.query.filter_by(user_id=current_user.id).all()
            
            # Calculate individual stock values proportionally
            total_cost_basis = sum(stock.quantity * stock.purchase_price for stock in stocks if stock.purchase_price)
            
            for stock in stocks:
                if total_cost_basis > 0:
                    # Estimate current price based on proportional portfolio performance
                    cost_basis = stock.quantity * stock.purchase_price if stock.purchase_price else 0
                    portfolio_multiplier = total_portfolio_value / total_cost_basis if total_cost_basis > 0 else 1
                    estimated_current_price = stock.purchase_price * portfolio_multiplier if stock.purchase_price else 0
                    
                    value = stock.quantity * estimated_current_price
                    gain_loss = value - cost_basis
                    gain_loss_percent = (gain_loss / cost_basis) * 100 if cost_basis > 0 else 0
                    
                    portfolio_data.append({
                        'ticker': stock.ticker,
                        'quantity': stock.quantity,
                        'purchase_price': stock.purchase_price,
                        'current_price': estimated_current_price,
                        'value': value,
                        'gain_loss': gain_loss,
                        'gain_loss_percent': gain_loss_percent
                    })
                else:
                    portfolio_data.append({
                        'ticker': stock.ticker,
                        'quantity': stock.quantity,
                        'purchase_price': stock.purchase_price,
                        'current_price': 'N/A',
                        'value': 'N/A',
                        'gain_loss': 'N/A',
                        'gain_loss_percent': 'N/A'
                    })
            
            # Smart refresh logic: Only make API calls if user requests refresh AND data is stale
            if force_refresh and is_market_hours_weekday:
                current_time = datetime.now()
                data_age_seconds = (current_time - data_timestamp).total_seconds()
                
                if data_age_seconds > 90:  # Data is stale, refresh with API calls
                    # Clear cached data and fall through to API refresh
                    portfolio_data = []
                    total_portfolio_value = 0
                    use_cached_data = False
                # If data is fresh (<90s), keep using cached data even though user clicked refresh
        
        if not use_cached_data:
            # Use live API data during market hours on weekdays
            stocks = Stock.query.filter_by(user_id=current_user.id).all()
            
            # Get all tickers for batch processing
            tickers = [stock.ticker for stock in stocks]
            batch_prices = get_batch_stock_data(tickers)
            
            for stock in stocks:
                ticker_upper = stock.ticker.upper()
                current_price = batch_prices.get(ticker_upper)
                
                if current_price is not None:
                    value = stock.quantity * current_price
                    gain_loss = value - (stock.quantity * stock.purchase_price)
                    gain_loss_percent = (gain_loss / (stock.quantity * stock.purchase_price)) * 100 if stock.purchase_price else 0
                    
                    portfolio_data.append({
                        'ticker': stock.ticker,
                        'quantity': stock.quantity,
                        'purchase_price': stock.purchase_price,
                        'current_price': current_price,
                        'value': value,
                        'gain_loss': gain_loss,
                        'gain_loss_percent': gain_loss_percent
                    })
                    
                    total_portfolio_value += value
                else:
                    # Handle cases where stock data couldn't be fetched
                    portfolio_data.append({
                        'ticker': stock.ticker,
                        'quantity': stock.quantity,
                        'purchase_price': stock.purchase_price,
                        'current_price': 'N/A',
                        'value': 'N/A',
                        'gain_loss': 'N/A',
                        'gain_loss_percent': 'N/A'
                    })
    
    # Generate portfolio slug if user doesn't have one
    if current_user.is_authenticated and not current_user.portfolio_slug:
        try:
            current_user.portfolio_slug = generate_portfolio_slug()
            db.session.commit()
            logger.info(f"Generated portfolio slug for user {current_user.id}")
        except Exception as e:
            logger.error(f"Error generating portfolio slug: {str(e)}")
            db.session.rollback()
    
    # Build share URL
    share_url = f"https://apestogether.ai/p/{current_user.portfolio_slug}" if current_user.is_authenticated and current_user.portfolio_slug else ""
    
    # Get user's leaderboard positions (if in top 20)
    leaderboard_positions = {}
    try:
        from leaderboard_utils import get_user_leaderboard_positions
        leaderboard_positions = get_user_leaderboard_positions(current_user.id, top_n=20)
        logger.info(f"DEBUG: Leaderboard positions for user {current_user.id}: {leaderboard_positions}")
    except Exception as e:
        logger.error(f"Error fetching leaderboard positions: {str(e)}")
        logger.error(traceback.format_exc())
    
    # Get user's portfolio stats (Phase 3)
    portfolio_stats = None
    try:
        from models import UserPortfolioStats
        portfolio_stats = UserPortfolioStats.query.filter_by(user_id=current_user.id).first()
        logger.info(f"DEBUG: Portfolio stats for user {current_user.id}: {portfolio_stats}")
    except Exception as e:
        logger.error(f"Error fetching portfolio stats: {str(e)}")
    
    return render_template_with_defaults('dashboard.html', 
                                       portfolio_data=portfolio_data,
                                       stocks=portfolio_data,  # Template expects 'stocks' variable
                                       total_portfolio_value=total_portfolio_value,
                                       leaderboard_positions=leaderboard_positions,
                                       portfolio_stats=portfolio_stats,
                                       share_url=share_url,
                                       now=datetime.now())

@app.route('/update_username', methods=['POST'])
@login_required
def update_username():
    """Update the user's username."""
    new_username = request.form.get('username')
    if not new_username or len(new_username) < 3:
        flash('Username must be at least 3 characters long', 'danger')
        return redirect(url_for('dashboard'))
    
    # Check if username already exists (except for current user)
    existing_user = User.query.filter(User.username == new_username, User.id != current_user.id).first()
    if existing_user:
        flash('Username already taken', 'danger')
        return redirect(url_for('dashboard'))
    
    try:
        current_user.username = new_username
        db.session.commit()
        flash('Username updated successfully', 'success')
        # Update session if using session-based auth
        if 'username' in session:
            session['username'] = new_username
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"Error updating username: {str(e)}")
        flash('An error occurred while updating your username', 'danger')
    
    return redirect(url_for('dashboard'))

@app.route('/api/portfolio_value')
@login_required
def portfolio_value():
    """API endpoint to get portfolio value data"""
    from datetime import date, datetime
    from models import PortfolioSnapshot
    
    # Get current user from session
    current_user_id = session.get('user_id')
    
    # Check if it's weekend or after market hours - use cached snapshots
    today = get_market_date()  # FIX: Use ET not UTC
    is_weekend = today.weekday() >= 5  # Saturday = 5, Sunday = 6
    current_hour = get_market_time().hour  # FIX: Use ET not UTC
    is_after_hours = current_hour < 9 or current_hour >= 16  # Before 9 AM or after 4 PM
    
    # ALWAYS fetch actual stock prices (smart cache handles efficiency)
    # Smart cache returns:
    # - Market hours: 90-second fresh cache
    # - After hours/weekends: Friday's closing price from cache (or API if not cached)
    stocks = Stock.query.filter_by(user_id=current_user_id).all()
    portfolio_data = []
    total_value = 0
    
    # Get all tickers for batch processing
    tickers = [stock.ticker for stock in stocks]
    batch_prices = get_batch_stock_data(tickers)
    
    # Use actual stock prices (not proportional estimates)
    for stock in stocks:
        ticker_upper = stock.ticker.upper()
        current_price = batch_prices.get(ticker_upper)
        
        if current_price is not None:
            value = stock.quantity * current_price
            total_value += value
            stock_info = {
                'id': stock.id,
                'ticker': stock.ticker,
                'quantity': stock.quantity,
                'purchase_price': stock.purchase_price,
                'current_price': current_price,
                'value': value,
                'gain_loss': (current_price - stock.purchase_price) * stock.quantity if stock.purchase_price else 0
            }
            portfolio_data.append(stock_info)
        else:
            # Handle cases where stock data couldn't be fetched
            stock_info = {
                'id': stock.id,
                'ticker': stock.ticker,
                'quantity': stock.quantity,
                'purchase_price': stock.purchase_price,
                'current_price': 'N/A',
                'value': 'N/A',
                'gain_loss': 'N/A'
            }
            portfolio_data.append(stock_info)
    
    return jsonify({
        'stocks': portfolio_data,
        'total_value': total_value
    })

@app.route('/save_onboarding', methods=['POST'])
@login_required
def save_onboarding():
    """Process the submitted stocks from onboarding"""
    # Get current user from session
    current_user_id = session.get('user_id')
    current_user = User.query.get(current_user_id)
    
    # Process the submitted stocks
    stocks_to_add = []
    
    # First collect all valid ticker/quantity pairs
    for i in range(10):  # We have 10 possible stock entries
        ticker = request.form.get(f'ticker_{i}')
        quantity = request.form.get(f'quantity_{i}')
        
        # Only process rows where both fields are filled
        if ticker and quantity:
            try:
                stocks_to_add.append({
                    'ticker': ticker.upper(),
                    'quantity': float(quantity)
                })
            except ValueError:
                flash(f'Invalid quantity for {ticker}. Skipped.', 'warning')
    
    # If we have stocks to check, validate and add them
    if stocks_to_add:
        stocks_added_count = 0
        for stock_data in stocks_to_add:
            stock_data_db = get_stock_data(stock_data['ticker'])
            if stock_data_db and stock_data_db.get('price') is not None:
                stock = Stock(
                    ticker=stock_data['ticker'],
                    quantity=stock_data['quantity'],
                    purchase_price=stock_data_db['price'],
                    user_id=current_user_id
                )
                db.session.add(stock)
                stocks_added_count += 1
            else:
                flash(f"Could not find ticker '{stock_data['ticker']}'. It was not added.", 'warning')
        
        if stocks_added_count > 0:
            db.session.commit()
            flash(f'Successfully added {stocks_added_count} stock(s) to your portfolio!', 'success')
    else:
        flash('No stocks were entered.', 'warning')
    
    return redirect(url_for('dashboard'))

@app.route('/stock-comparison')
def stock_comparison():
    """Stock comparison page with mock data"""
    try:
        # Generate mock data for the last 30 days
        from datetime import datetime, timedelta
        import random
        
        dates = []
        tsla_prices = []
        sp500_prices = []
        
        # Start with base prices
        tsla_base = 240.0
        spy_base = 500.0
        
        # Generate 30 days of mock data
        for i in range(30):
            day = datetime.now() - timedelta(days=30-i)
            dates.append(day.strftime('%Y-%m-%d'))
            
            # Add some random variation
            tsla_change = random.uniform(-10, 10)
            spy_change = random.uniform(-5, 5)
            
            tsla_base += tsla_change
            spy_base += spy_change
            
            tsla_prices.append(round(tsla_base, 2))
            sp500_prices.append(round(spy_base, 2))

        # Return the mock data
        return render_template_with_defaults('stock_comparison.html', dates=dates, tsla_prices=tsla_prices, sp500_prices=sp500_prices)
        
    except Exception as e:
        flash(f"Error generating mock stock comparison data: {e}", "danger")
        return render_template_with_defaults('stock_comparison.html', dates=[], tsla_prices=[], sp500_prices=[])

@app.route('/profile/<username>')
@login_required
def profile(username):
    """Display a user's profile page."""
    # Redirect to own dashboard if viewing self
    current_user_id = session.get('user_id')
    current_user = User.query.get(current_user_id)
    
    if current_user.username == username:
        return redirect(url_for('dashboard'))

    user_to_view = User.query.filter_by(username=username).first_or_404()

    # Check if the current user has an active subscription to this profile
    subscription = Subscription.query.filter_by(
        subscriber_id=current_user.id,
        subscribed_to_id=user_to_view.id,
        status='active'
    ).first()

    portfolio_data = None
    if subscription:
        # If subscribed, fetch portfolio data to display
        stocks = Stock.query.filter_by(user_id=user_to_view.id).all()
        total_value = 0
        stock_details = []
        # Get all tickers for batch processing
        tickers = [stock.ticker for stock in stocks]
        batch_prices = get_batch_stock_data(tickers)
        
        for stock in stocks:
            ticker_upper = stock.ticker.upper()
            price = batch_prices.get(ticker_upper)
            
            if price is not None:
                value = stock.quantity * price
                total_value += value
                stock_details.append({'ticker': stock.ticker, 'quantity': stock.quantity, 'price': price, 'value': value})
        portfolio_data = {
            'stocks': stock_details,
            'total_value': total_value
        }

    return render_template_with_defaults(
        'profile.html',
        user_to_view=user_to_view,
        subscription=subscription,
        portfolio_data=portfolio_data,
        price=user_to_view.subscription_price,
        stripe_public_key=app.config['STRIPE_PUBLIC_KEY']
    )

@app.route('/admin/subscription-analytics')
@login_required
def admin_subscription_analytics():
    """Admin dashboard for subscription analytics"""
    # Check if user is admin
    current_user_id = session.get('user_id')
    current_user = User.query.get(current_user_id)
    
    # Only allow access to the admin (fordutilityapps@gmail.com or witty-raven)
    if current_user.email != ADMIN_EMAIL and current_user.username != ADMIN_USERNAME:
        flash('You do not have permission to access this page', 'danger')
        return redirect(url_for('dashboard'))
    
    try:
        # Get subscription analytics data
        active_subscriptions_count = Subscription.query.filter_by(status='active').count()
        
        # Calculate total revenue (assuming $5 per subscription per month)
        subscription_price = 5.00
        total_revenue = active_subscriptions_count * subscription_price
        
        # Get recent subscriptions
        recent_subscriptions = Subscription.query.order_by(Subscription.start_date.desc()).limit(10).all()
        
        # Calculate conversion rate (subscriptions / total users)
        total_users = User.query.count()
        conversion_rate = round((active_subscriptions_count / total_users) * 100, 2) if total_users > 0 else 0
        
        # Calculate churn rate (canceled subscriptions / total subscriptions)
        canceled_subscriptions = Subscription.query.filter_by(status='canceled').count()
        total_subscriptions = active_subscriptions_count + canceled_subscriptions
        churn_rate = round((canceled_subscriptions / total_subscriptions) * 100, 2) if total_subscriptions > 0 else 0
        
        # Generate subscription growth data for the last 30 days
        subscription_dates = []
        subscription_counts = []
        revenue_dates = []
        revenue_amounts = []
        
        # Get the last 30 days
        today = datetime.utcnow().date()
        for i in range(30, 0, -1):
            date = today - timedelta(days=i)
            date_str = date.strftime('%Y-%m-%d')
            subscription_dates.append(date_str)
            
            # Count subscriptions created on this date
            count = Subscription.query.filter(
                func.date(Subscription.start_date) == date
            ).count()
            subscription_counts.append(count)
        
        # Generate monthly revenue data for the last 6 months
        for i in range(6, 0, -1):
            # Calculate the month and year correctly
            month = today.month - i + 1
            year = today.year
            
            # Handle year boundary
            if month <= 0:
                month += 12
                year -= 1
                
            # Get the first day of the month
            first_day = datetime(year, month, 1).date()
            month_name = first_day.strftime('%b %Y')
            revenue_dates.append(month_name)
            
            # Count active subscriptions in this month
            month_subscriptions = Subscription.query.filter(
                func.extract('month', Subscription.start_date) == first_day.month,
                func.extract('year', Subscription.start_date) == first_day.year,
                Subscription.status == 'active'
            ).count()
            revenue_amounts.append(month_subscriptions * subscription_price)
    except Exception as e:
        app.logger.error(f"Database error in admin_subscription_analytics: {str(e)}")
        # Fallback to mock data if database fails
        active_subscriptions_count = 15
        total_revenue = 75.00
        recent_subscriptions = []
        conversion_rate = 25.5
        churn_rate = 10.2
        subscription_dates = [f"2025-06-{i}" for i in range(16, 16+30)]
        subscription_counts = [random.randint(0, 3) for _ in range(30)]
        revenue_dates = ['Feb 2025', 'Mar 2025', 'Apr 2025', 'May 2025', 'Jun 2025', 'Jul 2025']
        revenue_amounts = [15.0, 25.0, 35.0, 45.0, 60.0, 75.0]
    
    return render_template_with_defaults(
        'admin/subscription_analytics.html',
        active_subscriptions_count=active_subscriptions_count,
        total_revenue=total_revenue,
        recent_subscriptions=recent_subscriptions,
        subscription_price=subscription_price,
        conversion_rate=conversion_rate,
        churn_rate=churn_rate,
        subscription_dates=subscription_dates,
        subscription_counts=subscription_counts,
        revenue_dates=revenue_dates,
        revenue_amounts=revenue_amounts
    )

@app.route('/admin/debug')
def admin_debug():
    """Return debug information about the environment"""
    import sys
    
@app.route('/admin/debug/users')
def admin_debug_users():
    """Debug endpoint to check user credentials"""
    try:
        # Only allow access from localhost or if user is admin
        if request.remote_addr != '127.0.0.1' and not (current_user.is_authenticated and current_user.is_admin()):
            return jsonify({'error': 'Unauthorized'}), 403
            
        admin_user = User.query.filter_by(email=ADMIN_EMAIL).first()
        if admin_user:
            # Don't return the actual password hash for security reasons
            return jsonify({
                'admin_user_exists': True,
                'username': admin_user.username,
                'has_password_hash': bool(admin_user.password_hash),
                'password_hash_length': len(admin_user.password_hash) if admin_user.password_hash else 0
            })
        else:
            return jsonify({'admin_user_exists': False})
    except Exception as e:
        logger.error(f"Error in debug users endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug/oauth')
def admin_debug_oauth():
    """Debug endpoint to check OAuth configuration"""
    try:
        # Only show if environment variables are set, not their actual values
        oauth_config = {
            'google_client_id_set': bool(os.environ.get('GOOGLE_CLIENT_ID')),
            'google_client_secret_set': bool(os.environ.get('GOOGLE_CLIENT_SECRET')),
            'apple_client_id_set': bool(os.environ.get('APPLE_CLIENT_ID')),
            'apple_client_secret_set': bool(os.environ.get('APPLE_CLIENT_SECRET')),
            'redirect_uri': url_for('authorize_google', _external=True),
            'base_url': request.host_url,
            'is_https': request.is_secure
        }
        return jsonify(oauth_config)
    except Exception as e:
        logger.error(f"Error in debug OAuth endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500
        
@app.route('/admin/debug/database')
def admin_debug_database():
    """Debug endpoint to check database connection"""
    try:
        # Check database connection
        db_config = {
            'database_url_exists': bool(os.environ.get('DATABASE_URL')),
            'postgres_prisma_url_exists': bool(os.environ.get('POSTGRES_PRISMA_URL')),
            'effective_database_url_exists': bool(DATABASE_URL),
            'sqlalchemy_database_uri_set': bool(app.config.get('SQLALCHEMY_DATABASE_URI')),
            'db_initialized': bool(db),
            'db_engine_initialized': bool(db.engine)
        }
        
        # Test database connection
        try:
            # Try to execute a simple query
            test_result = db.session.execute("SELECT 1").scalar()
            db_config['connection_test'] = 'Success' if test_result == 1 else f'Failed: {test_result}'
            
            # Count users in database
            user_count = User.query.count()
            db_config['user_count'] = user_count
            
            # Check if admin user exists
            admin_exists = User.query.filter_by(email=ADMIN_EMAIL).first() is not None
            db_config['admin_user_exists'] = admin_exists
            
        except Exception as db_test_error:
            db_config['connection_test'] = f'Error: {str(db_test_error)}'
            logger.error(f"Database connection test failed: {str(db_test_error)}")
            logger.error(traceback.format_exc())
        
        return jsonify(db_config)
    except Exception as e:
        logger.error(f"Error in debug database endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500
        
@app.route('/admin/debug/models')
def admin_debug_models():
    """Debug endpoint to check SQLAlchemy model relationships"""
    try:
        # Get model information
        model_info = {
            'user_model': {
                'attributes': [attr for attr in dir(User) if not attr.startswith('_')],
                'relationships': [
                    {'name': 'stocks', 'target': 'Stock', 'type': 'one-to-many'},
                    {'name': 'transactions', 'target': 'Transaction', 'type': 'one-to-many'},
                    {'name': 'subscriptions_made', 'target': 'Subscription', 'type': 'one-to-many'},
                    {'name': 'subscribers', 'target': 'Subscription', 'type': 'one-to-many'}
                ]
            },
            'subscription_model': {
                'attributes': [attr for attr in dir(Subscription) if not attr.startswith('_')],
                'relationships': [
                    {'name': 'subscriber', 'target': 'User', 'type': 'many-to-one'},
                    {'name': 'subscribed_to', 'target': 'User', 'type': 'many-to-one'}
                ],
                'foreign_keys': [
                    {'name': 'subscriber_id', 'references': 'user.id'},
                    {'name': 'subscribed_to_id', 'references': 'user.id'}
                ]
            }
        }
        
        return jsonify(model_info)
    except Exception as e:
        logger.error(f"Error in debug models endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500
        
@app.route('/admin/debug/oauth-login')
def admin_debug_oauth_login():
    """Debug endpoint to test Google OAuth login without going through the full flow"""
    try:
        # Create a mock user for testing
        test_email = 'test@example.com'
        
        # Check if test user exists
        test_user = User.query.filter_by(email=test_email).first()
        
        if not test_user:
            # Create test user
            test_user = User(
                email=test_email,
                username='test-user',
                oauth_provider='google',
                oauth_id='test123',
                stripe_price_id='price_1RbX0yQWUhVa3vgDB8vGzoFN',
                subscription_price=4.00
            )
            test_user.set_password('')  # Empty password for OAuth users
            
            # Make sure we have a valid database session
            if not db.session.is_active:
                logger.warning("Database session is not active during test user creation, creating new session")
                db.session = db.create_scoped_session()
                
            db.session.add(test_user)
            db.session.commit()
            logger.info(f"Created test user with ID: {test_user.id}")
        
        # Try to log in the test user
        try:
            # Make sure the user object is attached to the current session
            if hasattr(test_user, '_sa_instance_state') and test_user._sa_instance_state.session is not db.session:
                logger.warning("Test user object is not attached to the current session, merging")
                test_user = db.session.merge(test_user)
                
            # Try to log in the user
            login_success = login_user(test_user)
            logger.info(f"Test user login_user result: {login_success}")
            
            if login_success:
                # For backward compatibility, also set session variables
                session['user_id'] = test_user.id
                session['email'] = test_user.email
                session['username'] = test_user.username
                
                return jsonify({
                    'success': True,
                    'message': 'Test user logged in successfully',
                    'user_id': test_user.id,
                    'email': test_user.email,
                    'username': test_user.username,
                    'session_variables': {
                        'user_id': session.get('user_id'),
                        'email': session.get('email'),
                        'username': session.get('username')
                    }
                })
            else:
                return jsonify({
                    'success': False,
                    'message': 'login_user() returned False',
                    'user_details': {
                        'id': test_user.id,
                        'email': test_user.email,
                        'username': test_user.username
                    }
                }), 500
        except Exception as login_error:
            logger.error(f"Error during test user login: {str(login_error)}")
            logger.error(traceback.format_exc())
            return jsonify({
                'success': False,
                'message': f'Error during login: {str(login_error)}',
                'traceback': traceback.format_exc()
            }), 500
    except Exception as e:
        logger.error(f"Error in debug OAuth login endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'message': f'Error: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/fix-leaderboard-period-length')
@login_required
def admin_fix_leaderboard_period_length():
    """Widen leaderboard_cache.period column to support keys like '1D_large_cap'.

    Some production databases have leaderboard_cache.period defined as VARCHAR(10),
    which truncates values such as '1D_large_cap'. This endpoint alters the column to
    VARCHAR(50) to safely store period_category keys.
    """
    try:
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403

        from sqlalchemy import text

        # Inspect current column length
        col = db.session.execute(text("""
            SELECT character_maximum_length
            FROM information_schema.columns
            WHERE table_name = 'leaderboard_cache' AND column_name = 'period'
        """)).fetchone()

        before_len = col[0] if col else None

        # Alter to VARCHAR(50) if needed
        if before_len is None or before_len < 50:
            db.session.execute(text("""
                ALTER TABLE leaderboard_cache
                ALTER COLUMN period TYPE VARCHAR(50)
            """))
            db.session.commit()
            return jsonify({'success': True, 'message': 'leaderboard_cache.period widened to VARCHAR(50)', 'before_length': before_len, 'after_length': 50})
        else:
            return jsonify({'success': True, 'message': 'No change needed; sufficient length', 'current_length': before_len})

    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({'success': False, 'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/debug/oauth-session')
def admin_debug_oauth_session():
    """Debug endpoint to check Flask-Login session state"""
    try:
        # Get current user info
        current_user_info = {
            'is_authenticated': current_user.is_authenticated if hasattr(current_user, 'is_authenticated') else False,
            'session_vars': {
                'user_id': session.get('user_id'),
                'email': session.get('email'),
                'username': session.get('username')
            },
            'flask_login_user': str(current_user) if hasattr(current_user, 'id') else 'No current_user',
            'request_cookies': dict(request.cookies)
        }
        
        # Check if session is working
        test_key = str(uuid.uuid4())
        session['test_key'] = test_key
        session_test = {'set': test_key, 'retrieved': session.get('test_key')}
        
        return jsonify({
            'success': True,
            'current_user': current_user_info,
            'session_test': session_test,
            'app_config': {
                'secret_key_set': app.secret_key is not None,
                'session_cookie_name': app.config.get('SESSION_COOKIE_NAME'),
                'session_cookie_secure': app.config.get('SESSION_COOKIE_SECURE'),
                'session_cookie_domain': app.config.get('SESSION_COOKIE_DOMAIN'),
                'session_cookie_path': app.config.get('SESSION_COOKIE_PATH'),
                'remember_cookie_duration': str(app.config.get('REMEMBER_COOKIE_DURATION')),
                'login_view': login_manager._login_view if hasattr(login_manager, '_login_view') else None
            }
        })
    except Exception as e:
        logger.error(f"Error in debug OAuth session endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'message': f'Error: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/debug/admin-check')
def admin_debug_admin_check():
    """Debug endpoint to check if admin user exists"""
    try:
        admin_user = User.query.filter_by(email=ADMIN_EMAIL).first()
        if admin_user:
            # Don't return the actual password hash for security reasons
            return jsonify({
                'admin_user_exists': True,
                'username': admin_user.username,
                'has_password_hash': bool(admin_user.password_hash),
                'password_hash_length': len(admin_user.password_hash) if admin_user.password_hash else 0
            })
        else:
            return jsonify({'admin_user_exists': False})
    except Exception as e:
        logger.error(f"Error in debug users endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500
        
@app.route('/admin/reset-admin-password')
def reset_admin_password():
    """Reset the admin password - only accessible from localhost"""
    try:
        # Only allow access from localhost for security
        if request.remote_addr != '127.0.0.1':
            return jsonify({'error': 'This endpoint can only be accessed from localhost'}), 403
            
        # Find admin user or create if doesn't exist
        admin_user = User.query.filter_by(email=ADMIN_EMAIL).first()
        
        if not admin_user:
            # Create admin user if it doesn't exist
            admin_user = User(username=ADMIN_USERNAME, email=ADMIN_EMAIL)
            db.session.add(admin_user)
            
        # Set a new password
        new_password = 'admin123'
        admin_user.set_password(new_password)
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f"Admin password reset successfully. Username: witty-raven, Password: {new_password}"
        })
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error resetting admin password: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500
    try:
        # Collect environment information
        debug_info = {
            'vercel_env': os.environ.get('VERCEL_ENV', 'Not set'),
            'database_url_exists': bool(os.environ.get('DATABASE_URL')),
            'postgres_prisma_url_exists': bool(os.environ.get('POSTGRES_PRISMA_URL')),
            'effective_database_url_exists': bool(DATABASE_URL),
            'secret_key_exists': bool(os.environ.get('SECRET_KEY')),
            'python_version': sys.version,
            'current_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Test database connection
        try:
            # Try to query the database
            user_count = User.query.count()
            debug_info['database_connection'] = 'Success'
            debug_info['user_count'] = user_count
        except Exception as e:
            debug_info['database_connection'] = 'Failed'
            debug_info['database_error'] = str(e)
        
        return jsonify(debug_info)
    except Exception as e:
        return jsonify({
            'error': str(e),
            'type': str(type(e))
        }), 500

# Migration endpoint has been removed for security reasons after successful database schema update
# The migration added the following columns to the User table:
# - created_at (TIMESTAMP DEFAULT CURRENT_TIMESTAMP)
# - stripe_customer_id (VARCHAR(120))

# Cache for stock prices to avoid excessive API calls
stock_price_cache = {}
cache_duration = 90  # 90 seconds

def get_batch_stock_data(ticker_symbols):
    """Fetch multiple stock prices efficiently with caching and batch API calls."""
    from datetime import datetime
    
    current_time = datetime.now()
    result = {}
    tickers_to_fetch = []
    
    # No mock prices - only use real API data or cached data
    for ticker in ticker_symbols:
        ticker_upper = ticker.upper()
        
        # Check if we have fresh cached data (< 90 seconds)
        if ticker_upper in stock_price_cache:
            cached_data = stock_price_cache[ticker_upper]
            cache_time = cached_data.get('timestamp')
            if cache_time and (current_time - cache_time).total_seconds() < cache_duration:
                result[ticker_upper] = cached_data['price']
                continue
        
        # Need to fetch this ticker
        tickers_to_fetch.append(ticker_upper)
    
    # If we need to fetch any tickers, make batch API call
    if tickers_to_fetch:
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not api_key:
            logger.warning("Alpha Vantage API key not found, cannot fetch stock prices")
            # Do not add any prices if API key is missing - only use real data
        else:
            # Make individual API calls for all tickers (no mock fallback)
            for ticker in tickers_to_fetch:
                try:
                    url = f'https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={ticker}&apikey={api_key}'
                    response = requests.get(url, timeout=3)  # Reduced timeout
                    data = response.json()
                    
                    if 'Global Quote' in data and '05. price' in data['Global Quote']:
                        price = float(data['Global Quote']['05. price'])
                        stock_price_cache[ticker] = {'price': price, 'timestamp': current_time}
                        result[ticker] = price
                        
                        # Log successful API call
                        try:
                            from models import AlphaVantageAPILog
                            api_log = AlphaVantageAPILog(
                                endpoint='GLOBAL_QUOTE',
                                symbol=ticker,
                                response_status='success',
                                timestamp=current_time
                            )
                            db.session.add(api_log)
                            # Don't commit immediately - batch commits for better performance
                        except Exception as log_error:
                            logger.error(f"Error logging API call: {log_error}")
                    else:
                        logger.warning(f"Could not get price for {ticker} from API - no fallback used")
                        # Log failed API call
                        try:
                            from models import AlphaVantageAPILog
                            api_log = AlphaVantageAPILog(
                                endpoint='GLOBAL_QUOTE',
                                symbol=ticker,
                                response_status='error',
                                timestamp=current_time
                            )
                            db.session.add(api_log)
                            # Don't commit immediately - batch commits for better performance
                        except Exception as log_error:
                            logger.error(f"Error logging API call: {log_error}")
                        
                except Exception as e:
                    logger.error(f"Error fetching {ticker}: {e}")
                    # Log failed API call
                    try:
                        from models import AlphaVantageAPILog
                        api_log = AlphaVantageAPILog(
                            endpoint='GLOBAL_QUOTE',
                            symbol=ticker,
                            response_status='error',
                            timestamp=current_time
                        )
                        db.session.add(api_log)
                        # Don't commit immediately - batch commits for better performance
                    except Exception as log_error:
                        logger.error(f"Error logging API call: {log_error}")
    
    # Batch commit all API logs at the end for better performance
    try:
        db.session.commit()
    except Exception as e:
        logger.error(f"Error committing API logs: {e}")
        db.session.rollback()
    
    return result

def get_stock_data(ticker_symbol):
    """Fetches single stock data - wrapper around batch function for backward compatibility."""
    batch_result = get_batch_stock_data([ticker_symbol])
    ticker_upper = ticker_symbol.upper()
    if ticker_upper in batch_result:
        return {'price': batch_result[ticker_upper]}
    else:
        # Return None if no real price available
        return None

# HTML Templates for core functionality
LOGIN_HTML = """
<!DOCTYPE html>
<html>
<head>
    <title>Login - ApesTogether</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; background-color: #f4f4f4; }
        .container { max-width: 500px; margin: 0 auto; background: white; padding: 20px; border-radius: 5px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
        h1 { color: #333; text-align: center; }
        .form-group { margin-bottom: 15px; }
        label { display: block; margin-bottom: 5px; font-weight: bold; }
        input[type="email"], input[type="password"] { width: 100%; padding: 10px; border: 1px solid #ddd; border-radius: 3px; box-sizing: border-box; }
        .btn { display: inline-block; background: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 3px; border: none; cursor: pointer; font-size: 16px; }
        .btn-block { width: 100%; }
        .alert { padding: 10px; margin-bottom: 15px; border-radius: 3px; }
        .alert-danger { background-color: #f8d7da; color: #721c24; }
        .alert-success { background-color: #d4edda; color: #155724; }
        .text-center { text-align: center; }
        .mt-3 { margin-top: 15px; }
        .oauth-buttons { margin-top: 20px; border-top: 1px solid #ddd; padding-top: 20px; }
        .btn-google { background-color: #4285F4; }
        .btn-apple { background-color: #000; }
        .btn-oauth { width: 100%; margin-bottom: 10px; color: white; text-align: center; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Login</h1>
        
        {% with messages = get_flashed_messages(with_categories=true) %}
            {% if messages %}
                {% for category, message in messages %}
                    <div class="alert alert-{{ category }}">{{ message }}</div>
                {% endfor %}
            {% endif %}
        {% endwith %}
        
        <form method="POST" action="/login">
            <div class="form-group">
                <label for="email">Email:</label>
                <input type="email" id="email" name="email" required>
            </div>
            <div class="form-group">
                <label for="password">Password:</label>
                <input type="password" id="password" name="password" required>
            </div>
            <button type="submit" class="btn btn-block">Login</button>
        </form>
        
        <div class="oauth-buttons">
            <p class="text-center">Or login with:</p>
            <a href="/login/google" class="btn btn-oauth btn-google">Login with Google</a>
            <a href="/login/apple" class="btn btn-oauth btn-apple">Login with Apple</a>
        </div>
        
        <div class="text-center mt-3">
            <p>Don't have an account? <a href="/register">Register</a></p>
            <p><a href="/">Back to Home</a></p>
        </div>
    </div>
</body>
</html>
"""

REGISTER_HTML = """
<!DOCTYPE html>
<html>
<head>
    <title>Register - ApesTogether</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; background-color: #f4f4f4; }
        .container { max-width: 500px; margin: 0 auto; background: white; padding: 20px; border-radius: 5px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
        h1 { color: #333; text-align: center; }
        .form-group { margin-bottom: 15px; }
        label { display: block; margin-bottom: 5px; font-weight: bold; }
        input[type="text"], input[type="email"], input[type="password"] { width: 100%; padding: 10px; border: 1px solid #ddd; border-radius: 3px; box-sizing: border-box; }
        .btn { display: inline-block; background: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; border-radius: 3px; border: none; cursor: pointer; font-size: 16px; }
        .btn-block { width: 100%; }
        .alert { padding: 10px; margin-bottom: 15px; border-radius: 3px; }
        .alert-danger { background-color: #f8d7da; color: #721c24; }
        .alert-success { background-color: #d4edda; color: #155724; }
        .text-center { text-align: center; }
        .mt-3 { margin-top: 15px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Register</h1>
        
        {% with messages = get_flashed_messages(with_categories=true) %}
            {% if messages %}
                {% for category, message in messages %}
                    <div class="alert alert-{{ category }}">{{ message }}</div>
                {% endfor %}
            {% endif %}
        {% endwith %}
        
        <form method="POST" action="/register">
            <div class="form-group">
                <label for="username">Username:</label>
                <input type="text" id="username" name="username" required>
            </div>
            <div class="form-group">
                <label for="email">Email:</label>
                <input type="email" id="email" name="email" required>
            </div>
            <div class="form-group">
                <label for="password">Password:</label>
                <input type="password" id="password" name="password" required>
            </div>
            <button type="submit" class="btn btn-block">Register</button>
        </form>
        
        <div class="text-center mt-3">
            <p>Already have an account? <a href="/login">Login</a></p>
            <p><a href="/">Back to Home</a></p>
        </div>
    </div>
</body>
</html>
"""

DASHBOARD_HTML = """
<!DOCTYPE html>
<html>
<head>
    <title>Dashboard - ApesTogether</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 0; background-color: #f4f4f4; }
        .container { width: 80%; margin: 0 auto; background: white; padding: 20px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
        .header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 20px; padding-bottom: 10px; border-bottom: 1px solid #eee; }
        .nav a { margin-left: 15px; text-decoration: none; color: #333; }
        h1, h2 { color: #333; }
        .portfolio-summary { background: #f9f9f9; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
        .stocks-table { width: 100%; border-collapse: collapse; }
        .stocks-table th, .stocks-table td { padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }
        .stocks-table th { background-color: #f2f2f2; }
        .stocks-table tr:hover { background-color: #f5f5f5; }
        .add-stock-form { background: #f9f9f9; padding: 15px; border-radius: 5px; margin-top: 20px; }
        .form-group { margin-bottom: 15px; }
        label { display: block; margin-bottom: 5px; }
        input[type="text"], input[type="number"] { width: 100%; padding: 8px; box-sizing: border-box; }
        .btn { display: inline-block; background: #4CAF50; color: white; padding: 10px 15px; text-decoration: none; border-radius: 3px; border: none; cursor: pointer; }
        .btn-danger { background: #f44336; }
        .alert { padding: 10px; margin-bottom: 15px; border-radius: 3px; }
        .alert-danger { background-color: #f8d7da; color: #721c24; }
        .alert-success { background-color: #d4edda; color: #155724; }
        .profit { color: green; }
        .loss { color: red; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Dashboard</h1>
            <div class="nav">
                <a href="/">Home</a>
                <a href="/logout">Logout</a>
            </div>
        </div>
        
        {% with messages = get_flashed_messages(with_categories=true) %}
            {% if messages %}
                {% for category, message in messages %}
                    <div class="alert alert-{{ category }}">{{ message }}</div>
                {% endfor %}
            {% endif %}
        {% endwith %}
        
        <div class="portfolio-summary">
            <h2>Welcome, {{ user.username }}!</h2>
            <p>Your portfolio summary:</p>
            <p><strong>Total Stocks:</strong> {{ stocks|length }}</p>
            {% if stocks %}
                {% set total_value = 0 %}
                {% set total_cost = 0 %}
                {% for stock in stocks %}
                    {% set total_value = total_value + stock.current_value() %}
                    {% set total_cost = total_cost + (stock.purchase_price * stock.quantity) %}
                {% endfor %}
                <p><strong>Total Value:</strong> ${{ "%.2f"|format(total_value) }}</p>
                <p><strong>Total Cost:</strong> ${{ "%.2f"|format(total_cost) }}</p>
                {% set total_profit_loss = total_value - total_cost %}
                <p><strong>Total Profit/Loss:</strong> 
                    <span class="{% if total_profit_loss >= 0 %}profit{% else %}loss{% endif %}">
                        ${{ "%.2f"|format(total_profit_loss) }}
                        ({{ "%.2f"|format((total_profit_loss / total_cost) * 100) }}%)
                    </span>
                </p>
            {% endif %}
        </div>
        
        <h2>Your Stocks</h2>
        {% if stocks %}
            <table class="stocks-table">
                <thead>
                    <tr>
                        <th>Ticker</th>
                        <th>Quantity</th>
                        <th>Purchase Price</th>
                        <th>Purchase Date</th>
                        <th>Current Value</th>
                        <th>Profit/Loss</th>
                        <th>Actions</th>
                    </tr>
                </thead>
                <tbody>
                    {% for stock in stocks %}
                        <tr>
                            <td>{{ stock.ticker }}</td>
                            <td>{{ stock.quantity }}</td>
                            <td>${{ "%.2f"|format(stock.purchase_price) }}</td>
                            <td>{{ stock.purchase_date.strftime('%Y-%m-%d') }}</td>
                            <td>${{ "%.2f"|format(stock.current_value()) }}</td>
                            {% set profit_loss = stock.profit_loss() %}
                            <td class="{% if profit_loss >= 0 %}profit{% else %}loss{% endif %}">
                                ${{ "%.2f"|format(profit_loss) }}
                                ({{ "%.2f"|format((profit_loss / (stock.purchase_price * stock.quantity)) * 100) }}%)
                            </td>
                            <td>
                                <form action="/delete_stock/{{ stock.id }}" method="POST" style="display:inline;">
                                    <button type="submit" class="btn btn-danger">Delete</button>
                                </form>
                            </td>
                        </tr>
                    {% endfor %}
                </tbody>
            </table>
        {% else %}
            <p>You don't have any stocks yet. Add some below!</p>
        {% endif %}
        
        <div class="add-stock-form">
            <h2>Add New Stock</h2>
            <form action="/add_stock" method="POST">
                <div class="form-group">
                    <label for="ticker">Ticker Symbol:</label>
                    <input type="text" id="ticker" name="ticker" required>
                </div>
                <div class="form-group">
                    <label for="quantity">Quantity:</label>
                    <input type="number" id="quantity" name="quantity" step="0.01" min="0.01" required>
                </div>
                <div class="form-group">
                    <label for="purchase_price">Purchase Price ($):</label>
                    <input type="number" id="purchase_price" name="purchase_price" step="0.01" min="0.01" required>
                </div>
                <button type="submit" class="btn">Add Stock</button>
            </form>
        </div>
    </div>
</body>
</html>
"""

# Serve favicon directly to avoid session issues
@app.route('/favicon.png')
def serve_favicon():
    try:
        return send_from_directory(app.static_folder, 'favicon.png')
    except Exception as e:
        logger.error(f"Error serving favicon: {str(e)}")
        return '', 404

# Health check endpoints
@app.route('/api/health')
def health_check():
    try:
        # Check database connectivity
        db_status = False
        version = None
        try:
            result = db.session.execute(text('SELECT version()'))
            version = result.scalar()
            db_status = True
        except Exception as e:
            logger.error(f"Database health check failed: {str(e)}")
        
        # Return health status
        return jsonify({
            'status': 'ok',
            'timestamp': datetime.now().isoformat(),
            'database': {
                'connected': db_status,
                'version': version
            },
            'environment': os.environ.get('VERCEL_ENV', 'development')
        })
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return jsonify({'status': 'error', 'message': 'Health check failed'}), 500

# Root health check endpoint
@app.route('/health')
def root_health_check():
    try:
        # Simple health check that doesn't access the database
        return jsonify({
            'status': 'ok',
            'timestamp': datetime.now().isoformat(),
            'environment': os.environ.get('VERCEL_ENV', 'development')
        })
    except Exception as e:
        logger.error(f"Root health check failed: {str(e)}")
        return jsonify({'status': 'error', 'message': 'Health check failed'}), 500

@app.route('/')
def index():
    """Main landing page - redirect to 5D leaderboard for public access"""
    try:
        # Redirect to 5D leaderboard as the default homepage
        return redirect(url_for('leaderboard.leaderboard_home', period='5D', category='all'))
    except Exception as e:
        logger.error(f"Error in index route redirect: {str(e)}")
        logger.error(traceback.format_exc())
        # Fallback to direct leaderboard URL if blueprint routing fails
        return redirect('/leaderboard/?period=5D&category=all')

@app.route('/login', methods=['GET', 'POST'])
def login():
    """User login page"""
    try:
        if request.method == 'POST':
            email = request.form.get('email')
            password = request.form.get('password')
            
            logger.info(f"Login attempt for email: {email}")
            
            try:
                user = User.query.filter_by(email=email).first()
                logger.info(f"User found: {user is not None}")
                
                if user:
                    password_check = user.check_password(password)
                    logger.info(f"Password check result: {password_check}")
                    
                    if password_check:
                        # Use Flask-Login to handle user session
                        login_user(user)
                        logger.info(f"User logged in successfully: {user.username}")
                        flash('Login successful!', 'success')
                        
                        # Log login activity
                        try:
                            from models import UserActivity
                            activity = UserActivity(
                                user_id=user.id,
                                activity_type='login',
                                ip_address=request.remote_addr,
                                user_agent=request.headers.get('User-Agent', '')[:255]
                            )
                            db.session.add(activity)
                            db.session.commit()
                            db.session.flush()  # Ensure record is immediately visible
                            logger.info(f"Successfully logged login activity for user {user.id}")
                        except Exception as e:
                            logger.error(f"Error logging login activity: {str(e)}")
                            logger.error(traceback.format_exc())
                            db.session.rollback()
                        
                        # For backward compatibility, also set session variables
                        session['user_id'] = user.id
                        session['email'] = user.email
                        session['username'] = user.username
                        
                        # Redirect to next page or dashboard
                        next_page = request.args.get('next')
                        return redirect(next_page or url_for('dashboard'))
                    else:
                        logger.warning(f"Invalid password for user: {email}")
                        flash('Invalid email or password', 'danger')
                else:
                    logger.warning(f"User not found with email: {email}")
                    flash('Invalid email or password', 'danger')
            except Exception as e:
                logger.error(f"Error during user lookup or password check: {str(e)}")
                logger.error(traceback.format_exc())
                flash('An error occurred during login. Please try again.', 'danger')
        
    except Exception as e:
        logger.error(f"Unexpected error in login route: {str(e)}")
        logger.error(traceback.format_exc())
        flash('An unexpected error occurred. Please try again later.', 'danger')
    
    return render_template_with_defaults('login.html')

@app.route('/terms-of-service')
def terms_of_service():
    """Terms of Service page"""
    return render_template_with_defaults('terms_of_service.html')

@app.route('/privacy-policy')
def privacy_policy():
    """Privacy Policy page"""
    return render_template_with_defaults('privacy_policy.html')

@app.route('/register', methods=['GET', 'POST'])
def register():
    """User registration page"""
    if request.method == 'POST':
        username = request.form.get('username')
        email = request.form.get('email')
        password = request.form.get('password')
        
        # Check if user already exists
        existing_user = User.query.filter((User.username == username) | (User.email == email)).first()
        if existing_user:
            flash('Username or email already exists', 'danger')
            return redirect(url_for('register'))
        
        # Create new user
        new_user = User(username=username, email=email)
        new_user.set_password(password)
        
        try:
            db.session.add(new_user)
            db.session.commit()
            
            # Log the user in
            session['user_id'] = new_user.id
            session['email'] = new_user.email
            session['username'] = new_user.username
            
            flash('Registration successful!', 'success')
            return redirect(url_for('dashboard'))
        except Exception as e:
            db.session.rollback()
            flash(f'Error creating account: {str(e)}', 'danger')
    
    return render_template_with_defaults('register.html')

@app.route('/logout')
def logout():
    """Logout the current user"""
    # Use Flask-Login to handle logout
    logout_user()
    
    # For backward compatibility, also clear session variables
    session.pop('user_id', None)
    session.pop('email', None)
    session.pop('username', None)
    
    flash('You have been logged out', 'success')
    return redirect(url_for('index'))

@app.route('/login/google')
def login_google():
    """Redirect to Google for OAuth login"""
    try:
        logger.info("Starting Google OAuth login")
        redirect_uri = url_for('authorize_google', _external=True)
        logger.info(f"Redirect URI: {redirect_uri}")
        return google.authorize_redirect(redirect_uri)
    except Exception as e:
        logger.error(f"Error in Google OAuth redirect: {str(e)}")
        logger.error(traceback.format_exc())
        flash('Error connecting to Google. Please try again.', 'danger')
        return redirect(url_for('login'))

@app.route('/admin/debug/flask-login')
def admin_debug_flask_login():
    """Debug endpoint to check Flask-Login configuration"""
    try:
        # Test database connection
        db_test_result = {}
        try:
            # Check if we can query users
            user_count = User.query.count()
            db_test_result['user_count'] = user_count
            db_test_result['connection'] = 'Success'
            
            # Check if load_user works
            if user_count > 0:
                first_user = User.query.first()
                if first_user:
                    test_user = load_user(first_user.id)
                    db_test_result['load_user'] = {
                        'success': test_user is not None,
                        'user_id': test_user.id if test_user else None
                    }
        except Exception as db_error:
            db_test_result['connection'] = 'Failed'
            db_test_result['error'] = str(db_error)
        
        # Check Flask-Login configuration
        login_config = {
            'login_manager': {
                'login_view': login_manager._login_view if hasattr(login_manager, '_login_view') else None,
                'login_message': login_manager._login_message if hasattr(login_manager, '_login_message') else None,
                'session_protection': login_manager.session_protection,
                'anonymous_user': str(login_manager.anonymous_user),
                'user_callback': login_manager._user_callback.__name__ if login_manager._user_callback else None
            },
            'current_user': {
                'is_authenticated': current_user.is_authenticated if hasattr(current_user, 'is_authenticated') else False,
                'is_active': current_user.is_active if hasattr(current_user, 'is_active') else False,
                'is_anonymous': current_user.is_anonymous if hasattr(current_user, 'is_anonymous') else True,
                'get_id': current_user.get_id() if hasattr(current_user, 'get_id') else None
            },
            'session': {
                'keys': list(session.keys()) if session else [],
                'user_id': session.get('user_id'),
                'email': session.get('email'),
                'username': session.get('username'),
                '_user_id': session.get('_user_id'),  # Flask-Login session key
                '_id': session.get('_id'),  # Session ID
                '_fresh': session.get('_fresh')  # Flask-Login session freshness
            }
        }
        
        return jsonify({
            'success': True,
            'db_test': db_test_result,
            'flask_login_config': login_config,
            'request_info': {
                'cookies': dict(request.cookies),
                'headers': dict(request.headers),
                'is_secure': request.is_secure,
                'host': request.host
            }
        })
    except Exception as e:
        logger.error(f"Error in debug Flask-Login endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'message': f'Error: {str(e)}',
            'traceback': traceback.format_exc()
        }), 500

@app.route('/login/google/authorize')
def authorize_google():
    """Handle the callback from Google OAuth"""
    # Initialize user variable at the beginning to avoid UnboundLocalError
    user = None
    
    try:
        # Step 1: Get token and user info from Google
        token = google.authorize_access_token()
        user_info = token.get('userinfo')
        
        # Log the token structure to help diagnose issues
        logger.info(f"OAuth token keys present: {', '.join(token.keys()) if token else 'None'}")
        
        # If userinfo is not directly available, try to extract from id_token
        if not user_info or 'email' not in user_info:
            logger.info("No direct userinfo, trying to extract from id_token")
            if 'id_token' in token:
                try:
                    # This is for debugging purposes only
                    id_token_payload = jwt.decode(token['id_token'], options={"verify_signature": False})
                    logger.info(f"Decoded id_token payload keys: {', '.join(id_token_payload.keys()) if id_token_payload else 'Empty'}")
                    if id_token_payload and 'email' in id_token_payload:
                        logger.info(f"Found email in id_token: {id_token_payload['email'].split('@')[0]}[REDACTED]")
                        user_info = id_token_payload
                except Exception as jwt_error:
                    logger.error(f"Error decoding id_token: {str(jwt_error)}")
                    logger.error(traceback.format_exc())
            
            # Check other common token keys
            if not user_info or 'email' not in user_info:
                for key in ['user', 'profile', 'info']:
                    if key in token and isinstance(token[key], dict):
                        logger.info(f"Checking token['{key}']: {', '.join(token[key].keys()) if token[key] else 'Empty'}")
                        if 'email' in token[key]:
                            logger.info(f"Found email in token['{key}']: {token[key]['email'].split('@')[0]}[REDACTED]")
                            user_info = token[key]
                            break
        
        # If we still don't have user info, try direct API call
        if not user_info or 'email' not in user_info:
            logger.info("Attempting to get userinfo from google.get('userinfo')")
            try:
                user_info = google.get('userinfo')
                logger.info(f"Got user_info from google.get('userinfo'): {user_info.keys() if user_info else 'Empty'}")
            except Exception as api_error:
                logger.error(f"Error getting user_info from google.get('userinfo'): {str(api_error)}")
        
        # Last resort: direct API call with access token
        if not user_info or 'email' not in user_info:
            logger.info("Last resort: Trying direct Google API call with access token")
            try:
                if 'access_token' in token:
                    import requests
                    headers = {'Authorization': f'Bearer {token["access_token"]}'}  
                    userinfo_response = requests.get('https://www.googleapis.com/oauth2/v3/userinfo', headers=headers)
                    
                    if userinfo_response.status_code == 200:
                        user_info = userinfo_response.json()
                        logger.info(f"Last resort user info: {user_info.keys() if user_info else 'Empty'}")
                    else:
                        logger.error(f"Last resort API call failed with status {userinfo_response.status_code}")
                else:
                    logger.error("No access token available for last resort attempt")
            except Exception as last_error:
                logger.error(f"Error in last resort attempt: {str(last_error)}")
        
        # Final validation that we have the required user information
        if not user_info or 'email' not in user_info:
            logger.error(f"Missing email in user_info after all attempts: {user_info}")
            flash('Could not retrieve your email from Google. Please try again or use another login method.', 'danger')
            return redirect(url_for('login'))
            
        logger.info(f"User email found: {user_info.get('email').split('@')[0]}[REDACTED]")
        
        # Step 2: Check if user exists in our database
        email = user_info.get('email')
        
        # Make sure we have a valid database session
        if not db.session.is_active:
            logger.warning("Database session is not active, creating new session")
            db.session = db.create_scoped_session()
            
        # Try to find the user
        try:
            user = User.query.filter_by(email=email).first()
            logger.info(f"User exists in database: {user is not None}")
        except Exception as user_query_error:
            logger.error(f"Error querying user: {str(user_query_error)}")
            # Try to reconnect to the database
            db.session.remove()
            db.session = db.create_scoped_session()
            # Try one more time
            user = User.query.filter_by(email=email).first()
            logger.info(f"User exists in database (after reconnect): {user is not None}")
        
        # Step 3: Create new user if not found
        if not user:
            # Generate a unique random username
            while True:
                adjectives = ['clever', 'brave', 'sharp', 'wise', 'happy', 'lucky', 'sunny', 'proud', 'witty', 'gentle']
                nouns = ['fox', 'lion', 'eagle', 'tiger', 'river', 'ocean', 'bear', 'wolf', 'horse', 'raven']
                adjective = random.choice(adjectives)
                noun = random.choice(nouns)
                username = f"{adjective}-{noun}"
                try:
                    if not User.query.filter_by(username=username).first():
                        break
                except Exception as username_error:
                    logger.error(f"Error checking username uniqueness: {str(username_error)}")
                    # Use a timestamp-based username as a fallback
                    username = f"user-{int(time.time())}"
                    break

            # Create new user
            logger.info(f"Creating new OAuth user with email: {user_info['email'].split('@')[0]}[REDACTED] and username: {username}")
            user = User(
                email=user_info['email'],
                username=username,
                oauth_provider='google',
                oauth_id=user_info.get('sub', user_info.get('id', str(uuid.uuid4()))),  # Use sub, id, or generate UUID as fallback
                stripe_price_id='price_1RbX0yQWUhVa3vgDB8vGzoFN',  # Default $4 price
                subscription_price=4.00
            )
            user.set_password('')  # Empty password for OAuth users
            
            # Make sure we have a valid database session
            if not db.session.is_active:
                logger.warning("Database session is not active during user creation, creating new session")
                db.session = db.create_scoped_session()
                
            db.session.add(user)
            db.session.commit()
            logger.info(f"Successfully created new OAuth user with ID: {user.id}")
        
        # Step 4: Log the user in using Flask-Login
        # Double check we have a valid user object
        if not user:
            logger.error("User object is None after all attempts to find or create it")
            flash('Error logging in. Please try again later.', 'danger')
            return redirect(url_for('login'))
                
        # Make sure the user object is attached to the current session
        if hasattr(user, '_sa_instance_state') and user._sa_instance_state.session is not db.session:
            logger.warning("User object is not attached to the current session, merging")
            user = db.session.merge(user)
                
        # Try to log in the user
        try:
            login_success = login_user(user)
            logger.info(f"OAuth user login_user result: {login_success}")
            
            if login_success:
                logger.info(f"OAuth user logged in successfully: {user.username}")
                
                # For backward compatibility, also set session variables
                session['user_id'] = user.id
                session['email'] = user.email
                session['username'] = user.username
            else:
                logger.error("login_user returned False, falling back to session-based auth")
                # Fall back to session-based auth if Flask-Login fails
                session['user_id'] = user.id
                session['email'] = user.email
                session['username'] = user.username
        except Exception as login_error:
            logger.error(f"Error during OAuth login_user: {str(login_error)}")
            logger.error(traceback.format_exc())
            # Fall back to session-based auth if Flask-Login fails
            session['user_id'] = user.id
            session['email'] = user.email
            session['username'] = user.username
        
        # Step 5: Check if this is the user's first login (no stocks yet)
        try:
            if Stock.query.filter_by(user_id=user.id).count() == 0:
                flash('Welcome! Please add some stocks to your portfolio.', 'info')
            else:
                flash('Login successful!', 'success')
        except Exception as stock_check_error:
            logger.error(f"Error checking user's stocks: {str(stock_check_error)}")
            flash('Login successful!', 'success')
            
        # Step 6: Final redirect - check if user is admin
        if user.is_admin:
            logger.info(f"Admin user {user.email} logged in, redirecting to admin dashboard")
            return redirect(url_for('admin_dashboard'))
        else:
            return redirect(url_for('dashboard'))
            
    except Exception as general_error:
        # Catch-all error handler
        logger.error(f"Unexpected error in Google OAuth flow: {str(general_error)}")
        logger.error(traceback.format_exc())
        flash('An unexpected error occurred during login. Please try again later.', 'danger')
        return redirect(url_for('login'))

@app.route('/login/apple')
def login_apple():
    """Redirect to Apple OAuth login"""
    redirect_uri = url_for('authorize_apple', _external=True)
    return apple.authorize_redirect(redirect_uri)

@app.route('/login/apple/authorize')
def authorize_apple():
    """Handle Apple OAuth callback"""
    try:
        token = apple.authorize_access_token()
        user_info = token.get('userinfo')
        
        # Check if user exists
        user = User.query.filter_by(email=user_info['email']).first()
        
        if not user:
            # Generate a unique random username
            while True:
                adjectives = ['clever', 'brave', 'sharp', 'wise', 'happy', 'lucky', 'sunny', 'proud', 'witty', 'gentle']
                nouns = ['fox', 'lion', 'eagle', 'tiger', 'river', 'ocean', 'bear', 'wolf', 'horse', 'raven']
                adjective = random.choice(adjectives)
                noun = random.choice(nouns)
                username = f"{adjective}-{noun}"
                if not User.query.filter_by(username=username).first():
                    break

            # Create new user
            user = User(
                email=user_info['email'],
                username=username,
                oauth_provider='apple',
                oauth_id=user_info['sub'],
                stripe_price_id='price_1RbX0yQWUhVa3vgDB8vGzoFN',  # Default $4 price
                subscription_price=4.00
            )
            user.set_password('') # Empty password for OAuth users
            db.session.add(user)
            db.session.commit()
        
        # Log the user in using Flask-Login
        try:
            login_user(user)
            logger.info(f"Apple OAuth user logged in successfully: {user.username}")
            
            # For backward compatibility, also set session variables
            session['user_id'] = user.id
            session['email'] = user.email
            session['username'] = user.username
        except Exception as e:
            logger.error(f"Error during Apple OAuth login_user: {str(e)}")
            logger.error(traceback.format_exc())
            # Fall back to session-based auth if Flask-Login fails
            session['user_id'] = user.id
            session['email'] = user.email
            session['username'] = user.username
        
        # Check if this is the user's first login (no stocks yet)
        if Stock.query.filter_by(user_id=user.id).count() == 0:
            flash('Welcome! Please add some stocks to your portfolio.', 'info')
        else:
            flash('Login successful!', 'success')
            
        return redirect(url_for('dashboard'))
    except Exception as e:
        logger.error(f"Error in Apple OAuth: {e}")
        flash('Error logging in with Apple. Please try again or use email login.', 'danger')
        return redirect(url_for('login'))

# Dashboard route is now defined earlier in the file (around line 1087)
# The duplicate dashboard route definition was removed to fix the server error

@app.route('/add_stock', methods=['POST'])
def add_stock():
    """Add a stock to user's portfolio"""
    if 'user_id' not in session:
        flash('Please login to add stocks', 'warning')
        return redirect(url_for('login'))
    
    # Validate and parse form inputs
    ticker = request.form.get('ticker')
    if not ticker:
        flash('Ticker symbol is required', 'danger')
        return redirect(url_for('dashboard'))
    
    ticker = ticker.upper().strip()
    
    try:
        quantity = request.form.get('quantity')
        if not quantity:
            flash('Quantity is required', 'danger')
            return redirect(url_for('dashboard'))
        quantity = float(quantity)
        
        if quantity <= 0:
            flash('Quantity must be greater than zero', 'danger')
            return redirect(url_for('dashboard'))
        
    except ValueError as e:
        logger.error(f"Invalid quantity for add_stock: ticker={ticker}, quantity={request.form.get('quantity')}")
        flash(f'Invalid input: Please enter a valid number for quantity', 'danger')
        return redirect(url_for('dashboard'))
    
    # Fetch current stock price (with caching and API logic)
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        calculator = PortfolioPerformanceCalculator()
        stock_data = calculator.get_stock_data(ticker)
        
        if not stock_data or 'price' not in stock_data:
            flash(f'Could not fetch current price for {ticker}. Please check the ticker symbol and try again.', 'danger')
            logger.warning(f"Failed to fetch price for {ticker} when adding stock")
            return redirect(url_for('dashboard'))
        
        purchase_price = stock_data['price']
        logger.info(f"Fetched price for {ticker}: ${purchase_price:.2f}")
        
    except Exception as price_fetch_error:
        logger.error(f"Error fetching stock price for {ticker}: {str(price_fetch_error)}")
        flash(f'Error fetching stock price. Please try again later.', 'danger')
        return redirect(url_for('dashboard'))
    
    # Check if user already owns this stock
    existing_stock = Stock.query.filter_by(user_id=session['user_id'], ticker=ticker).first()
    
    try:
        if existing_stock:
            # Combine with existing position using weighted average cost basis
            old_cost_basis = existing_stock.quantity * existing_stock.purchase_price
            new_cost = quantity * purchase_price
            total_cost = old_cost_basis + new_cost
            total_quantity = existing_stock.quantity + quantity
            weighted_avg_price = total_cost / total_quantity
            
            logger.info(f"Combining with existing position: {existing_stock.quantity} @ ${existing_stock.purchase_price:.2f}")
            logger.info(f"New weighted average: {total_quantity} @ ${weighted_avg_price:.2f}")
            
            # Update existing stock
            existing_stock.quantity = total_quantity
            existing_stock.purchase_price = weighted_avg_price
            
            stock_to_commit = existing_stock
        else:
            # Create new stock
            new_stock = Stock(
                ticker=ticker,
                quantity=quantity,
                purchase_price=purchase_price,
                user_id=session['user_id']
            )
            db.session.add(new_stock)
            stock_to_commit = new_stock
            
            logger.info(f"Creating new stock position: {quantity} {ticker} @ ${purchase_price:.2f}")
        
        # Determine transaction type: 'initial' for first purchase, 'buy' for subsequent
        from models import Transaction
        existing_transactions = Transaction.query.filter_by(user_id=session['user_id']).count()
        transaction_type = 'initial' if existing_transactions == 0 else 'buy'
        
        # CRITICAL FIX: Process transaction and update cash tracking
        from cash_tracking import process_transaction
        cash_result = process_transaction(
            db=db,
            user_id=session['user_id'],
            ticker=ticker,
            quantity=quantity,
            price=purchase_price,
            transaction_type=transaction_type,
            timestamp=datetime.utcnow()
        )
        
        db.session.commit()
        
        # Auto-populate stock info for new stocks
        try:
            populate_single_stock_info(ticker.upper())
        except Exception as stock_info_error:
            logger.warning(f"Failed to populate stock info for {ticker}: {str(stock_info_error)}")
        
        # Recalculate portfolio stats immediately so dashboard updates
        try:
            from leaderboard_utils import calculate_user_portfolio_stats
            from models import UserPortfolioStats
            
            logger.info("Calculating portfolio stats after stock addition...")
            stats = calculate_user_portfolio_stats(session['user_id'])
            logger.info(f"Stats calculated: unique_stocks={stats['unique_stocks_count']}, trades/week={stats['avg_trades_per_week']:.1f}")
            
            # Convert cap percentages to market_cap_mix JSON
            market_cap_mix = {
                'small_cap': stats.get('small_cap_percent', 0),
                'large_cap': stats.get('large_cap_percent', 0)
            }
            
            user_stats = UserPortfolioStats.query.filter_by(user_id=session['user_id']).first()
            
            if user_stats:
                # Update existing stats
                logger.info(f"Updating existing stats for user {session['user_id']}")
                user_stats.unique_stocks_count = stats['unique_stocks_count']
                user_stats.avg_trades_per_week = stats['avg_trades_per_week']
                user_stats.market_cap_mix = market_cap_mix
                user_stats.industry_mix = stats.get('industry_mix', {})
                user_stats.subscriber_count = stats['subscriber_count']
                user_stats.updated_at = datetime.utcnow()
                db.session.merge(user_stats)  # Use merge for cross-session safety
            else:
                # Create new stats entry
                logger.info(f"Creating new stats entry for user {session['user_id']}")
                user_stats = UserPortfolioStats(
                    user_id=session['user_id'],
                    unique_stocks_count=stats['unique_stocks_count'],
                    avg_trades_per_week=stats['avg_trades_per_week'],
                    market_cap_mix=market_cap_mix,
                    industry_mix=stats.get('industry_mix', {}),
                    subscriber_count=stats['subscriber_count']
                )
                db.session.add(user_stats)
            
            db.session.commit()
            logger.info(f" Portfolio stats committed successfully")
        except Exception as stats_error:
            logger.error(f"ERROR updating portfolio stats: {str(stats_error)}")
            import traceback
            logger.error(traceback.format_exc())
            # Don't fail the whole operation if stats update fails
        
        flash(f'Added {quantity} shares of {ticker}', 'success')
        logger.info(f"Created stock + transaction: {quantity} {ticker} @ ${purchase_price}")
        logger.info(f"Cash tracking: max_deployed=${cash_result['max_cash_deployed']}, proceeds=${cash_result['cash_proceeds']}")
    except Exception as e:
        db.session.rollback()
        flash(f'Error adding stock: {str(e)}', 'danger')
        logger.error(f"Failed to add stock/transaction: {str(e)}")
    
    return redirect(url_for('dashboard'))

@app.route('/sell_stock', methods=['POST'])
def sell_stock():
    """Sell stock from user's portfolio"""
    if 'user_id' not in session:
        flash('Please login to sell stocks', 'warning')
        return redirect(url_for('login'))
    
    # Validate and parse form inputs
    ticker = request.form.get('ticker')
    if not ticker:
        flash('Ticker symbol is required', 'danger')
        return redirect(url_for('dashboard'))
    
    ticker = ticker.upper().strip()
    
    try:
        quantity = request.form.get('quantity')
        if not quantity:
            flash('Quantity is required', 'danger')
            return redirect(url_for('dashboard'))
        quantity = float(quantity)
        
        if quantity <= 0:
            flash('Quantity must be greater than zero', 'danger')
            return redirect(url_for('dashboard'))
        
    except ValueError as e:
        logger.error(f"Invalid quantity for sell_stock: ticker={ticker}, quantity={request.form.get('quantity')}")
        flash(f'Invalid input: Please enter a valid number for quantity', 'danger')
        return redirect(url_for('dashboard'))
    
    # Find the stock in user's portfolio
    stock = Stock.query.filter_by(user_id=session['user_id'], ticker=ticker).first()
    if not stock:
        flash(f'You do not own any shares of {ticker}', 'danger')
        return redirect(url_for('dashboard'))
    
    # Check if user has enough shares
    if stock.quantity < quantity:
        flash(f'You only have {stock.quantity} shares of {ticker}, cannot sell {quantity}', 'danger')
        return redirect(url_for('dashboard'))
    
    # Fetch current stock price (with caching and API logic)
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        calculator = PortfolioPerformanceCalculator()
        stock_data = calculator.get_stock_data(ticker)
        
        if not stock_data or 'price' not in stock_data:
            flash(f'Could not fetch current price for {ticker}. Please try again.', 'danger')
            logger.warning(f"Failed to fetch price for {ticker} when selling stock")
            return redirect(url_for('dashboard'))
        
        sale_price = stock_data['price']
        logger.info(f"Fetched price for {ticker}: ${sale_price:.2f}")
        
    except Exception as price_fetch_error:
        logger.error(f"Error fetching stock price for {ticker}: {str(price_fetch_error)}")
        flash(f'Error fetching stock price. Please try again later.', 'danger')
        return redirect(url_for('dashboard'))
    
    try:
        # Determine transaction type
        from models import Transaction
        existing_transactions = Transaction.query.filter_by(user_id=session['user_id']).count()
        transaction_type = 'sell'
        
        # Process transaction and update cash tracking
        from cash_tracking import process_transaction
        cash_result = process_transaction(
            db=db,
            user_id=session['user_id'],
            ticker=ticker,
            quantity=quantity,
            price=sale_price,
            transaction_type=transaction_type,
            timestamp=datetime.utcnow()
        )
        
        # Update or remove stock
        if stock.quantity == quantity:
            # Selling all shares, remove stock
            db.session.delete(stock)
            logger.info(f"Removed {ticker} from portfolio (sold all shares)")
        else:
            # Selling partial shares, reduce quantity
            stock.quantity -= quantity
            logger.info(f"Reduced {ticker} quantity from {stock.quantity + quantity} to {stock.quantity}")
        
        db.session.commit()
        
        # Recalculate portfolio stats immediately so dashboard updates
        try:
            from leaderboard_utils import calculate_user_portfolio_stats
            from models import UserPortfolioStats
            
            logger.info("Calculating portfolio stats after stock sale...")
            stats = calculate_user_portfolio_stats(session['user_id'])
            logger.info(f"Stats calculated: unique_stocks={stats['unique_stocks_count']}, trades/week={stats['avg_trades_per_week']:.1f}")
            
            # Convert cap percentages to market_cap_mix JSON
            market_cap_mix = {
                'small_cap': stats.get('small_cap_percent', 0),
                'large_cap': stats.get('large_cap_percent', 0)
            }
            
            user_stats = UserPortfolioStats.query.filter_by(user_id=session['user_id']).first()
            
            if user_stats:
                # Update existing stats
                logger.info(f"Updating existing stats for user {session['user_id']}")
                user_stats.unique_stocks_count = stats['unique_stocks_count']
                user_stats.avg_trades_per_week = stats['avg_trades_per_week']
                user_stats.market_cap_mix = market_cap_mix
                user_stats.industry_mix = stats.get('industry_mix', {})
                user_stats.subscriber_count = stats['subscriber_count']
                user_stats.updated_at = datetime.utcnow()
                db.session.merge(user_stats)  # Use merge for cross-session safety
            else:
                # Create new stats entry
                logger.info(f"Creating new stats entry for user {session['user_id']}")
                user_stats = UserPortfolioStats(
                    user_id=session['user_id'],
                    unique_stocks_count=stats['unique_stocks_count'],
                    avg_trades_per_week=stats['avg_trades_per_week'],
                    market_cap_mix=market_cap_mix,
                    industry_mix=stats.get('industry_mix', {}),
                    subscriber_count=stats['subscriber_count']
                )
                db.session.add(user_stats)
            
            db.session.commit()
            logger.info(f" Portfolio stats committed successfully")
        except Exception as stats_error:
            logger.error(f"ERROR updating portfolio stats: {str(stats_error)}")
            import traceback
            logger.error(traceback.format_exc())
            # Don't fail the whole operation if stats update fails
        
        flash(f'Sold {quantity} shares of {ticker} at ${sale_price:.2f}', 'success')
        logger.info(f"Sold stock: {quantity} {ticker} @ ${sale_price}")
        logger.info(f"Cash tracking: max_deployed=${cash_result['max_cash_deployed']}, proceeds=${cash_result['cash_proceeds']}")
    except Exception as e:
        db.session.rollback()
        flash(f'Error selling stock: {str(e)}', 'danger')
        logger.error(f"Failed to sell stock: {str(e)}")
    
    return redirect(url_for('dashboard'))

@app.route('/admin/merge-duplicate-stocks')
@login_required
def merge_duplicate_stocks():
    """
    Merge duplicate stock entries for a user with weighted average cost basis.
    Usage: /admin/merge-duplicate-stocks?user_id=5&ticker=TSLA
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    user_id = request.args.get('user_id', type=int)
    ticker = request.args.get('ticker', '').upper()
    
    if not user_id or not ticker:
        return jsonify({'error': 'user_id and ticker required'}), 400
    
    try:
        # Find all stock entries for this user/ticker
        stocks = Stock.query.filter_by(user_id=user_id, ticker=ticker).all()
        
        if len(stocks) <= 1:
            return jsonify({
                'success': False,
                'message': f'No duplicates found for {ticker}',
                'count': len(stocks)
            })
        
        # Calculate weighted average
        total_cost = sum(s.quantity * s.purchase_price for s in stocks)
        total_quantity = sum(s.quantity for s in stocks)
        weighted_avg_price = total_cost / total_quantity
        
        # Keep first entry, update it
        primary_stock = stocks[0]
        primary_stock.quantity = total_quantity
        primary_stock.purchase_price = weighted_avg_price
        
        # Delete other entries
        for stock in stocks[1:]:
            db.session.delete(stock)
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'Merged {len(stocks)} {ticker} entries',
            'result': {
                'ticker': ticker,
                'entries_merged': len(stocks),
                'final_quantity': total_quantity,
                'weighted_avg_price': weighted_avg_price,
                'total_cost_basis': total_cost
            }
        })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/delete_stock/<int:stock_id>', methods=['POST'])
def delete_stock(stock_id):
    """Delete a stock from user's portfolio"""
    if 'user_id' not in session:
        flash('Please login to delete stocks', 'warning')
        return redirect(url_for('login'))
    
    stock = Stock.query.get(stock_id)
    
    if not stock or stock.user_id != session['user_id']:
        flash('Stock not found or you do not have permission to delete it', 'danger')
        return redirect(url_for('dashboard'))
    
    try:
        db.session.delete(stock)
        db.session.commit()
        flash(f'Deleted {stock.ticker} from your portfolio', 'success')
    except Exception as e:
        db.session.rollback()
        flash(f'Error deleting stock: {str(e)}', 'danger')
        return redirect(url_for('admin_dashboard'))

@app.route('/admin/cleanup-intraday-data')
@login_required
def cleanup_intraday_data():
    """Admin endpoint to clean up old intraday snapshots while preserving 4PM market close data"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from api.cleanup_intraday import cleanup_old_intraday_data
        
        # Run cleanup (keep 14 days of data)
        results = cleanup_old_intraday_data(days_to_keep=14)
        
        if results['errors']:
            message = f'Cleanup completed with errors: {len(results["errors"])} errors occurred'
        else:
            message = f'Cleanup successful: {results["snapshots_deleted"]} snapshots deleted, {results["market_close_preserved"]} market close snapshots preserved'
        
        return jsonify({
            'success': len(results['errors']) == 0,
            'message': message,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Intraday cleanup error: {str(e)}")
        flash(f'Intraday cleanup error: {str(e)}', 'danger')
        return redirect(url_for('admin_dashboard'))

@app.route('/admin/diagnose-missing-today/<username>')
@login_required
def diagnose_missing_today(username):
    """Comprehensive diagnostic for why today's snapshot isn't appearing in charts"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import User, PortfolioSnapshot, MarketData, UserPortfolioChartCache
        from portfolio_performance import PortfolioPerformanceCalculator
        from zoneinfo import ZoneInfo
        import json
        
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': 'User not found'}), 404
        
        today_et = datetime.now(ZoneInfo('America/New_York')).date()
        yesterday_et = today_et - timedelta(days=1)
        
        results = {
            'today': today_et.isoformat(),
            'yesterday': yesterday_et.isoformat(),
            'checks': {}
        }
        
        # CHECK 1: Does today's portfolio snapshot exist?
        today_snapshot = PortfolioSnapshot.query.filter_by(
            user_id=user.id,
            date=today_et
        ).first()
        
        results['checks']['portfolio_snapshot_today'] = {
            'exists': today_snapshot is not None,
            'value': float(today_snapshot.total_value) if today_snapshot else None,
            'date': today_snapshot.date.isoformat() if today_snapshot else None
        }
        
        # CHECK 2: Does yesterday's snapshot exist for comparison?
        yesterday_snapshot = PortfolioSnapshot.query.filter_by(
            user_id=user.id,
            date=yesterday_et
        ).first()
        
        results['checks']['portfolio_snapshot_yesterday'] = {
            'exists': yesterday_snapshot is not None,
            'value': float(yesterday_snapshot.total_value) if yesterday_snapshot else None
        }
        
        # CHECK 3: Does S&P 500 data exist for today?
        sp500_today = MarketData.query.filter_by(
            ticker='SPY_SP500',
            date=today_et
        ).first()
        
        results['checks']['sp500_data_today'] = {
            'exists': sp500_today is not None,
            'value': float(sp500_today.close_price) if sp500_today else None
        }
        
        # CHECK 4: What's the last snapshot date in DB?
        last_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id)\
            .order_by(PortfolioSnapshot.date.desc()).first()
        
        results['checks']['last_snapshot_in_db'] = {
            'date': last_snapshot.date.isoformat() if last_snapshot else None,
            'value': float(last_snapshot.total_value) if last_snapshot else None
        }
        
        # CHECK 5: When was 1M chart cache last generated?
        cache_1m = UserPortfolioChartCache.query.filter_by(
            user_id=user.id,
            period='1M'
        ).first()
        
        if cache_1m:
            cached_data = json.loads(cache_1m.chart_data)
            results['checks']['chart_cache_1m'] = {
                'generated_at': cache_1m.generated_at.isoformat() if cache_1m.generated_at else None,
                'last_label': cached_data.get('labels', [])[-1] if cached_data.get('labels') else None,
                'labels_count': len(cached_data.get('labels', [])),
                'last_5_labels': cached_data.get('labels', [])[-5:] if cached_data.get('labels') else []
            }
        else:
            results['checks']['chart_cache_1m'] = {'exists': False}
        
        # CHECK 6: What does live calculation produce?
        calculator = PortfolioPerformanceCalculator()
        live_1m = calculator.get_performance_data(user.id, '1M')
        
        results['checks']['live_calculation_1m'] = {
            'chart_data_points': len(live_1m.get('chart_data', [])),
            'last_5_points': live_1m.get('chart_data', [])[-5:] if live_1m.get('chart_data') else [],
            'portfolio_return': live_1m.get('portfolio_return')
        }
        
        # CHECK 7: Count all snapshots for date range analysis
        one_month_ago = today_et - timedelta(days=30)
        recent_snapshots = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.user_id == user.id,
            PortfolioSnapshot.date >= one_month_ago
        ).order_by(PortfolioSnapshot.date.desc()).all()
        
        results['checks']['recent_snapshots'] = {
            'count': len(recent_snapshots),
            'dates': [s.date.isoformat() for s in recent_snapshots[:10]]  # Last 10
        }
        
        return jsonify({
            'success': True,
            'user': username,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Diagnostic error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/cleanup-bogus-snapshots')
@login_required
def cleanup_bogus_snapshots():
    """Admin endpoint to delete bogus $0 snapshots before user's first transaction (GROK FIX: Raw SQL + explicit conn)"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import User, Transaction
        from sqlalchemy import text
        
        deleted_counts = {}
        preview = {}
        errors = []
        
        # Get all users (query outside transaction for efficiency)
        users = User.query.all()
        
        # GROK FIX: Use explicit connection + transaction for serverless
        with db.engine.connect() as conn:
            with conn.begin():  # Explicit transaction block
                for user in users:
                    # Find user's first transaction date
                    first_txn = Transaction.query.filter_by(user_id=user.id)\
                        .order_by(Transaction.timestamp.asc()).first()
                    
                    if not first_txn:
                        continue  # User has no transactions, skip
                    
                    first_txn_date = first_txn.timestamp.date()
                    
                    # Find bogus snapshot IDs using raw SQL (avoids loading objects)
                    bogus_ids = conn.execute(text("""
                        SELECT id FROM portfolio_snapshot
                        WHERE user_id = :uid
                          AND total_value = 0.0
                          AND date < :first_date
                        ORDER BY date ASC
                    """), {'uid': user.id, 'first_date': first_txn_date}).scalars().all()
                    
                    if bogus_ids:
                        # Get sample dates for preview
                        sample_dates = conn.execute(text("""
                            SELECT date FROM portfolio_snapshot
                            WHERE id IN :ids
                            ORDER BY date ASC
                            LIMIT 5
                        """), {'ids': tuple(bogus_ids)}).scalars().all()
                        
                        preview[user.username] = {
                            'first_transaction_date': first_txn_date.isoformat(),
                            'bogus_count': len(bogus_ids),
                            'bogus_dates': [d.isoformat() for d in sample_dates]
                        }
                        
                        # RAW SQL BULK DELETE - bypasses ORM state issues
                        logger.info(f"Deleting {len(bogus_ids)} snapshots for {user.username} via RAW SQL: IDs {list(bogus_ids)[:10]}")
                        
                        result = conn.execute(text("""
                            DELETE FROM portfolio_snapshot 
                            WHERE id IN :ids
                        """), {'ids': tuple(bogus_ids)})
                        
                        deleted_counts[user.username] = len(bogus_ids)
                        logger.info(f"RAW SQL reported {result.rowcount} rows deleted for {user.username}")
                
                # Note: with conn.begin() auto-commits on successful exit
                logger.info("Transaction block exiting - auto-commit will occur")
        
        # CRITICAL: Dispose engine to release serverless connections
        db.engine.dispose()
        logger.info("Engine disposed - connections released")
        
        # Verification: Query to confirm deletion
        verification = {}
        for username in deleted_counts.keys():
            user = User.query.filter_by(username=username).first()
            if user:
                remaining = db.session.execute(text("""
                    SELECT COUNT(*) FROM portfolio_snapshot
                    WHERE user_id = :uid AND total_value = 0.0
                """), {'uid': user.id}).scalar()
                verification[username] = remaining
                logger.info(f"Verification: {username} has {remaining} bogus snapshots remaining")
        
        return jsonify({
            'success': True,
            'message': f'Cleaned up bogus snapshots for {len(deleted_counts)} users',
            'results': {
                'deleted': deleted_counts,
                'preview': preview,
                'verification': verification,
                'errors': errors
            }
        })
    
    except Exception as e:
        logger.error(f"Cleanup error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/diagnose-missing-recent/<username>/<period>')
@login_required
def diagnose_missing_recent(username, period):
    """COMPREHENSIVE diagnostic: Why are recent snapshots missing from charts?"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import User, PortfolioSnapshot, MarketData
        from datetime import date, timedelta
        
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': 'User not found'}), 404
        
        today = get_market_date()
        
        # Get last 10 business days
        date_range = []
        check_date = today
        while len(date_range) < 10:
            date_range.append(check_date)
            check_date -= timedelta(days=1)
        
        results = {
            'today': today.isoformat(),
            'today_weekday': today.strftime('%A'),
            'last_10_days': []
        }
        
        for d in date_range:
            # Check portfolio snapshot
            portfolio_snap = PortfolioSnapshot.query.filter_by(
                user_id=user.id,
                date=d
            ).first()
            
            # Check S&P 500 data
            sp500_snap = MarketData.query.filter_by(
                ticker='SPY_SP500',
                date=d
            ).first()
            
            day_info = {
                'date': d.isoformat(),
                'weekday': d.strftime('%A'),
                'is_weekend': d.weekday() >= 5,
                'has_portfolio_snapshot': portfolio_snap is not None,
                'portfolio_value': float(portfolio_snap.total_value) if portfolio_snap else None,
                'has_sp500_data': sp500_snap is not None,
                'sp500_close': float(sp500_snap.close_price) if sp500_snap else None,
                'VERDICT': 'OK' if portfolio_snap and sp500_snap else 'MISSING'
            }
            
            if not portfolio_snap and d.weekday() < 5:
                day_info['PROBLEM'] = 'Missing portfolio snapshot on weekday!'
            if not sp500_snap and d.weekday() < 5:
                day_info['PROBLEM'] = 'Missing S&P 500 data on weekday!'
            
            results['last_10_days'].append(day_info)
        
        # Now check what sampling logic would do
        period_upper = period.upper()
        if period_upper == '1Y':
            start_date = today - timedelta(days=365)
        elif period_upper == 'YTD':
            start_date = date(today.year, 1, 1)
        elif period_upper == '3M':
            start_date = today - timedelta(days=90)
        elif period_upper == '1M':
            start_date = today - timedelta(days=30)
        else:
            start_date = today - timedelta(days=30)
        
        # Get all snapshots for period
        all_snapshots = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.user_id == user.id,
            PortfolioSnapshot.date >= start_date,
            PortfolioSnapshot.date <= today
        ).order_by(PortfolioSnapshot.date.desc()).limit(10).all()
        
        results['period_check'] = {
            'period': period_upper,
            'start_date': start_date.isoformat(),
            'end_date': today.isoformat(),
            'total_snapshots': len(all_snapshots),
            'most_recent_10': [
                {
                    'date': s.date.isoformat(),
                    'value': float(s.total_value),
                    'weekday': s.date.strftime('%A')
                }
                for s in all_snapshots
            ]
        }
        
        return jsonify({
            'success': True,
            'user': username,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Missing recent diagnostic error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/trace-chart-sampling/<username>/<period>')
@login_required
def trace_chart_sampling(username, period):
    """COMPREHENSIVE diagnostic: Trace EXACT sampling logic to find where 6/19 gets filtered"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import User, PortfolioSnapshot, MarketData
        from portfolio_performance import PortfolioPerformanceCalculator
        from datetime import date, timedelta
        
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': 'User not found'}), 404
        
        period_upper = period.upper()
        
        # Get date range (mirror live calculation logic)
        today = get_market_date()
        if today.weekday() == 5:  # Saturday
            end_date = today - timedelta(days=1)
        elif today.weekday() == 6:  # Sunday
            end_date = today - timedelta(days=2)
        else:
            end_date = today
        
        if period_upper == 'YTD':
            start_date = date(end_date.year, 1, 1)
        elif period_upper == '1Y':
            start_date = end_date - timedelta(days=365)
        elif period_upper == '3M':
            start_date = end_date - timedelta(days=90)
        elif period_upper == '1M':
            start_date = end_date - timedelta(days=30)
        else:
            start_date = end_date - timedelta(days=30)
        
        # STEP 1: All portfolio snapshots in DB
        all_snapshots = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.user_id == user.id,
            PortfolioSnapshot.date >= start_date,
            PortfolioSnapshot.date <= end_date
        ).order_by(PortfolioSnapshot.date.asc()).all()
        
        snapshot_dates = [s.date.isoformat() for s in all_snapshots]
        
        # STEP 2: All S&P 500 data available
        sp500_records = MarketData.query.filter(
            MarketData.ticker == 'SPY_SP500',
            MarketData.date >= start_date,
            MarketData.date <= end_date
        ).order_by(MarketData.date.asc()).all()
        
        sp500_dates = [r.date.isoformat() for r in sp500_records]
        
        # STEP 3: Check if 6/19 exists in both
        june_19 = date(2025, 6, 19)
        has_snapshot_619 = any(s.date == june_19 for s in all_snapshots)
        has_sp500_619 = any(r.date == june_19 for r in sp500_records)
        
        # STEP 4: Run actual live calculation
        calculator = PortfolioPerformanceCalculator()
        live_data = calculator.get_performance_data(user.id, period_upper)
        
        # STEP 5: Extract what actually got rendered
        chart_data = live_data.get('chart_data', [])
        rendered_dates = [point['date'] for point in chart_data]
        
        results = {
            'period': period_upper,
            'date_range': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'step1_all_snapshots': {
                'count': len(all_snapshots),
                'dates': snapshot_dates,
                'first': snapshot_dates[0] if snapshot_dates else None,
                'last': snapshot_dates[-1] if snapshot_dates else None
            },
            'step2_all_sp500_data': {
                'count': len(sp500_records),
                'dates': sp500_dates,
                'first': sp500_dates[0] if sp500_dates else None,
                'last': sp500_dates[-1] if sp500_dates else None
            },
            'step3_june_19_check': {
                'has_portfolio_snapshot': has_snapshot_619,
                'has_sp500_data': has_sp500_619,
                'VERDICT': 'BOTH EXIST' if (has_snapshot_619 and has_sp500_619) else 'MISSING DATA'
            },
            'step4_rendered_chart': {
                'count': len(chart_data),
                'dates': rendered_dates,
                'first': rendered_dates[0] if rendered_dates else None,
                'last': rendered_dates[-1] if rendered_dates else None,
                'first_5_points': chart_data[:5]
            },
            'SMOKING_GUN': {
                'june_19_in_database': has_snapshot_619 and has_sp500_619,
                'june_19_in_rendered_chart': 'Jun 19' in rendered_dates,
                'PROBLEM': 'Data exists but not rendered!' if (has_snapshot_619 and has_sp500_619 and 'Jun 19' not in rendered_dates) else 'Check data availability'
            }
        }
        
        return jsonify({
            'success': True,
            'user': username,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Trace sampling error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/debug-live-calculation/<username>/<period>')
@login_required
def debug_live_calculation(username, period):
    """Admin endpoint to see what live calculation produces vs cached data"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import User, UserPortfolioChartCache, PortfolioSnapshot
        from portfolio_performance import PortfolioPerformanceCalculator
        import json
        
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': 'User not found'}), 404
        
        period_upper = period.upper()
        
        # Get what's in the cache
        chart_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user.id,
            period=period_upper
        ).first()
        
        cache_info = None
        if chart_cache:
            cached_data = json.loads(chart_cache.chart_data)
            cache_info = {
                'exists': True,
                'datasets_count': len(cached_data.get('datasets', [])),
                'dataset_labels': [d.get('label') for d in cached_data.get('datasets', [])],
                'first_label': cached_data.get('labels', [])[0] if cached_data.get('labels') else None,
                'last_label': cached_data.get('labels', [])[-1] if cached_data.get('labels') else None,
                'sample_portfolio_values': cached_data.get('datasets', [{}])[0].get('data', [])[:5] if cached_data.get('datasets') else []
            }
        else:
            cache_info = {'exists': False}
        
        # Get what live calculation produces
        calculator = PortfolioPerformanceCalculator()
        live_data = calculator.get_performance_data(user.id, period_upper)
        
        live_info = {
            'portfolio_return': live_data.get('portfolio_return'),
            'sp500_return': live_data.get('sp500_return'),
            'chart_data_points': len(live_data.get('chart_data', [])),
            'first_chart_point': live_data.get('chart_data', [{}])[0] if live_data.get('chart_data') else None,
            'last_chart_point': live_data.get('chart_data', [{}])[-1] if live_data.get('chart_data') else None,
            'sample_chart_data': live_data.get('chart_data', [])[:5],
            'has_error': 'error' in live_data
        }
        
        # Get user's snapshot info
        first_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id).order_by(PortfolioSnapshot.date.asc()).first()
        last_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id).order_by(PortfolioSnapshot.date.desc()).first()
        
        snapshot_info = {
            'first_date': first_snapshot.date.isoformat() if first_snapshot else None,
            'first_value': float(first_snapshot.total_value) if first_snapshot else None,
            'last_date': last_snapshot.date.isoformat() if last_snapshot else None,
            'last_value': float(last_snapshot.total_value) if last_snapshot else None,
            'total_count': PortfolioSnapshot.query.filter_by(user_id=user.id).count()
        }
        
        return jsonify({
            'success': True,
            'user': username,
            'period': period_upper,
            'cache': cache_info,
            'live_calculation': live_info,
            'snapshots': snapshot_info
        })
    
    except Exception as e:
        logger.error(f"Debug live calculation error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/inspect-chart-cache/<username>/<period>')
@login_required
def inspect_chart_cache(username, period):
    """Admin endpoint to inspect what's in the chart cache"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import User, UserPortfolioChartCache
        import json
        
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': 'User not found'}), 404
        
        period_upper = period.upper()
        chart_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user.id,
            period=period_upper
        ).first()
        
        if not chart_cache:
            return jsonify({'error': 'No cache found for this user/period'}), 404
        
        cached_data = json.loads(chart_cache.chart_data)
        
        # CRITICAL: Show ALL labels to trace exactly what gets rendered
        all_labels = cached_data.get('labels', [])
        
        results = {
            'user': username,
            'period': period_upper,
            'generated_at': chart_cache.generated_at.isoformat() if chart_cache.generated_at else None,
            'cache_structure': {
                'has_labels': 'labels' in cached_data,
                'labels_count': len(all_labels),
                'first_label': all_labels[0] if all_labels else None,
                'last_label': all_labels[-1] if all_labels else None,
                'has_datasets': 'datasets' in cached_data,
                'datasets_count': len(cached_data.get('datasets', []))
            },
            'ALL_LABELS': all_labels,  # SHOW EVERY SINGLE LABEL
            'first_10_labels': all_labels[:10] if all_labels else [],
            'datasets': []
        }
        
        for i, dataset in enumerate(cached_data.get('datasets', [])):
            dataset_info = {
                'index': i,
                'label': dataset.get('label'),
                'data_count': len(dataset.get('data', [])),
                'first_value': dataset.get('data', [])[0] if dataset.get('data') else None,
                'last_value': dataset.get('data', [])[-1] if dataset.get('data') else None,
                'sample_values': dataset.get('data', [])[:5] if dataset.get('data') else [],
                'has_all_zeros': all(v == 0 for v in dataset.get('data', [])),
                'has_any_nonzero': any(v != 0 for v in dataset.get('data', []))
            }
            results['datasets'].append(dataset_info)
        
        return jsonify({
            'success': True,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Chart cache inspection error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/backfill-sp500-data', methods=['GET', 'POST'])
@login_required
def backfill_sp500_data():
    """Backfill missing S&P 500 data for specific dates"""
    try:
        if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, timedelta
        from models import MarketData
        from portfolio_performance import PortfolioPerformanceCalculator
        
        # GET: Show missing dates and confirm
        if request.method == 'GET':
            # Check last 10 business days for missing S&P 500 data
            today = date.today()
            missing_dates = []
            
            for days_back in range(10):
                check_date = today - timedelta(days=days_back)
                
                # Skip weekends
                if check_date.weekday() >= 5:
                    continue
                
                # Check if S&P 500 data exists
                sp500_data = MarketData.query.filter_by(
                    ticker='SPY_SP500',
                    date=check_date
                ).first()
                
                if not sp500_data:
                    missing_dates.append(check_date.isoformat())
            
            return jsonify({
                'missing_dates': missing_dates,
                'count': len(missing_dates),
                'message': f'Found {len(missing_dates)} missing S&P 500 dates',
                'action': 'POST to this endpoint to backfill'
            })
        
        # POST: Perform backfill
        results = {
            'started_at': datetime.now().isoformat(),
            'dates_processed': [],
            'successes': 0,
            'failures': 0,
            'errors': []
        }
        
        # Get dates to backfill from request or auto-detect
        # Handle both JSON and form POST requests
        try:
            dates_to_fill = request.json.get('dates', []) if request.json else []
        except:
            dates_to_fill = []
        
        if not dates_to_fill:
            # Auto-detect missing dates from last 10 days
            today = date.today()
            for days_back in range(10):
                check_date = today - timedelta(days=days_back)
                if check_date.weekday() >= 5:
                    continue
                
                sp500_data = MarketData.query.filter_by(
                    ticker='SPY_SP500',
                    date=check_date
                ).first()
                
                if not sp500_data:
                    dates_to_fill.append(check_date.isoformat())
        
        # Backfill each missing date - fetch historical SPY data ONCE
        import os
        import requests
        
        ALPHA_VANTAGE_API_KEY = os.environ.get('ALPHA_VANTAGE_API_KEY')
        
        if not dates_to_fill:
            return jsonify({
                'success': True,
                'message': 'No missing dates to backfill',
                'results': results
            })
        
        # Fetch historical SPY data (single API call for all dates)
        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=SPY&apikey={ALPHA_VANTAGE_API_KEY}&outputsize=compact'
        
        response = requests.get(url, timeout=10)
        data = response.json()
        time_series = data.get('Time Series (Daily)', {})
        
        if not time_series:
            return jsonify({
                'success': False,
                'error': 'Failed to fetch SPY data from Alpha Vantage',
                'response': data
            }), 500
        
        # Process each missing date
        for date_str in dates_to_fill:
            try:
                fill_date = date.fromisoformat(date_str)
                
                # Extract close price for this specific date
                day_data = time_series.get(date_str)
                
                if not day_data:
                    results['errors'].append(f"{date_str}: No SPY data in Alpha Vantage response")
                    results['failures'] += 1
                    continue
                
                spy_close = float(day_data.get('4. close'))
                sp500_value = spy_close * 10  # Convert SPY to S&P 500
                
                # Create MarketData entry
                market_data = MarketData(
                    ticker='SPY_SP500',
                    date=fill_date,
                    close_price=sp500_value
                )
                db.session.add(market_data)
                
                results['dates_processed'].append({
                    'date': date_str,
                    'spy_close': spy_close,
                    'sp500_value': sp500_value,
                    'status': 'created'
                })
                results['successes'] += 1
                
                logger.info(f"Backfilled S&P 500 for {date_str}: SPY=${spy_close:.2f}  S&P 500=${sp500_value:.2f}")
                
            except Exception as e:
                error_msg = f"{date_str}: {str(e)}"
                results['errors'].append(error_msg)
                results['failures'] += 1
                logger.error(f"Backfill error: {error_msg}")
        
        # Commit all changes
        db.session.commit()
        results['completed_at'] = datetime.now().isoformat()
        results['database_committed'] = True
        
        # Optionally regenerate chart cache
        if results['successes'] > 0:
            try:
                from leaderboard_utils import update_leaderboard_cache
                updated_count = update_leaderboard_cache()
                db.session.commit()  # Commit chart cache updates
                results['chart_cache_regenerated'] = True
                results['chart_cache_entries'] = updated_count
            except Exception as e:
                results['chart_cache_error'] = str(e)
                db.session.rollback()
        
        return jsonify({
            'success': True,
            'results': results
        })
        
    except Exception as e:
        logger.error(f"Backfill error: {e}")
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/command-center', methods=['GET'])
@login_required
def admin_command_center():
    """One-time use admin command center with buttons for common operations"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    return '''
    <!DOCTYPE html>
    <html>
    <head>
        <title>Admin Command Center</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
        <style>
            .command-card { margin-bottom: 20px; }
            .result-box { 
                margin-top: 10px; 
                padding: 15px; 
                background: #f8f9fa; 
                border-radius: 5px;
                max-height: 400px;
                overflow-y: auto;
            }
            .success { background: #d4edda; border: 1px solid #c3e6cb; }
            .error { background: #f8d7da; border: 1px solid #f5c6cb; }
            .loading { background: #fff3cd; border: 1px solid #ffeaa7; }
        </style>
    </head>
    <body>
        <div class="container mt-4">
            <h1> Admin Command Center</h1>
            <p class="text-muted">One-click admin operations</p>
            
            <div class="row">
                <!-- Backfill S&P 500 -->
                <div class="col-md-6">
                    <div class="card command-card">
                        <div class="card-header bg-primary text-white">
                            <h5> Backfill S&P 500 Data</h5>
                        </div>
                        <div class="card-body">
                            <p>Backfill missing S&P 500 data for 10/15, 10/16, 10/17</p>
                            <button class="btn btn-primary" onclick="runCommand('backfill')">
                                Run Backfill
                            </button>
                            <div id="backfill-result" class="result-box" style="display:none;"></div>
                        </div>
                    </div>
                </div>
                
                <!-- Check Snapshot Data -->
                <div class="col-md-6">
                    <div class="card command-card">
                        <div class="card-header bg-info text-white">
                            <h5> Check Snapshot Data</h5>
                        </div>
                        <div class="card-body">
                            <p>Verify portfolio + S&P 500 snapshots exist</p>
                            <button class="btn btn-info" onclick="runCommand('check')">
                                Check Data
                            </button>
                            <div id="check-result" class="result-box" style="display:none;"></div>
                        </div>
                    </div>
                </div>
                
                <!-- Regenerate Chart Cache -->
                <div class="col-md-6">
                    <div class="card command-card">
                        <div class="card-header bg-success text-white">
                            <h5> Regenerate Chart Cache</h5>
                        </div>
                        <div class="card-body">
                            <p>Force regenerate all chart caches</p>
                            <button class="btn btn-success" onclick="runCommand('regenerate')">
                                Regenerate Cache
                            </button>
                            <div id="regenerate-result" class="result-box" style="display:none;"></div>
                        </div>
                    </div>
                </div>
                
                <!-- Clear Leaderboard HTML Cache -->
                <div class="col-md-6">
                    <div class="card command-card">
                        <div class="card-header bg-warning text-dark">
                            <h5> Clear Leaderboard HTML Cache</h5>
                        </div>
                        <div class="card-body">
                            <p>Clear pre-rendered HTML (fixes nav menu issues)</p>
                            <button class="btn btn-warning" onclick="runCommand('clear-html')">
                                Clear HTML Cache
                            </button>
                            <div id="clear-html-result" class="result-box" style="display:none;"></div>
                        </div>
                    </div>
                </div>
                
                <!-- Check Recent Snapshots -->
                <div class="col-md-6">
                    <div class="card command-card">
                        <div class="card-header bg-danger text-white">
                            <h5> Check Recent Snapshots (Last 5 Days)</h5>
                        </div>
                        <div class="card-body">
                            <p>Diagnose 0% performance issue for all users</p>
                            <button class="btn btn-danger" onclick="runCommand('check-snapshots')">
                                Check Snapshots
                            </button>
                            <div id="check-snapshots-result" class="result-box" style="display:none;"></div>
                        </div>
                    </div>
                </div>
                
                <!-- Diagnose Portfolio Calculations -->
                <div class="col-md-6">
                    <div class="card command-card">
                        <div class="card-header bg-danger text-white">
                            <h5> Diagnose Portfolio Calculations</h5>
                        </div>
                        <div class="card-body">
                            <p>Test portfolio value calculations for users with stale snapshots</p>
                            <button class="btn btn-danger" onclick="runCommand('diagnose-calc')">
                                Run Diagnostic
                            </button>
                            <div id="diagnose-calc-result" class="result-box" style="display:none;"></div>
                        </div>
                    </div>
                </div>
                
                <!-- Check Historical Stock Data -->
                <div class="col-md-6">
                    <div class="card command-card">
                        <div class="card-header bg-info text-white">
                            <h5> Check Historical Stock Data (Oct 7-17)</h5>
                        </div>
                        <div class="card-body">
                            <p>Check if stock prices were collected for users' holdings during the gap period</p>
                            <button class="btn btn-info" onclick="runCommand('check-stock-data')">
                                Check Stock Data
                            </button>
                            <div id="check-stock-data-result" class="result-box" style="display:none;"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <script>
        async function runCommand(cmd) {
            const resultDiv = document.getElementById(cmd + '-result');
            resultDiv.style.display = 'block';
            resultDiv.className = 'result-box loading';
            resultDiv.innerHTML = ' Running...';
            
            try {
                let url, method = 'POST';
                if (cmd === 'backfill') {
                    url = '/admin/backfill-sp500-data';
                } else if (cmd === 'check') {
                    url = '/admin/check-snapshot-data';
                    method = 'GET';
                } else if (cmd === 'regenerate') {
                    url = '/admin/regenerate-chart-cache';
                    method = 'GET';
                } else if (cmd === 'clear-html') {
                    url = '/admin/clear-leaderboard-html';
                    method = 'POST';
                } else if (cmd === 'check-snapshots') {
                    url = '/admin/check-recent-snapshots';
                    method = 'GET';
                } else if (cmd === 'diagnose-calc') {
                    url = '/admin/diagnose-portfolio-calculations';
                    method = 'GET';
                } else if (cmd === 'check-stock-data') {
                    url = '/admin/check-historical-stock-data';
                    method = 'GET';
                }
                
                const response = await fetch(url, { method });
                const data = await response.json();
                
                if (data.success !== false && response.ok) {
                    resultDiv.className = 'result-box success';
                    resultDiv.innerHTML = '<strong> Success!</strong><pre>' + 
                        JSON.stringify(data, null, 2) + '</pre>';
                } else {
                    resultDiv.className = 'result-box error';
                    resultDiv.innerHTML = '<strong> Error!</strong><pre>' + 
                        JSON.stringify(data, null, 2) + '</pre>';
                }
            } catch (error) {
                resultDiv.className = 'result-box error';
                resultDiv.innerHTML = '<strong> Error!</strong><p>' + error.message + '</p>';
            }
        }
        </script>
    </body>
    </html>
    '''

@app.route('/admin/check-historical-stock-data')
@login_required
def check_historical_stock_data():
    """Check if stock price data exists for users' holdings during Oct 7-17"""
    try:
        if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import User, Transaction, MarketData, UserPortfolioChartCache
        from datetime import date, datetime, timedelta
        from sqlalchemy import and_
        
        # Check the 4 users with missing snapshots
        problem_users = ['testing2', 'testing3', 'wild-bronco', 'wise-buffalo']
        oct_7 = date(2025, 10, 7)
        oct_17 = date(2025, 10, 17)
        oct_7_dt = datetime(2025, 10, 7)
        oct_17_dt = datetime(2025, 10, 17, 23, 59, 59)
        
        results = {
            'date_range': f'{oct_7.isoformat()} to {oct_17.isoformat()}',
            'users': []
        }
        
        for username in problem_users:
            user = User.query.filter_by(username=username).first()
            if not user:
                continue
            
            user_data = {
                'username': username,
                'user_id': user.id,
                'holdings': [],
                'chart_cache_entries': []
            }
            
            # Check if chart cache was generated during this period (indicates calculations worked)
            chart_caches = UserPortfolioChartCache.query.filter(
                and_(
                    UserPortfolioChartCache.user_id == user.id,
                    UserPortfolioChartCache.generated_at >= oct_7_dt,
                    UserPortfolioChartCache.generated_at <= oct_17_dt
                )
            ).all()
            
            for cache in chart_caches:
                user_data['chart_cache_entries'].append({
                    'period': cache.period,
                    'generated_at': cache.generated_at.isoformat() if cache.generated_at else None
                })
            
            # Get all transactions to determine holdings
            transactions = Transaction.query.filter_by(user_id=user.id).order_by(Transaction.timestamp).all()
            
            # Calculate current holdings
            holdings = {}
            for txn in transactions:
                if txn.transaction_type in ('buy', 'initial'):
                    holdings[txn.ticker] = holdings.get(txn.ticker, 0) + txn.quantity
                elif txn.transaction_type == 'sell':
                    holdings[txn.ticker] = holdings.get(txn.ticker, 0) - txn.quantity
            
            # Check if price data exists for each holding during Oct 7-17
            for ticker, quantity in holdings.items():
                if quantity <= 0:
                    continue
                
                # Check MarketData table for this ticker during the period
                price_data = MarketData.query.filter(
                    and_(
                        MarketData.ticker == ticker,
                        MarketData.date >= oct_7,
                        MarketData.date <= oct_17
                    )
                ).order_by(MarketData.date).all()
                
                holding_info = {
                    'ticker': ticker,
                    'quantity': quantity,
                    'price_data_points': len(price_data),
                    'dates_with_data': [p.date.isoformat() for p in price_data]
                }
                
                user_data['holdings'].append(holding_info)
            
            results['users'].append(user_data)
        
        return jsonify({
            'success': True,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Error checking historical stock data: {e}")
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/diagnose-portfolio-calculations')
@login_required
def diagnose_portfolio_calculations():
    """Diagnose why portfolio calculations are failing for specific users"""
    try:
        if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import User, Transaction
        from cash_tracking import calculate_portfolio_value_with_cash
        from portfolio_performance import PortfolioPerformanceCalculator
        from datetime import date
        
        # Test the 4 users with stale snapshots
        problem_users = ['testing2', 'testing3', 'wild-bronco', 'wise-buffalo']
        results = {
            'test_date': date.today().isoformat(),
            'users': []
        }
        
        for username in problem_users:
            user = User.query.filter_by(username=username).first()
            if not user:
                results['users'].append({'username': username, 'error': 'User not found'})
                continue
            
            user_diag = {
                'username': username,
                'user_id': user.id,
                'email': user.email,
                'max_cash_deployed': user.max_cash_deployed,
                'cash_proceeds': user.cash_proceeds
            }
            
            # Get transaction count
            txn_count = Transaction.query.filter_by(user_id=user.id).count()
            user_diag['transaction_count'] = txn_count
            
            # Get recent transactions
            recent_txns = Transaction.query.filter_by(user_id=user.id).order_by(Transaction.timestamp.desc()).limit(3).all()
            user_diag['recent_transactions'] = [
                {
                    'ticker': t.ticker,
                    'type': t.transaction_type,
                    'quantity': t.quantity,
                    'price': t.price,
                    'date': t.timestamp.isoformat()
                } for t in recent_txns
            ]
            
            # Try to calculate portfolio value
            try:
                portfolio_data = calculate_portfolio_value_with_cash(user.id, date.today())
                user_diag['calculation_result'] = {
                    'stock_value': portfolio_data['stock_value'],
                    'cash_proceeds': portfolio_data['cash_proceeds'],
                    'total_value': portfolio_data['total_value'],
                    'status': 'SUCCESS' if portfolio_data['total_value'] > 0 else 'ZERO_VALUE'
                }
                
                # Also test the raw calculator
                calculator = PortfolioPerformanceCalculator()
                raw_stock_value = calculator.calculate_portfolio_value(user.id, date.today())
                user_diag['raw_stock_calculation'] = {
                    'value': raw_stock_value,
                    'status': 'SUCCESS' if raw_stock_value > 0 else 'ZERO_OR_NONE'
                }
                
            except Exception as e:
                user_diag['calculation_result'] = {
                    'error': str(e),
                    'status': 'FAILED'
                }
                import traceback
                user_diag['traceback'] = traceback.format_exc()
            
            results['users'].append(user_diag)
        
        return jsonify({
            'success': True,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Error diagnosing portfolio calculations: {e}")
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-recent-snapshots')
@login_required
def check_recent_snapshots():
    """Check last 5 days of market close snapshots for all users to diagnose 0% performance"""
    try:
        if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, timedelta
        from models import PortfolioSnapshot, User, MarketData
        from sqlalchemy import func
        
        # Get all active users
        users = User.query.all()
        
        # Check Oct 7-17 specifically
        today = date.today()
        oct_7 = date(2025, 10, 7)
        oct_17 = date(2025, 10, 17)
        
        results = {
            'check_date': today.isoformat(),
            'date_range_checked': f'{oct_7.isoformat()} to {oct_17.isoformat()}',
            'users_checked': len(users),
            'users': []
        }
        
        # Check S&P 500 data for Oct 7-17
        sp500_dates = MarketData.query.filter(
            MarketData.ticker == 'SPY_SP500',
            MarketData.date >= oct_7,
            MarketData.date <= oct_17
        ).order_by(MarketData.date).all()
        
        results['sp500_data_count'] = len(sp500_dates)
        results['sp500_dates'] = [{'date': s.date.isoformat(), 'value': float(s.close_price)} for s in sp500_dates]
        
        for user in users:
            user_data = {
                'username': user.username,
                'email': user.email,
                'snapshots': []
            }
            
            # Get ALL snapshots from Oct 7-17
            snapshots = PortfolioSnapshot.query.filter(
                PortfolioSnapshot.user_id == user.id,
                PortfolioSnapshot.date >= oct_7,
                PortfolioSnapshot.date <= oct_17
            ).order_by(PortfolioSnapshot.date.desc()).all()
            
            for snapshot in snapshots:
                user_data['snapshots'].append({
                    'date': snapshot.date.isoformat(),
                    'total_value': float(snapshot.total_value) if snapshot.total_value else 0,
                    'stock_value': float(snapshot.stock_value) if snapshot.stock_value else 0,
                    'cash_proceeds': float(snapshot.cash_proceeds) if snapshot.cash_proceeds else 0,
                    'max_cash_deployed': float(snapshot.max_cash_deployed) if snapshot.max_cash_deployed else 0
                })
            
            # Check if last 5 snapshots are identical (indicates issue)
            if len(snapshots) >= 5:
                values = [s.total_value for s in snapshots[:5]]
                all_same = len(set(values)) == 1
                user_data['all_values_identical'] = all_same
                if all_same:
                    user_data['issue'] = f'Last 5 snapshots all have value ${values[0]:.2f} - likely causing 0% performance'
            
            results['users'].append(user_data)
        
        return jsonify({
            'success': True,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Error checking recent snapshots: {e}")
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/clear-leaderboard-html', methods=['POST'])
@login_required
def clear_leaderboard_html():
    """Clear pre-rendered HTML from leaderboard cache to force regeneration"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import LeaderboardCache
        from sqlalchemy import update
        
        # Clear all rendered_html fields
        result = db.session.execute(
            update(LeaderboardCache).values(rendered_html=None)
        )
        db.session.commit()
        
        cleared_count = result.rowcount
        
        logger.info(f"Cleared {cleared_count} pre-rendered HTML cache entries")
        
        return jsonify({
            'success': True,
            'message': f'Cleared {cleared_count} HTML cache entries',
            'cleared_count': cleared_count,
            'next_step': 'Visit /leaderboard to trigger fresh rendering with correct nav menu'
        })
    
    except Exception as e:
        logger.error(f"Error clearing HTML cache: {e}")
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/clear-leaderboard-cache', methods=['GET', 'POST'])
@login_required
def clear_leaderboard_cache():
    """
    DELETE old leaderboard cache entries to force use of new calculate_leaderboard_data() code
    
    This will make get_leaderboard_data() fall back to calculate_leaderboard_data(),
    which now uses UserPortfolioChartCache (correct source)
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import LeaderboardCache
        
        # Get count before deletion
        before_count = LeaderboardCache.query.count()
        
        # DELETE all cache entries
        LeaderboardCache.query.delete()
        db.session.commit()
        
        logger.info(f"DELETED {before_count} old leaderboard cache entries - will force on-demand calculation")
        
        return jsonify({
            'success': True,
            'message': f'Deleted {before_count} old cache entries',
            'deleted_count': before_count,
            'next_step': 'Visit /leaderboard - it will now use NEW calculate_leaderboard_data() code with UserPortfolioChartCache',
            'warning': 'First page load will be slower (~1 second) as it calculates on-demand'
        })
    
    except Exception as e:
        logger.error(f"Error clearing leaderboard cache: {e}")
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/generate-chart-cache-only')
@login_required
def generate_chart_cache_only():
    """Generate ONLY UserPortfolioChartCache without leaderboard calculation - fixes chicken-egg problem"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import User, UserPortfolioChartCache
        from leaderboard_utils import generate_chart_from_snapshots
        import json
        
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']
        all_users = User.query.all()
        
        results = {
            'users_processed': 0,
            'charts_generated': 0,
            'charts_skipped': 0,
            'errors': []
        }
        
        for user in all_users:
            results['users_processed'] += 1
            
            for period in periods:
                try:
                    # Generate chart data
                    chart_data = generate_chart_from_snapshots(user.id, period)
                    
                    if chart_data:
                        # Update or create chart cache
                        chart_cache = UserPortfolioChartCache.query.filter_by(
                            user_id=user.id, period=period
                        ).first()
                        
                        if chart_cache:
                            chart_cache.chart_data = json.dumps(chart_data)
                            chart_cache.generated_at = datetime.now()
                        else:
                            chart_cache = UserPortfolioChartCache(
                                user_id=user.id,
                                period=period,
                                chart_data=json.dumps(chart_data),
                                generated_at=datetime.now()
                            )
                            db.session.add(chart_cache)
                        
                        results['charts_generated'] += 1
                        logger.info(f"Generated chart cache for user {user.id}, period {period}")
                    else:
                        results['charts_skipped'] += 1
                        logger.warning(f"No chart data for user {user.id}, period {period}")
                        
                except Exception as e:
                    error_msg = f"Error for user {user.id}, period {period}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
        
        # Commit all chart caches
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'Generated {results["charts_generated"]} chart caches for {results["users_processed"]} users',
            'results': results,
            'next_step': 'Now run /admin/clear-leaderboard-cache then visit /leaderboard'
        })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-intraday-dates')
@login_required
def check_intraday_dates():
    """Check what dates have intraday snapshots - debug weekend data loss"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import PortfolioSnapshotIntraday
        from sqlalchemy import func, cast, Date
        from datetime import datetime, timedelta
        from zoneinfo import ZoneInfo
        
        # Get ET timezone
        ET = ZoneInfo('America/New_York')
        now_et = datetime.now(ET)
        today_et = now_et.date()
        
        # Get all distinct dates with intraday data
        dates_with_data = db.session.query(
            cast(PortfolioSnapshotIntraday.timestamp, Date).label('snapshot_date'),
            func.count(PortfolioSnapshotIntraday.id).label('count')
        ).group_by('snapshot_date').order_by('snapshot_date').all()
        
        # Get last 7 days to see what's missing
        last_7_days = []
        for i in range(7):
            check_date = today_et - timedelta(days=i)
            count = db.session.query(func.count(PortfolioSnapshotIntraday.id)).filter(
                cast(PortfolioSnapshotIntraday.timestamp, Date) == check_date
            ).scalar()
            
            # Check if it's a weekday
            is_weekday = check_date.weekday() < 5  # Monday=0, Friday=4
            
            last_7_days.append({
                'date': check_date.isoformat(),
                'day_name': check_date.strftime('%A'),
                'is_weekday': is_weekday,
                'snapshot_count': count,
                'expected': 'Should have data' if is_weekday else 'Weekend - no data expected',
                'status': ' OK' if (is_weekday and count > 0) or (not is_weekday and count == 0) else ' MISSING' if is_weekday else ' Unexpected data'
            })
        
        return jsonify({
            'success': True,
            'current_time_et': now_et.isoformat(),
            'today_et': today_et.isoformat(),
            'all_dates_with_data': [{'date': str(d[0]), 'count': d[1]} for d in dates_with_data],
            'last_7_days_analysis': last_7_days,
            'diagnosis': {
                'friday_data_missing': last_7_days[1]['status'] == ' MISSING' if len(last_7_days) > 1 and last_7_days[1]['day_name'] == 'Friday' else None,
                'possible_causes': [
                    'Intraday cleanup ran with wrong days_to_keep value',
                    'Timezone mismatch in cleanup logic (using UTC instead of ET)',
                    'Intraday collection cron failed on Friday',
                    'Manual deletion via /admin/cleanup-intraday-data'
                ]
            }
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/force-regenerate-all-caches')
@login_required
def force_regenerate_all_caches():
    """Force regenerate both chart cache AND leaderboard cache in correct order"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import User, UserPortfolioChartCache, LeaderboardCache
        from leaderboard_utils import generate_chart_from_snapshots, calculate_leaderboard_data
        import json
        
        results = {
            'started_at': datetime.now().isoformat(),
            'phase1_chart_cache': {},
            'phase2_leaderboard_cache': {},
            'errors': []
        }
        
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']
        categories = ['all', 'small_cap', 'large_cap']
        
        # PHASE 1: Generate Chart Cache for ALL users and periods
        logger.info("PHASE 1: Generating chart cache...")
        all_users = User.query.all()
        charts_generated = 0
        
        for user in all_users:
            for period in periods:
                try:
                    chart_data = generate_chart_from_snapshots(user.id, period)
                    
                    if chart_data:
                        chart_cache = UserPortfolioChartCache.query.filter_by(
                            user_id=user.id, period=period
                        ).first()
                        
                        if chart_cache:
                            chart_cache.chart_data = json.dumps(chart_data)
                            chart_cache.generated_at = datetime.now()
                            db.session.merge(chart_cache)  # Use merge for Vercel serverless
                        else:
                            chart_cache = UserPortfolioChartCache(
                                user_id=user.id,
                                period=period,
                                chart_data=json.dumps(chart_data),
                                generated_at=datetime.now()
                            )
                            db.session.add(chart_cache)
                        
                        charts_generated += 1
                        logger.info(f"Cached chart for user {user.id}, period {period}")
                except Exception as e:
                    error_msg = f"Chart gen failed for user {user.id}, period {period}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
        
        # Commit chart caches
        db.session.commit()
        results['phase1_chart_cache'] = {
            'charts_generated': charts_generated,
            'status': 'completed'
        }
        logger.info(f"PHASE 1 Complete: {charts_generated} chart caches generated")
        
        # PHASE 2: Generate Leaderboard Cache using the chart caches
        logger.info("PHASE 2: Generating leaderboard cache...")
        leaderboards_generated = 0
        
        for period in periods:
            for category in categories:
                try:
                    # Calculate using chart cache
                    leaderboard_data = calculate_leaderboard_data(period, 20, category)
                    
                    if leaderboard_data:
                        cache_key = f"{period}_{category}"
                        
                        # Store both auth and anon versions
                        for suffix in ['_auth', '_anon']:
                            full_key = cache_key + suffix
                            cache_entry = LeaderboardCache.query.filter_by(period=full_key).first()
                            
                            if cache_entry:
                                cache_entry.leaderboard_data = json.dumps(leaderboard_data)
                                cache_entry.generated_at = datetime.now()
                            else:
                                cache_entry = LeaderboardCache(
                                    period=full_key,
                                    leaderboard_data=json.dumps(leaderboard_data),
                                    generated_at=datetime.now()
                                )
                                db.session.add(cache_entry)
                            
                            leaderboards_generated += 1
                
                except Exception as e:
                    error_msg = f"Leaderboard gen failed for {period}_{category}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
        
        # Commit leaderboard caches
        db.session.commit()
        results['phase2_leaderboard_cache'] = {
            'leaderboards_generated': leaderboards_generated,
            'status': 'completed'
        }
        logger.info(f"PHASE 2 Complete: {leaderboards_generated} leaderboard caches generated")
        
        results['completed_at'] = datetime.now().isoformat()
        
        return jsonify({
            'success': True,
            'message': f'Generated {charts_generated} chart caches and {leaderboards_generated} leaderboard caches',
            'results': results,
            'next_step': 'Visit /leaderboard to see correct data'
        })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

# ============================================================================
# ADMIN ROUTES: New Unified Performance Calculator Testing
# ============================================================================

@app.route('/admin/test-unified-calculator')
@login_required
def test_unified_calculator():
    """
    Test the new unified performance calculator with real user data.
    
    Usage: /admin/test-unified-calculator?username=witty-raven&period=YTD
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from performance_calculator import calculate_portfolio_performance, get_period_dates
        from models import UserPortfolioChartCache
        import json
        
        # Get parameters
        username = request.args.get('username', 'witty-raven')
        period = request.args.get('period', 'YTD').upper()
        
        # Find user
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        # Calculate with new unified calculator
        start_date, end_date = get_period_dates(period, user_id=user.id)
        result = calculate_portfolio_performance(
            user.id, start_date, end_date, include_chart_data=True
        )
        
        # Get current cached data for comparison
        chart_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user.id, period=period
        ).first()
        
        cached_data = None
        if chart_cache:
            try:
                cached_chart = json.loads(chart_cache.chart_data)
                if isinstance(cached_chart, dict):
                    if 'datasets' in cached_chart:
                        datasets = cached_chart.get('datasets', [])
                        portfolio_data = datasets[0].get('data', []) if datasets else []
                        cached_data = {
                            'format': 'old_chartjs',
                            'last_value': portfolio_data[-1] if portfolio_data else None,
                            'generated_at': chart_cache.generated_at.isoformat()
                        }
                    elif 'portfolio_return' in cached_chart:
                        cached_data = {
                            'format': 'new_unified',
                            'portfolio_return': cached_chart.get('portfolio_return'),
                            'sp500_return': cached_chart.get('sp500_return'),
                            'generated_at': chart_cache.generated_at.isoformat()
                        }
            except Exception as e:
                cached_data = {'error': str(e)}
        
        return jsonify({
            'success': True,
            'test_info': {
                'username': username,
                'user_id': user.id,
                'period': period,
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat()
            },
            'new_calculator_result': {
                'portfolio_return': result['portfolio_return'],
                'sp500_return': result['sp500_return'],
                'metadata': result['metadata'],
                'chart_points': len(result['chart_data']) if result['chart_data'] else 0,
                'sample_chart_data': result['chart_data'][:3] if result['chart_data'] else []
            },
            'current_cached_data': cached_data,
            'comparison': {
                'cache_exists': chart_cache is not None,
                'match': (
                    cached_data.get('portfolio_return') == result['portfolio_return']
                    if cached_data and 'portfolio_return' in cached_data else None
                )
            },
            'expected_behavior': {
                'message': 'New calculator should show ~28.57% for witty-raven YTD',
                'formula': 'Modified Dietz: (V_end - V_start - CF_net) / (V_start + W * CF_net)',
                'note': 'Old cache may show 25.87% (wrong) until regenerated'
            }
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/compare-all-users-ytd')
@login_required
def compare_all_users_ytd():
    """
    Compare YTD performance for all users using new calculator vs cached values.
    
    Usage: /admin/compare-all-users-ytd
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from performance_calculator import calculate_portfolio_performance, get_period_dates
        from models import UserPortfolioChartCache
        import json
        
        period = 'YTD'
        start_date, end_date = get_period_dates(period)
        
        results = []
        users = User.query.all()
        
        for user in users:
            try:
                # Calculate with new calculator
                result = calculate_portfolio_performance(
                    user.id, start_date, end_date, include_chart_data=False
                )
                
                # Get cached value
                chart_cache = UserPortfolioChartCache.query.filter_by(
                    user_id=user.id, period=period
                ).first()
                
                cached_return = None
                if chart_cache:
                    try:
                        cached_chart = json.loads(chart_cache.chart_data)
                        if isinstance(cached_chart, dict) and 'datasets' in cached_chart:
                            datasets = cached_chart.get('datasets', [])
                            portfolio_data = datasets[0].get('data', []) if datasets else []
                            cached_return = portfolio_data[-1] if portfolio_data else None
                        elif isinstance(cached_chart, dict) and 'portfolio_return' in cached_chart:
                            cached_return = cached_chart.get('portfolio_return')
                    except:
                        pass
                
                discrepancy = None
                if cached_return is not None:
                    discrepancy = abs(result['portfolio_return'] - cached_return)
                
                results.append({
                    'username': user.username,
                    'user_id': user.id,
                    'new_calculator': result['portfolio_return'],
                    'cached_value': cached_return,
                    'discrepancy': round(discrepancy, 2) if discrepancy else None,
                    'needs_update': discrepancy > 0.1 if discrepancy else False,
                    'snapshots': result['metadata']['snapshots_count'],
                    'capital_deployed': result['metadata']['net_capital_deployed']
                })
            except Exception as e:
                results.append({
                    'username': user.username,
                    'user_id': user.id,
                    'error': str(e)
                })
        
        needs_update = sum(1 for r in results if r.get('needs_update'))
        has_discrepancy = sum(1 for r in results if r.get('discrepancy'))
        
        return jsonify({
            'success': True,
            'period': period,
            'date_range': f"{start_date.isoformat()} to {end_date.isoformat()}",
            'total_users': len(users),
            'summary': {
                'users_with_discrepancy': has_discrepancy,
                'users_needing_cache_update': needs_update,
                'message': (
                    f'{needs_update}/{len(users)} users need cache regeneration'
                    if needs_update > 0 else 'All caches match new calculator!'
                )
            },
            'results': results
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/update-single-user-cache')
@login_required
def update_single_user_cache():
    """
    Update performance cache for a single user using new calculator.
    
    Usage: /admin/update-single-user-cache?username=witty-raven&period=YTD
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from performance_calculator import calculate_portfolio_performance, get_period_dates
        from models import UserPortfolioChartCache
        import json
        
        # Get parameters
        username = request.args.get('username')
        period = request.args.get('period', 'YTD').upper()
        
        if not username:
            return jsonify({'error': 'username parameter required'}), 400
        
        # Find user
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        # Calculate with new calculator
        start_date, end_date = get_period_dates(period, user_id=user.id)
        result = calculate_portfolio_performance(
            user.id, start_date, end_date, include_chart_data=True
        )
        
        # Update cache
        chart_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user.id, period=period
        ).first()
        
        if chart_cache:
            chart_cache.chart_data = json.dumps(result)
            chart_cache.generated_at = datetime.now()
            action = 'updated'
        else:
            chart_cache = UserPortfolioChartCache(
                user_id=user.id,
                period=period,
                chart_data=json.dumps(result),
                generated_at=datetime.now()
            )
            db.session.add(chart_cache)
            action = 'created'
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'action': action,
            'user': username,
            'period': period,
            'new_return': result['portfolio_return'],
            'sp500_return': result['sp500_return'],
            'updated_at': datetime.now().isoformat(),
            'message': f'Cache {action} successfully with new calculator'
        })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/regenerate-all-performance-caches')
@login_required
def regenerate_all_performance_caches():
    """
    Regenerate ALL performance caches using new unified calculator.
    
    Usage: /admin/regenerate-all-performance-caches
    
    WARNING: Updates all 40 UserPortfolioChartCache entries. Takes 1-2 minutes.
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from performance_calculator import calculate_portfolio_performance, get_period_dates
        from models import UserPortfolioChartCache
        import json
        
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']
        users = User.query.all()
        
        results = {
            'updated': 0,
            'created': 0,
            'errors': 0,
            'details': []
        }
        
        for user in users:
            for period in periods:
                try:
                    # Calculate with new calculator
                    start_date, end_date = get_period_dates(period, user_id=user.id)
                    result = calculate_portfolio_performance(
                        user.id, start_date, end_date, include_chart_data=True
                    )
                    
                    # Update cache
                    chart_cache = UserPortfolioChartCache.query.filter_by(
                        user_id=user.id, period=period
                    ).first()
                    
                    if chart_cache:
                        chart_cache.chart_data = json.dumps(result)
                        chart_cache.generated_at = datetime.now()
                        results['updated'] += 1
                        action = 'updated'
                    else:
                        chart_cache = UserPortfolioChartCache(
                            user_id=user.id,
                            period=period,
                            chart_data=json.dumps(result),
                            generated_at=datetime.now()
                        )
                        db.session.add(chart_cache)
                        results['created'] += 1
                        action = 'created'
                    
                    results['details'].append({
                        'user': user.username,
                        'period': period,
                        'action': action,
                        'return': result['portfolio_return']
                    })
                    
                except Exception as e:
                    results['errors'] += 1
                    results['details'].append({
                        'user': user.username,
                        'period': period,
                        'error': str(e)
                    })
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'summary': {
                'total_users': len(users),
                'total_periods': len(periods),
                'total_caches': len(users) * len(periods),
                'updated': results['updated'],
                'created': results['created'],
                'errors': results['errors']
            },
            'results': results['details'],
            'message': (
                f"Regenerated {results['updated'] + results['created']} caches "
                f"({results['updated']} updated, {results['created']} created, "
                f"{results['errors']} errors)"
            )
        })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/verify-calculator-consistency')
@login_required
def verify_calculator_consistency():
    """
    Verify dashboard, leaderboard, and new calculator show consistent values.
    
    Usage: /admin/verify-calculator-consistency?username=witty-raven
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from performance_calculator import calculate_portfolio_performance, get_period_dates
        from models import UserPortfolioChartCache, LeaderboardCache
        import json
        
        # Get parameters
        username = request.args.get('username', 'witty-raven')
        period = request.args.get('period', 'YTD').upper()
        
        # Find user
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        # 1. New calculator result
        start_date, end_date = get_period_dates(period, user_id=user.id)
        new_calc_result = calculate_portfolio_performance(
            user.id, start_date, end_date, include_chart_data=False
        )
        
        # 2. Dashboard cache
        dashboard_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user.id, period=period
        ).first()
        
        dashboard_value = None
        if dashboard_cache:
            try:
                cached_data = json.loads(dashboard_cache.chart_data)
                logger.info(f"Dashboard cache keys: {list(cached_data.keys())}")
                if 'portfolio_return' in cached_data:
                    dashboard_value = cached_data['portfolio_return']
                    logger.info(f"Found portfolio_return in dashboard cache: {dashboard_value}")
                else:
                    logger.warning(f"portfolio_return NOT in dashboard cache. Keys present: {list(cached_data.keys())}")
            except Exception as e:
                logger.error(f"Error reading dashboard cache: {str(e)}")
        
        # 3. Leaderboard cache
        leaderboard_value = None
        leaderboard_cache = LeaderboardCache.query.filter_by(
            period=f"{period}_all_auth"
        ).first()
        
        if leaderboard_cache:
            try:
                leaderboard_data = json.loads(leaderboard_cache.leaderboard_data)
                user_entry = next((e for e in leaderboard_data if e.get('username') == username), None)
                if user_entry:
                    leaderboard_value = (
                        user_entry.get('portfolio_return') or
                        user_entry.get('performance_percent') or
                        user_entry.get('return')
                    )
            except:
                pass
        
        # Compare
        all_values = [
            new_calc_result['portfolio_return'],
            dashboard_value,
            leaderboard_value
        ]
        non_none_values = [v for v in all_values if v is not None]
        
        consistent = (
            len(set(non_none_values)) == 1
            if non_none_values else False
        )
        
        return jsonify({
            'success': True,
            'username': username,
            'period': period,
            'values': {
                'new_calculator': new_calc_result['portfolio_return'],
                'dashboard_cache': dashboard_value,
                'leaderboard_cache': leaderboard_value
            },
            'consistency_check': {
                'consistent': consistent,
                'message': (
                    ' All values match!' if consistent
                    else ' Discrepancies found - caches need regeneration'
                ),
                'max_discrepancy': (
                    round(max(non_none_values) - min(non_none_values), 2)
                    if len(non_none_values) > 1 else 0
                )
            },
            'cache_status': {
                'dashboard_cache_exists': dashboard_cache is not None,
                'leaderboard_cache_exists': leaderboard_cache is not None
            }
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/test-chart-generation')
@login_required
def test_chart_generation():
    """
    Test chart generation with new unified calculator.
    Usage: /admin/test-chart-generation?username=witty-raven&period=YTD
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from leaderboard_utils import generate_chart_from_snapshots
        import json
        
        username = request.args.get('username', 'witty-raven')
        period = request.args.get('period', 'YTD').upper()
        
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        # Test the new function
        logger.info(f"Testing chart generation for {username}, period {period}")
        chart_data = generate_chart_from_snapshots(user.id, period)
        
        if chart_data:
            return jsonify({
                'success': True,
                'username': username,
                'period': period,
                'chart_data_keys': list(chart_data.keys()),
                'portfolio_return': chart_data.get('portfolio_return'),
                'sp500_return': chart_data.get('sp500_return'),
                'has_datasets': 'datasets' in chart_data,
                'num_datapoints': len(chart_data.get('labels', [])) if chart_data.get('labels') else 0
            })
        else:
            return jsonify({
                'success': False,
                'error': 'generate_chart_from_snapshots returned None',
                'username': username,
                'period': period
            })
    
    except Exception as e:
        import traceback
        logger.error(f"Chart generation test failed: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/rebuild-cash-tracking')
@login_required
def rebuild_cash_tracking():
    """
    Rebuild max_cash_deployed and cash_proceeds for all users.
    
    This fixes historical data where transactions were added via admin interface
    without calling process_transaction().
    
    Usage: /admin/rebuild-cash-tracking?user_id=5 (single user)
           /admin/rebuild-cash-tracking (all users)
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from rebuild_cash_tracking import rebuild_user_cash_tracking, rebuild_all_users_cash_tracking
        
        user_id_param = request.args.get('user_id')
        
        if user_id_param:
            # Rebuild single user
            user_id = int(user_id_param)
            result = rebuild_user_cash_tracking(user_id)
            db.session.commit()
            
            return jsonify({
                'success': True,
                'mode': 'single_user',
                'result': result,
                'message': (
                    f"Rebuilt cash tracking for user {user_id}. "
                    f"Max deployed: ${result['old_max_deployed']:.2f}  ${result['new_max_deployed']:.2f}, "
                    f"Cash proceeds: ${result['old_cash_proceeds']:.2f}  ${result['new_cash_proceeds']:.2f}"
                )
            })
        else:
            # Rebuild all users
            summary = rebuild_all_users_cash_tracking()
            
            return jsonify({
                'success': True,
                'mode': 'all_users',
                'summary': summary,
                'message': (
                    f"Rebuilt cash tracking for {summary['total_users']} users. "
                    f"{summary['users_changed']} changed, {summary['users_unchanged']} unchanged."
                )
            })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/diagnose-user-after-trade')
@login_required
def diagnose_user_after_trade():
    """
    Detailed diagnostic for a single user after buy/sell trade.
    Shows cash tracking, transactions, portfolio value, and gains.
    
    Usage: /admin/diagnose-user-after-trade?user_id=5
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import Transaction, PortfolioSnapshot
        from portfolio_performance import PortfolioPerformanceCalculator
        from datetime import date
        
        user_id = request.args.get('user_id', current_user.id, type=int)
        user = User.query.get(user_id)
        
        if not user:
            return jsonify({'error': f'User {user_id} not found'}), 404
        
        # Get all transactions ordered by time
        transactions = Transaction.query.filter_by(user_id=user_id).order_by(Transaction.timestamp).all()
        
        # Build transaction history
        transaction_history = []
        simulated_deployed = 0.0
        simulated_proceeds = 0.0
        
        for txn in transactions:
            value = txn.quantity * txn.price
            
            # Simulate cash tracking
            if txn.transaction_type in ('buy', 'initial'):
                if simulated_proceeds >= value:
                    simulated_proceeds -= value
                    capital_deployed = 0
                else:
                    capital_deployed = value - simulated_proceeds
                    simulated_proceeds = 0
                    simulated_deployed += capital_deployed
            elif txn.transaction_type == 'sell':
                simulated_proceeds += value
                capital_deployed = 0
            
            transaction_history.append({
                'timestamp': txn.timestamp.isoformat(),
                'type': txn.transaction_type,
                'ticker': txn.ticker,
                'quantity': txn.quantity,
                'price': txn.price,
                'value': value,
                'capital_deployed': capital_deployed,
                'simulated_proceeds_after': simulated_proceeds,
                'simulated_deployed_after': simulated_deployed
            })
        
        # Get current portfolio
        stocks = Stock.query.filter_by(user_id=user_id).all()
        portfolio_stocks = []
        total_cost_basis = 0
        total_current_value = 0
        
        calculator = PortfolioPerformanceCalculator()
        for stock in stocks:
            stock_data = calculator.get_stock_data(stock.ticker)
            current_price = stock_data.get('price') if stock_data else None
            
            if current_price:
                current_value = stock.quantity * current_price
                cost_basis = stock.quantity * stock.purchase_price
                gain_loss = current_value - cost_basis
                
                portfolio_stocks.append({
                    'ticker': stock.ticker,
                    'quantity': stock.quantity,
                    'purchase_price': stock.purchase_price,
                    'current_price': current_price,
                    'cost_basis': cost_basis,
                    'current_value': current_value,
                    'gain_loss': gain_loss
                })
                
                total_cost_basis += cost_basis
                total_current_value += current_value
        
        total_gain_loss = total_current_value - total_cost_basis
        
        # Compare actual vs simulated
        cash_tracking_match = (
            abs(user.max_cash_deployed - simulated_deployed) < 0.01 and
            abs(user.cash_proceeds - simulated_proceeds) < 0.01
        )
        
        return jsonify({
            'success': True,
            'user': {
                'id': user.id,
                'username': user.username
            },
            'cash_tracking': {
                'actual': {
                    'max_cash_deployed': user.max_cash_deployed,
                    'cash_proceeds': user.cash_proceeds
                },
                'simulated': {
                    'max_cash_deployed': simulated_deployed,
                    'cash_proceeds': simulated_proceeds
                },
                'match': cash_tracking_match,
                'discrepancy': {
                    'deployed': user.max_cash_deployed - simulated_deployed,
                    'proceeds': user.cash_proceeds - simulated_proceeds
                }
            },
            'transactions': {
                'total': len(transactions),
                'buy_count': len([t for t in transactions if t.transaction_type in ('buy', 'initial')]),
                'sell_count': len([t for t in transactions if t.transaction_type == 'sell']),
                'history': transaction_history
            },
            'portfolio': {
                'stocks': portfolio_stocks,
                'total_cost_basis': total_cost_basis,
                'total_current_value': total_current_value,
                'total_gain_loss': total_gain_loss,
                'total_gain_loss_percent': (total_gain_loss / total_cost_basis * 100) if total_cost_basis > 0 else 0
            },
            'summary': {
                'cash_tracking_correct': cash_tracking_match,
                'total_capital_deployed': user.max_cash_deployed,
                'available_cash_proceeds': user.cash_proceeds,
                'portfolio_value': total_current_value,
                'total_value_with_cash': total_current_value + user.cash_proceeds,
                'unrealized_gains': total_gain_loss,
                'notes': [
                    'max_cash_deployed = maximum cash ever deployed (never decreases)',
                    'cash_proceeds = available from sales (used for future purchases)',
                    'total_value_with_cash = portfolio value + cash proceeds'
                ]
            }
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/diagnose-all-users-cash-tracking')
@login_required
def diagnose_all_users_cash_tracking():
    """
    Check max_cash_deployed tracking and transaction history for ALL users.
    
    Shows which users have transactions that should have changed max_cash_deployed.
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import Transaction, PortfolioSnapshot
        from datetime import date
        
        all_users = User.query.all()
        results = []
        
        for user in all_users:
            # Get transactions
            transactions = Transaction.query.filter_by(user_id=user.id).order_by(Transaction.timestamp).all()
            
            # Analyze transaction types
            initial_txns = [t for t in transactions if t.transaction_type == 'initial']
            buy_txns = [t for t in transactions if t.transaction_type == 'buy']
            sell_txns = [t for t in transactions if t.transaction_type == 'sell']
            
            # Check first and last snapshot for this user
            first_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id).order_by(PortfolioSnapshot.date.asc()).first()
            last_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id).order_by(PortfolioSnapshot.date.desc()).first()
            
            # Simulate what max_cash_deployed SHOULD be
            simulated_deployed = 0.0
            simulated_proceeds = 0.0
            for txn in transactions:
                value = txn.quantity * txn.price
                if txn.transaction_type in ('buy', 'initial'):
                    if simulated_proceeds >= value:
                        simulated_proceeds -= value
                    else:
                        new_capital = value - simulated_proceeds
                        simulated_proceeds = 0
                        simulated_deployed += new_capital
                elif txn.transaction_type == 'sell':
                    simulated_proceeds += value
            
            user_data = {
                'user_id': user.id,
                'username': user.username,
                'current_max_deployed': user.max_cash_deployed,
                'simulated_max_deployed': round(simulated_deployed, 2),
                'match': abs(user.max_cash_deployed - simulated_deployed) < 0.01,
                'transactions': {
                    'total': len(transactions),
                    'initial': len(initial_txns),
                    'buy': len(buy_txns),
                    'sell': len(sell_txns)
                },
                'snapshots': {
                    'first_date': first_snapshot.date.isoformat() if first_snapshot else None,
                    'first_deployed': first_snapshot.max_cash_deployed if first_snapshot else None,
                    'last_date': last_snapshot.date.isoformat() if last_snapshot else None,
                    'last_deployed': last_snapshot.max_cash_deployed if last_snapshot else None,
                    'changed_over_time': (
                        first_snapshot.max_cash_deployed != last_snapshot.max_cash_deployed 
                        if first_snapshot and last_snapshot else False
                    )
                }
            }
            
            results.append(user_data)
        
        # Summary
        users_with_issues = [r for r in results if not r['match']]
        users_with_unchanging_deployed = [r for r in results if not r['snapshots']['changed_over_time'] and r['transactions']['total'] > 1]
        
        return jsonify({
            'success': True,
            'total_users': len(all_users),
            'results': results,
            'issues': {
                'users_with_mismatched_values': len(users_with_issues),
                'details': users_with_issues if users_with_issues else None
            },
            'analysis': {
                'users_with_unchanging_deployed': len(users_with_unchanging_deployed),
                'note': 'Unchanging max_cash_deployed is CORRECT if users only trade with proceeds from sales'
            }
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/admin/diagnose-max-cash-deployed')
@login_required
def diagnose_max_cash_deployed():
    """
    Check if max_cash_deployed is being properly tracked in snapshots.
    
    Usage: /admin/diagnose-max-cash-deployed?username=witty-raven
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import PortfolioSnapshot
        from datetime import date
        
        username = request.args.get('username', 'witty-raven')
        
        # Get user
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        # Get YTD snapshots
        start_date = date(2025, 1, 1)
        end_date = date.today()
        
        snapshots = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.user_id == user.id,
            PortfolioSnapshot.date >= start_date,
            PortfolioSnapshot.date <= end_date
        ).order_by(PortfolioSnapshot.date.asc()).all()
        
        if not snapshots:
            return jsonify({'error': 'No snapshots found'}), 404
        
        # Analyze max_cash_deployed changes
        first = snapshots[0]
        last = snapshots[-1]
        
        changes = []
        prev_deployed = first.max_cash_deployed
        
        for snapshot in snapshots:
            if snapshot.max_cash_deployed != prev_deployed:
                changes.append({
                    'date': snapshot.date.isoformat(),
                    'old_deployed': prev_deployed,
                    'new_deployed': snapshot.max_cash_deployed,
                    'change': snapshot.max_cash_deployed - prev_deployed,
                    'total_value': snapshot.total_value,
                    'stock_value': snapshot.stock_value,
                    'cash_proceeds': snapshot.cash_proceeds
                })
                prev_deployed = snapshot.max_cash_deployed
        
        # Sample snapshots
        sample_snapshots = []
        indices = [0, len(snapshots)//4, len(snapshots)//2, 3*len(snapshots)//4, -1]
        for i in indices:
            s = snapshots[i]
            sample_snapshots.append({
                'date': s.date.isoformat(),
                'total_value': s.total_value,
                'stock_value': s.stock_value,
                'cash_proceeds': s.cash_proceeds,
                'max_cash_deployed': s.max_cash_deployed
            })
        
        return jsonify({
            'success': True,
            'user': username,
            'period': f'{start_date} to {end_date}',
            'summary': {
                'total_snapshots': len(snapshots),
                'first_deployed': first.max_cash_deployed,
                'last_deployed': last.max_cash_deployed,
                'net_change': last.max_cash_deployed - first.max_cash_deployed,
                'changes_count': len(changes)
            },
            'deployed_changes': changes,
            'sample_snapshots': sample_snapshots,
            'diagnosis': {
                'issue': 'max_cash_deployed not changing' if len(changes) == 0 else 'max_cash_deployed is being tracked',
                'impact': (
                    'Modified Dietz collapses to simple % when CF_net = 0'
                    if len(changes) == 0
                    else 'Modified Dietz should account for capital deployment timing'
                ),
                'question': 'Is this a paper trading app where users start with virtual cash and never add more?'
            }
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

# End of unified calculator test routes
# ============================================================================

@app.route('/admin/test-database-persistence')
@login_required
def test_database_persistence():
    """Test if database changes actually persist - debugging cache resurrection"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import LeaderboardCache, UserPortfolioChartCache
        
        # Test both cache tables
        results = {}
        
        # ===== TEST 1: LeaderboardCache =====
        lb_count_before = LeaderboardCache.query.count()
        chart_count_before = UserPortfolioChartCache.query.count()
        
        results['before_any_changes'] = {
            'leaderboard_cache_count': lb_count_before,
            'chart_cache_count': chart_count_before
        }
        
        # Only delete if they exist
        if lb_count_before > 0:
            LeaderboardCache.query.delete()
        if chart_count_before > 0:
            UserPortfolioChartCache.query.delete()
        
        results['after_delete_before_commit'] = {
            'leaderboard_cache_count': LeaderboardCache.query.count(),
            'chart_cache_count': UserPortfolioChartCache.query.count()
        }
        
        # Commit
        db.session.commit()
        
        results['after_commit_same_request'] = {
            'leaderboard_cache_count': LeaderboardCache.query.count(),
            'chart_cache_count': UserPortfolioChartCache.query.count()
        }
        
        # Force fresh query
        db.session.expire_all()
        
        results['after_expire_fresh_query'] = {
            'leaderboard_cache_count': LeaderboardCache.query.count(),
            'chart_cache_count': UserPortfolioChartCache.query.count()
        }
        
        return jsonify({
            'success': True,
            'test_results': results,
            'diagnosis': {
                'leaderboard_delete_persisted': results['after_expire_fresh_query']['leaderboard_cache_count'] == 0,
                'chart_delete_persisted': results['after_expire_fresh_query']['chart_cache_count'] == 0,
                'issue_detected': results['after_expire_fresh_query']['leaderboard_cache_count'] > 0 or results['after_expire_fresh_query']['chart_cache_count'] > 0,
                'likely_cause': 'Database replication lag or read-replica caching' if results['after_expire_fresh_query']['leaderboard_cache_count'] > 0 else 'Unknown'
            },
            'next_action': 'Wait 5 seconds, then visit /admin/check-all-caches to see if caches reappear'
        })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/compare-chart-vs-live/<username>/<period>')
@login_required
def compare_chart_vs_live(username, period):
    """Compare cached chart data vs live calculation for debugging mismatches"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import User, UserPortfolioChartCache
        from portfolio_performance import PortfolioPerformanceCalculator
        import json
        
        # Find user
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        # Get cached chart data
        chart_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user.id, period=period.upper()
        ).first()
        
        cached_data = None
        if chart_cache:
            cached_chart = json.loads(chart_cache.chart_data)
            datasets = cached_chart.get('datasets', [])
            if datasets:
                portfolio_data = datasets[0].get('data', [])
                sp500_data = datasets[1].get('data', []) if len(datasets) > 1 else []
                cached_data = {
                    'last_portfolio_value': portfolio_data[-1] if portfolio_data else None,
                    'last_sp500_value': sp500_data[-1] if sp500_data else None,
                    'portfolio_data_points': len(portfolio_data),
                    'sp500_data_points': len(sp500_data),
                    'labels': cached_chart.get('labels', [])[:3] + ['...'] + cached_chart.get('labels', [])[-3:],
                    'generated_at': chart_cache.generated_at.isoformat()
                }
        
        # Get live calculation (what dashboard uses when no cache)
        calculator = PortfolioPerformanceCalculator()
        live_data = calculator.get_performance_data(user.id, period.upper())
        
        live_performance = {
            'portfolio_return': live_data.get('portfolio_return'),
            'sp500_return': live_data.get('sp500_return'),
            'chart_data_points': len(live_data.get('chart_data', []))
        }
        
        return jsonify({
            'success': True,
            'user': username,
            'period': period.upper(),
            'cached_chart_data': cached_data,
            'live_calculation': live_performance,
            'mismatch': cached_data['last_portfolio_value'] != live_performance['portfolio_return'] if cached_data else None,
            'diagnosis': 'Values should match - if not, chart cache generation has different logic than live calculation'
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-all-caches')
@login_required
def check_all_caches():
    """Check ALL cache tables to see what's actually stored"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import LeaderboardCache, UserPortfolioChartCache
        import json
        
        # Get ALL LeaderboardCache entries
        all_leaderboard_caches = LeaderboardCache.query.all()
        leaderboard_summary = []
        for cache in all_leaderboard_caches:
            try:
                data = json.loads(cache.leaderboard_data)
                sample_entry = data[0] if data else None
            except:
                sample_entry = None
            
            leaderboard_summary.append({
                'period': cache.period,
                'generated_at': cache.generated_at.isoformat() if cache.generated_at else None,
                'has_rendered_html': cache.rendered_html is not None,
                'entry_count': len(data) if isinstance(data, list) else 0,
                'sample_performance': sample_entry.get('performance_percent') if sample_entry else None,
                'sample_user': sample_entry.get('username') if sample_entry else None
            })
        
        # Get ALL UserPortfolioChartCache entries
        all_chart_caches = UserPortfolioChartCache.query.all()
        chart_summary = {}
        for cache in all_chart_caches:
            if cache.user_id not in chart_summary:
                chart_summary[cache.user_id] = {}
            
            try:
                chart_data = json.loads(cache.chart_data)
                datasets = chart_data.get('datasets', [])
                last_value = datasets[0].get('data', [])[-1] if datasets else None
            except:
                last_value = None
            
            chart_summary[cache.user_id][cache.period] = {
                'generated_at': cache.generated_at.isoformat(),
                'last_performance_value': last_value
            }
        
        return jsonify({
            'success': True,
            'timestamp': datetime.now().isoformat(),
            'leaderboard_cache_count': len(all_leaderboard_caches),
            'leaderboard_cache_entries': leaderboard_summary,
            'chart_cache_count': len(all_chart_caches),
            'chart_cache_by_user': chart_summary,
            'diagnosis': {
                'leaderboard_cache_recreated': len(all_leaderboard_caches) > 0,
                'chart_cache_exists': len(all_chart_caches) > 0,
                'next_steps': 'If leaderboard_cache_count > 0, something recreated the cache after clear'
            }
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/trace-leaderboard-value/<username>/<period>')
@login_required
def trace_leaderboard_value(username, period):
    """
    Trace EXACTLY where a leaderboard value comes from
    
    Example: /admin/trace-leaderboard-value/witty-raven/YTD
    This will show where the 6.69% value originates
    """
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import User, LeaderboardCache, UserPortfolioChartCache
        import json
        
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        trace = {
            'username': username,
            'user_id': user.id,
            'period': period,
            'timestamp': datetime.now().isoformat(),
            'data_sources': {}
        }
        
        # 1. Check LeaderboardCache table (what get_leaderboard_data reads)
        cache_key = f"{period}_all"
        leaderboard_cache = LeaderboardCache.query.filter_by(period=cache_key).first()
        
        if leaderboard_cache:
            cached_data = json.loads(leaderboard_cache.leaderboard_data)
            user_entry = next((e for e in cached_data if e['user_id'] == user.id), None)
            trace['data_sources']['leaderboard_cache'] = {
                'exists': True,
                'cache_key': cache_key,
                'generated_at': leaderboard_cache.generated_at.isoformat() if leaderboard_cache.generated_at else None,
                'user_performance': user_entry['performance_percent'] if user_entry else None,
                'full_entry': user_entry
            }
        else:
            trace['data_sources']['leaderboard_cache'] = {
                'exists': False,
                'note': 'Should trigger calculate_leaderboard_data() fallback'
            }
        
        # 2. Check UserPortfolioChartCache (what NEW code should use)
        chart_cache = UserPortfolioChartCache.query.filter_by(user_id=user.id, period=period).first()
        
        if chart_cache:
            chart_data = json.loads(chart_cache.chart_data)
            datasets = chart_data.get('datasets', [])
            portfolio_data = datasets[0].get('data', []) if datasets else []
            last_value = portfolio_data[-1] if portfolio_data else None
            
            trace['data_sources']['user_portfolio_chart_cache'] = {
                'exists': True,
                'generated_at': chart_cache.generated_at.isoformat(),
                'last_performance_value': last_value,
                'data_points_count': len(portfolio_data),
                'note': 'This is what calculate_leaderboard_data() should return'
            }
        else:
            trace['data_sources']['user_portfolio_chart_cache'] = {
                'exists': False,
                'error': 'Chart cache missing - calculate_leaderboard_data() will skip this user!'
            }
        
        # 3. Call get_leaderboard_data() and see what it returns
        from leaderboard_utils import get_leaderboard_data
        leaderboard_result = get_leaderboard_data(period, limit=50, category='all')
        user_leaderboard_entry = next((e for e in leaderboard_result if e['user_id'] == user.id), None)
        
        trace['data_sources']['get_leaderboard_data_result'] = {
            'user_found': user_leaderboard_entry is not None,
            'performance_percent': user_leaderboard_entry['performance_percent'] if user_leaderboard_entry else None,
            'full_entry': user_leaderboard_entry,
            'note': 'This is what the leaderboard page actually displays'
        }
        
        # 4. Call calculate_leaderboard_data() directly to see what NEW code returns
        from leaderboard_utils import calculate_leaderboard_data
        calculated_result = calculate_leaderboard_data(period, limit=50, category='all')
        user_calculated_entry = next((e for e in calculated_result if e['user_id'] == user.id), None)
        
        trace['data_sources']['calculate_leaderboard_data_result'] = {
            'user_found': user_calculated_entry is not None,
            'performance_percent': user_calculated_entry['performance_percent'] if user_calculated_entry else None,
            'full_entry': user_calculated_entry,
            'note': 'This is what NEW fixed code calculates'
        }
        
        # 5. Compare all sources
        trace['comparison'] = {
            'leaderboard_cache_value': trace['data_sources']['leaderboard_cache'].get('user_performance'),
            'chart_cache_value': trace['data_sources']['user_portfolio_chart_cache'].get('last_performance_value'),
            'get_leaderboard_data_value': trace['data_sources']['get_leaderboard_data_result'].get('performance_percent'),
            'calculate_leaderboard_data_value': trace['data_sources']['calculate_leaderboard_data_result'].get('performance_percent'),
            'mismatch_detected': len(set(filter(None, [
                trace['data_sources']['leaderboard_cache'].get('user_performance'),
                trace['data_sources']['user_portfolio_chart_cache'].get('last_performance_value'),
                trace['data_sources']['get_leaderboard_data_result'].get('performance_percent'),
                trace['data_sources']['calculate_leaderboard_data_result'].get('performance_percent')
            ]))) > 1
        }
        
        return jsonify({
            'success': True,
            'trace': trace
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-snapshot-data')
@login_required
def check_snapshot_data():
    """Check if both portfolio snapshots AND S&P 500 data exist for recent dates"""
    try:
        if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, timedelta
        from models import PortfolioSnapshot, MarketData, User
        from sqlalchemy import func
        
        # Check last 7 days
        today = date.today()
        results = {
            'check_date': today.isoformat(),
            'days_checked': [],
            'summary': {
                'total_days': 0,
                'days_with_portfolio_data': 0,
                'days_with_sp500_data': 0,
                'days_with_both': 0
            }
        }
        
        for days_back in range(7):
            check_date = today - timedelta(days=days_back)
            
            # Skip weekends
            if check_date.weekday() >= 5:
                continue
            
            # Count portfolio snapshots for this date
            portfolio_count = PortfolioSnapshot.query.filter_by(date=check_date).count()
            
            # Get user breakdown
            user_snapshots = db.session.query(
                PortfolioSnapshot.user_id, 
                PortfolioSnapshot.total_value
            ).filter_by(date=check_date).all()
            
            # Check S&P 500 data for this date
            sp500_data = MarketData.query.filter_by(
                ticker='SPY_SP500',
                date=check_date
            ).first()
            
            day_result = {
                'date': check_date.isoformat(),
                'weekday': check_date.strftime('%A'),
                'portfolio_snapshots': portfolio_count,
                'sp500_data_exists': sp500_data is not None,
                'sp500_value': float(sp500_data.close_price) if sp500_data else None,
                'user_details': [
                    {'user_id': uid, 'total_value': float(val)}
                    for uid, val in user_snapshots
                ]
            }
            
            results['days_checked'].append(day_result)
            results['summary']['total_days'] += 1
            
            if portfolio_count > 0:
                results['summary']['days_with_portfolio_data'] += 1
            if sp500_data:
                results['summary']['days_with_sp500_data'] += 1
            if portfolio_count > 0 and sp500_data:
                results['summary']['days_with_both'] += 1
        
        # Add diagnosis
        results['diagnosis'] = {
            'portfolio_snapshots_ok': results['summary']['days_with_portfolio_data'] == results['summary']['total_days'],
            'sp500_data_ok': results['summary']['days_with_sp500_data'] == results['summary']['total_days'],
            'system_healthy': results['summary']['days_with_both'] == results['summary']['total_days']
        }
        
        return jsonify(results)
        
    except Exception as e:
        logger.error(f"Error checking snapshot data: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/regenerate-chart-cache')
@login_required
def regenerate_chart_cache():
    """Admin endpoint to force regenerate chart cache for all users"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from leaderboard_utils import update_leaderboard_cache
        
        results = {
            'started_at': datetime.now().isoformat(),
            'cache_updated': False,
            'leaderboard_entries_updated': 0
        }
        
        # Force regenerate all chart caches
        updated_count = update_leaderboard_cache()
        db.session.commit()  # Commit the cache updates
        
        results['leaderboard_entries_updated'] = updated_count
        results['cache_updated'] = True
        results['completed_at'] = datetime.now().isoformat()
        
        return jsonify({
            'success': True,
            'message': f'Chart cache regenerated for {updated_count} periods',
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Chart cache regeneration error: {str(e)}")
        db.session.rollback()  # Rollback on error
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-cron-health')
@login_required
def check_cron_health():
    """Check if market-close cron is running and collecting S&P 500 data"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData, PortfolioSnapshot, User
        from datetime import timedelta
        
        today_et = get_market_date()
        results = {
            'today': today_et.isoformat(),
            'cron_schedule': '0 20 * * 1-5 (4:00 PM ET daily)',
            'last_10_weekdays': []
        }
        
        # Check last 10 weekdays for S&P 500 data collection
        check_date = today_et
        days_checked = 0
        
        while days_checked < 10:
            if check_date.weekday() < 5:  # Weekday
                # Check S&P 500 data
                sp500_data = MarketData.query.filter_by(
                    ticker='SPY_SP500',
                    date=check_date
                ).first()
                
                # Check portfolio snapshots (at least one user should have a snapshot)
                snapshot_count = PortfolioSnapshot.query.filter_by(date=check_date).count()
                
                day_result = {
                    'date': check_date.isoformat(),
                    'weekday': check_date.strftime('%A'),
                    'has_sp500_data': sp500_data is not None,
                    'sp500_value': float(sp500_data.close_price) if sp500_data else None,
                    'portfolio_snapshots': snapshot_count,
                    'status': 'OK' if sp500_data and snapshot_count > 0 else 'FAILED'
                }
                
                if not sp500_data:
                    day_result['issue'] = 'Missing S&P 500 data - cron may have failed'
                if snapshot_count == 0:
                    day_result['issue'] = 'Missing portfolio snapshots - cron may have failed'
                
                results['last_10_weekdays'].append(day_result)
                days_checked += 1
            
            check_date -= timedelta(days=1)
        
        # Count failures
        failures = [d for d in results['last_10_weekdays'] if d['status'] == 'FAILED']
        results['failure_count'] = len(failures)
        results['success_rate'] = f"{((10 - len(failures)) / 10) * 100}%"
        
        if failures:
            results['DIAGNOSIS'] = f"Cron failing {len(failures)}/10 days - investigate AlphaVantage API or cron execution"
        else:
            results['DIAGNOSIS'] = "All crons successful"
        
        return jsonify({
            'success': True,
            'results': results
        })
    
    except Exception as e:
        logger.error(f"Cron health check error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/backfill-sp500/<date_str>')
@login_required
def backfill_sp500_for_date(date_str):
    """Backfill S&P 500 data for a specific date (format: YYYY-MM-DD)"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from datetime import datetime
        from models import MarketData
        from portfolio_performance import PortfolioPerformanceCalculator
        
        target_date = datetime.strptime(date_str, '%Y-%m-%d').date()
        
        # Get HISTORICAL SPY price for that specific date
        calculator = PortfolioPerformanceCalculator()
        spy_price = calculator.get_historical_price('SPY', target_date, force_fetch=True)
        
        if not spy_price:
            return jsonify({
                'success': False,
                'error': f'Failed to fetch historical SPY data for {date_str}',
                'date': date_str
            }), 500
        
        sp500_value = spy_price * 10  # Convert SPY to S&P 500 approximation
        
        # Check if entry exists
        existing = MarketData.query.filter_by(
            ticker='SPY_SP500',
            date=target_date
        ).first()
        
        if existing:
            old_value = existing.close_price
            existing.close_price = sp500_value
            action = 'updated'
        else:
            market_data = MarketData(
                ticker='SPY_SP500',
                date=target_date,
                close_price=sp500_value
            )
            db.session.add(market_data)
            old_value = None
            action = 'created'
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'action': action,
            'date': date_str,
            'sp500_value': sp500_value,
            'spy_price': spy_price,
            'old_value': old_value
        })
    
    except Exception as e:
        logger.error(f"Backfill S&P 500 error: {str(e)}")
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/collect-sp500-manual')
@login_required
def collect_sp500_manual():
    """Admin endpoint to manually collect S&P 500 data for today"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import MarketData
        from portfolio_performance import PortfolioPerformanceCalculator
        from zoneinfo import ZoneInfo
        
        today_et = datetime.now(ZoneInfo('America/New_York')).date()
        
        results = {
            'date': today_et.isoformat(),
            'spy_data_fetched': False,
            'sp500_value': None,
            'database_updated': False,
            'errors': []
        }
        
        # Fetch SPY data
        calculator = PortfolioPerformanceCalculator()
        spy_data = calculator.get_stock_data('SPY')
        
        results['spy_api_response'] = spy_data
        
        if spy_data and spy_data.get('price'):
            results['spy_data_fetched'] = True
            spy_price = spy_data['price']
            sp500_value = spy_price * 10
            results['sp500_value'] = sp500_value
            
            # Check if entry exists
            existing = MarketData.query.filter_by(
                ticker='SPY_SP500',
                date=today_et
            ).first()
            
            if existing:
                existing.close_price = sp500_value
                results['action'] = 'updated'
                results['old_value'] = float(existing.close_price)
            else:
                market_data = MarketData(
                    ticker='SPY_SP500',
                    date=today_et,
                    close_price=sp500_value
                )
                db.session.add(market_data)
                results['action'] = 'created'
            
            db.session.commit()
            results['database_updated'] = True
            
        else:
            results['errors'].append("SPY data fetch returned None or no price")
        
        return jsonify({
            'success': results['database_updated'],
            'results': results
        })
    
    except Exception as e:
        db.session.rollback()
        logger.error(f"Manual S&P 500 collection error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/add-intraday-cash-fields')
@login_required
def add_intraday_cash_fields():
    """Admin endpoint to add cash tracking fields to portfolio_snapshot_intraday table"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from sqlalchemy import text
        
        results = {
            'columns_added': [],
            'columns_already_exist': [],
            'errors': []
        }
        
        # Add stock_value column
        try:
            db.session.execute(text("""
                ALTER TABLE portfolio_snapshot_intraday 
                ADD COLUMN IF NOT EXISTS stock_value FLOAT DEFAULT 0.0
            """))
            results['columns_added'].append('stock_value')
        except Exception as e:
            if 'already exists' in str(e).lower():
                results['columns_already_exist'].append('stock_value')
            else:
                results['errors'].append(f"stock_value: {str(e)}")
        
        # Add cash_proceeds column
        try:
            db.session.execute(text("""
                ALTER TABLE portfolio_snapshot_intraday 
                ADD COLUMN IF NOT EXISTS cash_proceeds FLOAT DEFAULT 0.0
            """))
            results['columns_added'].append('cash_proceeds')
        except Exception as e:
            if 'already exists' in str(e).lower():
                results['columns_already_exist'].append('cash_proceeds')
            else:
                results['errors'].append(f"cash_proceeds: {str(e)}")
        
        # Add max_cash_deployed column
        try:
            db.session.execute(text("""
                ALTER TABLE portfolio_snapshot_intraday 
                ADD COLUMN IF NOT EXISTS max_cash_deployed FLOAT DEFAULT 0.0
            """))
            results['columns_added'].append('max_cash_deployed')
        except Exception as e:
            if 'already exists' in str(e).lower():
                results['columns_already_exist'].append('max_cash_deployed')
            else:
                results['errors'].append(f"max_cash_deployed: {str(e)}")
        
        # For existing rows, set stock_value = total_value (conservative approach)
        try:
            db.session.execute(text("""
                UPDATE portfolio_snapshot_intraday 
                SET stock_value = total_value,
                    cash_proceeds = 0.0,
                    max_cash_deployed = 0.0
                WHERE stock_value IS NULL
            """))
            results['existing_rows_updated'] = True
        except Exception as e:
            results['errors'].append(f"update existing rows: {str(e)}")
        
        db.session.commit()
        
        return jsonify({
            'success': len(results['errors']) == 0,
            'message': 'Migration completed successfully' if len(results['errors']) == 0 else 'Migration completed with errors',
            'results': results
        })
    
    except Exception as e:
        db.session.rollback()
        logger.error(f"Intraday cash fields migration error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/verify-snapshot-data-flow')
@login_required
def verify_snapshot_data_flow():
    """Admin endpoint to verify entire data flow: snapshots  cache  charts"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from datetime import date, timedelta
        from models import PortfolioSnapshot, User, UserPortfolioChartCache, MarketData
        from zoneinfo import ZoneInfo
        
        # Use Eastern Time to match market close logic
        today_et = datetime.now(ZoneInfo('America/New_York')).date()
        results = {
            'date_checked': today_et.isoformat(),
            'date_checked_timezone': 'America/New_York',
            'portfolio_snapshots': {},
            'sp500_data': {},
            'chart_cache': {},
            'issues': []
        }
        
        # Check portfolio snapshots for today (ET)
        users = User.query.all()
        for user in users:
            snapshot = PortfolioSnapshot.query.filter_by(
                user_id=user.id,
                date=today_et
            ).first()
            
            results['portfolio_snapshots'][user.username] = {
                'exists': snapshot is not None,
                'total_value': float(snapshot.total_value) if snapshot else None,
                'stock_value': float(snapshot.stock_value) if snapshot and hasattr(snapshot, 'stock_value') else None,
                'cash_proceeds': float(snapshot.cash_proceeds) if snapshot and hasattr(snapshot, 'cash_proceeds') else None,
                'date': snapshot.date.isoformat() if snapshot else None
            }
            
            if not snapshot:
                results['issues'].append(f" No snapshot for {user.username} on {today_et}")
        
        # Check S&P 500 data for today (ET) - check both SPY_SP500 and recent dates
        sp500_today = MarketData.query.filter_by(
            ticker='SPY_SP500',
            date=today_et
        ).first()
        
        # Also check last 3 days of S&P 500 data for debugging
        sp500_recent = MarketData.query.filter_by(ticker='SPY_SP500')\
            .filter(MarketData.date >= today_et - timedelta(days=3))\
            .order_by(MarketData.date.desc()).all()
        
        results['sp500_data']['today'] = {
            'exists': sp500_today is not None,
            'value': float(sp500_today.close_price) if sp500_today else None,
            'date': sp500_today.date.isoformat() if sp500_today else None
        }
        
        results['sp500_data']['recent_dates'] = [
            {
                'date': s.date.isoformat(),
                'value': float(s.close_price)
            } for s in sp500_recent
        ]
        
        if not sp500_today:
            results['issues'].append(f" No S&P 500 data for {today_et}")
        
        # Check chart cache for each user
        for user in users:
            user_charts = UserPortfolioChartCache.query.filter_by(user_id=user.id).all()
            results['chart_cache'][user.username] = {
                'total_cached_periods': len(user_charts),
                'periods': {}
            }
            
            for chart in user_charts:
                import json
                chart_data = json.loads(chart.chart_data) if chart.chart_data else {}
                data_points = len(chart_data.get('labels', []))
                
                results['chart_cache'][user.username]['periods'][chart.period] = {
                    'generated_at': chart.generated_at.isoformat() if chart.generated_at else None,
                    'data_points': data_points,
                    'latest_label': chart_data.get('labels', [])[-1] if data_points > 0 else None
                }
                
                # Check if today's data is in 1M and 3M charts
                if chart.period in ['1M', '3M', 'YTD', '1Y']:
                    labels = chart_data.get('labels', [])
                    today_str = today_et.isoformat()
                    if today_str not in labels:
                        results['issues'].append(f" {user.username}: {chart.period} chart missing today's data ({today_str})")
        
        # Check for YTD/1Y data availability for witty-raven
        witty_raven = User.query.filter_by(username='witty-raven').first()
        if witty_raven:
            # Get first snapshot date
            first_snapshot = PortfolioSnapshot.query.filter_by(user_id=witty_raven.id)\
                .order_by(PortfolioSnapshot.date.asc()).first()
            
            all_snapshots = PortfolioSnapshot.query.filter_by(user_id=witty_raven.id)\
                .order_by(PortfolioSnapshot.date.asc()).all()
            
            results['witty_raven_analysis'] = {
                'first_snapshot_date': first_snapshot.date.isoformat() if first_snapshot else None,
                'total_snapshots': len(all_snapshots),
                'date_range': f"{first_snapshot.date.isoformat()} to {all_snapshots[-1].date.isoformat()}" if all_snapshots else None,
                'days_of_data': (all_snapshots[-1].date - first_snapshot.date).days if len(all_snapshots) > 1 else 0
            }
            
            if not first_snapshot:
                results['issues'].append(" witty-raven has NO portfolio snapshots at all!")
            elif len(all_snapshots) < 2:
                results['issues'].append(f" witty-raven only has {len(all_snapshots)} snapshot(s) - need at least 2 for charts")
        
        return jsonify({
            'success': len(results['issues']) == 0,
            'results': results,
            'summary': {
                'users_with_today_snapshot': sum(1 for v in results['portfolio_snapshots'].values() if v['exists']),
                'total_users': len(users),
                'sp500_data_exists': results['sp500_data']['today']['exists'],
                'total_issues': len(results['issues'])
            }
        })
    
    except Exception as e:
        logger.error(f"Snapshot data flow verification error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-intraday-retention')
@login_required
def check_intraday_retention():
    """Admin endpoint to check intraday data retention and verify purging is working"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from datetime import timedelta, date
        from models import PortfolioSnapshotIntraday, User
        from sqlalchemy import func
        
        results = {
            'total_intraday_snapshots': 0,
            'snapshots_by_user': {},
            'date_range': {},
            'snapshots_by_day': {},
            'estimated_daily_rate': 0,
            'retention_analysis': {}
        }
        
        # Total count
        results['total_intraday_snapshots'] = PortfolioSnapshotIntraday.query.count()
        
        # Count by user
        users = User.query.all()
        for user in users:
            count = PortfolioSnapshotIntraday.query.filter_by(user_id=user.id).count()
            results['snapshots_by_user'][user.username] = count
        
        # Date range
        oldest = PortfolioSnapshotIntraday.query.order_by(PortfolioSnapshotIntraday.timestamp.asc()).first()
        newest = PortfolioSnapshotIntraday.query.order_by(PortfolioSnapshotIntraday.timestamp.desc()).first()
        
        if oldest and newest:
            results['date_range'] = {
                'oldest': oldest.timestamp.isoformat(),
                'newest': newest.timestamp.isoformat(),
                'days_span': (newest.timestamp.date() - oldest.timestamp.date()).days
            }
            
            # Snapshots by day (last 20 days)
            snapshots_by_day = {}
            today = get_market_date()  # FIX: Use ET
            for days_ago in range(20):
                check_date = today - timedelta(days=days_ago)
                count = PortfolioSnapshotIntraday.query.filter(
                    func.date(PortfolioSnapshotIntraday.timestamp) == check_date
                ).count()
                
                if count > 0:
                    snapshots_by_day[check_date.isoformat()] = count
            
            results['snapshots_by_day'] = dict(sorted(snapshots_by_day.items(), reverse=True))
            
            # Estimate daily collection rate
            if results['date_range']['days_span'] > 0:
                results['estimated_daily_rate'] = round(results['total_intraday_snapshots'] / results['date_range']['days_span'], 1)
        
        # Retention analysis
        cutoff_14_days = get_market_date() - timedelta(days=14)  # FIX: Use ET
        old_snapshots = PortfolioSnapshotIntraday.query.filter(
            PortfolioSnapshotIntraday.timestamp < datetime.combine(cutoff_14_days, datetime.min.time())
        ).count()
        
        results['retention_analysis'] = {
            'cutoff_date_14_days': cutoff_14_days.isoformat(),
            'snapshots_older_than_14_days': old_snapshots,
            'snapshots_within_14_days': results['total_intraday_snapshots'] - old_snapshots,
            'purging_working': old_snapshots == 0,
            'recommendation': 'Run /admin/cleanup-intraday-data to purge old data' if old_snapshots > 0 else ' Purging is working correctly - no old snapshots found'
        }
        
        return jsonify({
            'success': True,
            'results': results,
            'summary': {
                'total_snapshots': results['total_intraday_snapshots'],
                'date_range_days': results['date_range'].get('days_span', 0),
                'daily_rate': results['estimated_daily_rate'],
                'purging_status': ' Working' if results['retention_analysis']['purging_working'] else ' Needs attention',
                'old_snapshots_count': results['retention_analysis']['snapshots_older_than_14_days']
            }
        })
    
    except Exception as e:
        logger.error(f"Intraday retention check error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/intraday-diagnostics')
@login_required
def admin_intraday_diagnostics():
    """Comprehensive intraday data collection diagnostics"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from datetime import datetime, date, timedelta
        from models import db, User, PortfolioSnapshotIntraday, PortfolioSnapshot, MarketData
        from sqlalchemy import func, text
        import json
        
        today = get_market_date()  # FIX: Use ET for market data queries
        yesterday = today - timedelta(days=1)
        
        diagnostics = {
            'timestamp': datetime.now().isoformat(),
            'database_status': {},
            'user_analysis': {},
            'snapshot_analysis': {},
            'cron_schedule': {},
            'market_data': {},
            'recommendations': []
        }
        
        # 1. Database connectivity
        try:
            user_count = User.query.count()
            diagnostics['database_status'] = {
                'connected': True,
                'total_users': user_count
            }
        except Exception as e:
            diagnostics['database_status'] = {
                'connected': False,
                'error': str(e)
            }
            
        # 2. User analysis
        try:
            users = User.query.all()
            user_details = []
            
            for user in users:
                user_info = {
                    'id': user.id,
                    'username': user.username,
                    'has_transactions': False,
                    'portfolio_value': 0,
                    'intraday_snapshots_today': 0,
                    'intraday_snapshots_yesterday': 0,
                    'eod_snapshot_today': False
                }
                
                # Check transactions
                from models import Transaction
                transaction_count = Transaction.query.filter_by(user_id=user.id).count()
                user_info['has_transactions'] = transaction_count > 0
                
                # Calculate portfolio value
                try:
                    from portfolio_performance import PortfolioPerformanceCalculator
                    calculator = PortfolioPerformanceCalculator()
                    user_info['portfolio_value'] = calculator.calculate_portfolio_value(user.id)
                except Exception as e:
                    user_info['portfolio_value_error'] = str(e)
                
                # Count intraday snapshots
                user_info['intraday_snapshots_today'] = PortfolioSnapshotIntraday.query.filter(
                    PortfolioSnapshotIntraday.user_id == user.id,
                    func.date(PortfolioSnapshotIntraday.timestamp) == today
                ).count()
                
                user_info['intraday_snapshots_yesterday'] = PortfolioSnapshotIntraday.query.filter(
                    PortfolioSnapshotIntraday.user_id == user.id,
                    func.date(PortfolioSnapshotIntraday.timestamp) == yesterday
                ).count()
                
                # Check EOD snapshot
                eod_snapshot = PortfolioSnapshot.query.filter_by(
                    user_id=user.id, date=today
                ).first()
                user_info['eod_snapshot_today'] = eod_snapshot is not None
                
                user_details.append(user_info)
            
            diagnostics['user_analysis'] = {
                'total_users': len(user_details),
                'users_with_transactions': sum(1 for u in user_details if u['has_transactions']),
                'users_with_portfolio_value': sum(1 for u in user_details if u['portfolio_value'] > 0),
                'total_intraday_today': sum(u['intraday_snapshots_today'] for u in user_details),
                'total_intraday_yesterday': sum(u['intraday_snapshots_yesterday'] for u in user_details),
                'user_details': user_details
            }
            
        except Exception as e:
            diagnostics['user_analysis']['error'] = str(e)
        
        # 3. Snapshot analysis
        try:
            # Total snapshots by type
            total_intraday = PortfolioSnapshotIntraday.query.count()
            total_eod = PortfolioSnapshot.query.count()
            
            # Recent intraday snapshots
            recent_intraday = PortfolioSnapshotIntraday.query.filter(
                PortfolioSnapshotIntraday.timestamp >= datetime.combine(yesterday, datetime.min.time())
            ).order_by(PortfolioSnapshotIntraday.timestamp.desc()).limit(10).all()
            
            diagnostics['snapshot_analysis'] = {
                'total_intraday_all_time': total_intraday,
                'total_eod_all_time': total_eod,
                'recent_intraday_snapshots': [
                    {
                        'user_id': s.user_id,
                        'timestamp': s.timestamp.isoformat(),
                        'value': s.total_value
                    } for s in recent_intraday
                ]
            }
            
        except Exception as e:
            diagnostics['snapshot_analysis']['error'] = str(e)
        
        # 4. Market data analysis
        try:
            spy_data_today = MarketData.query.filter(
                MarketData.ticker == 'SPY_INTRADAY',
                MarketData.date == today
            ).count()
            
            spy_data_yesterday = MarketData.query.filter(
                MarketData.ticker == 'SPY_INTRADAY', 
                MarketData.date == yesterday
            ).count()
            
            # Check Alpha Vantage API call logs
            from models import AlphaVantageAPILog
            api_calls_today = AlphaVantageAPILog.query.filter(
                func.date(AlphaVantageAPILog.timestamp) == today
            ).count()
            
            successful_api_calls = AlphaVantageAPILog.query.filter(
                func.date(AlphaVantageAPILog.timestamp) == today,
                AlphaVantageAPILog.response_status == 'success'
            ).count()
            
            failed_api_calls = AlphaVantageAPILog.query.filter(
                func.date(AlphaVantageAPILog.timestamp) == today,
                AlphaVantageAPILog.response_status == 'error'
            ).count()
            
            # Get recent API calls for analysis
            recent_api_calls = AlphaVantageAPILog.query.filter(
                func.date(AlphaVantageAPILog.timestamp) == today
            ).order_by(AlphaVantageAPILog.timestamp.desc()).limit(10).all()
            
            diagnostics['market_data'] = {
                'spy_intraday_today': spy_data_today,
                'spy_intraday_yesterday': spy_data_yesterday,
                'api_calls_today': api_calls_today,
                'successful_api_calls': successful_api_calls,
                'failed_api_calls': failed_api_calls,
                'recent_api_calls': [
                    {
                        'symbol': call.symbol,
                        'endpoint': call.endpoint,
                        'status': call.response_status,
                        'timestamp': call.timestamp.strftime('%H:%M:%S'),
                        'response_size': len(call.response_data) if call.response_data else 0
                    } for call in recent_api_calls
                ]
            }
            
        except Exception as e:
            diagnostics['market_data'] = {'error': str(e)}
        
        # 5. Generate recommendations
        if diagnostics['user_analysis'].get('total_intraday_today', 0) == 0:
            diagnostics['recommendations'].append(" NO INTRADAY SNAPSHOTS TODAY - Check cron job execution")
        
        if diagnostics['user_analysis'].get('total_intraday_today', 0) < 13 * diagnostics['user_analysis'].get('total_users', 0):
            diagnostics['recommendations'].append(" INSUFFICIENT INTRADAY SNAPSHOTS - Expected 13 per user per day")
        
        if diagnostics['market_data'].get('spy_intraday_today', 0) == 0:
            diagnostics['recommendations'].append(" NO SPY DATA TODAY - Market data collection failing")
        
        # 6. Cron schedule info
        diagnostics['cron_schedule'] = {
            'vercel_schedule': "0,30 13-20 * * 1-5 (UTC)",
            'expected_et_times': ["9:30 AM", "10:00 AM", "10:30 AM", "11:00 AM", "11:30 AM", "12:00 PM", "12:30 PM", "1:00 PM", "1:30 PM", "2:00 PM", "2:30 PM", "3:00 PM", "3:30 PM", "4:00 PM"],
            'expected_collections_per_day': 14,
            'current_et_time': datetime.now().strftime('%I:%M %p ET')
        }
        
        # 7. Analyze snapshot value uniqueness and fluctuation
        try:
            snapshot_analysis = {}
            for user_detail in diagnostics['user_analysis']['user_details']:
                user_id = user_detail['id']
                username = user_detail['username']
                
                # Get all today's snapshots for this user
                user_snapshots = PortfolioSnapshotIntraday.query.filter(
                    PortfolioSnapshotIntraday.user_id == user_id,
                    func.date(PortfolioSnapshotIntraday.timestamp) == today
                ).order_by(PortfolioSnapshotIntraday.timestamp.asc()).all()
                
                if user_snapshots:
                    values = [s.total_value for s in user_snapshots]
                    unique_values = list(set(values))
                    
                    snapshot_analysis[username] = {
                        'total_snapshots': len(user_snapshots),
                        'unique_values_count': len(unique_values),
                        'value_range': {
                            'min': min(values),
                            'max': max(values),
                            'difference': max(values) - min(values),
                            'percentage_change': round(((max(values) - min(values)) / min(values)) * 100, 2) if min(values) > 0 else 0
                        },
                        'first_value': values[0],
                        'last_value': values[-1],
                        'all_same_value': len(unique_values) == 1,
                        'sample_values': values[:5] + (['...'] if len(values) > 5 else []) + values[-3:] if len(values) > 8 else values,
                        'timestamps': [s.timestamp.strftime('%H:%M:%S') for s in user_snapshots[:3]] + (['...'] if len(user_snapshots) > 6 else []) + [s.timestamp.strftime('%H:%M:%S') for s in user_snapshots[-3:]] if len(user_snapshots) > 6 else [s.timestamp.strftime('%H:%M:%S') for s in user_snapshots]
                    }
            
            diagnostics['snapshot_value_analysis'] = snapshot_analysis
            
            # Generate recommendations based on value analysis
            stale_users = [username for username, data in snapshot_analysis.items() if data['all_same_value']]
            if stale_users:
                diagnostics['recommendations'].append(f" STALE DATA DETECTED - Users with identical values all day: {', '.join(stale_users)}")
            
            low_fluctuation_users = [username for username, data in snapshot_analysis.items() if data['value_range']['percentage_change'] < 0.1 and not data['all_same_value']]
            if low_fluctuation_users:
                diagnostics['recommendations'].append(f" LOW FLUCTUATION - Users with <0.1% daily change: {', '.join(low_fluctuation_users)}")
            
            # Check for mostly duplicate values (indicating stale price data)
            mostly_duplicate_users = [username for username, data in snapshot_analysis.items() if data['unique_values_count'] <= 3 and data['total_snapshots'] > 10]
            if mostly_duplicate_users:
                diagnostics['recommendations'].append(f" MOSTLY DUPLICATE VALUES - Users with 3 unique values despite many snapshots: {', '.join(mostly_duplicate_users)} - Check Alpha Vantage API calls")
            
            # Check if API calls are insufficient
            if diagnostics['market_data'].get('api_calls_today', 0) < 20:
                diagnostics['recommendations'].append(f" INSUFFICIENT API CALLS - Only {diagnostics['market_data'].get('api_calls_today', 0)} API calls today - Cron jobs may not be running automatically")
            
            if diagnostics['market_data'].get('failed_api_calls', 0) > diagnostics['market_data'].get('successful_api_calls', 0):
                diagnostics['recommendations'].append(f" API FAILURES EXCEED SUCCESSES - {diagnostics['market_data'].get('failed_api_calls', 0)} failed vs {diagnostics['market_data'].get('successful_api_calls', 0)} successful - Check Alpha Vantage API key/limits")
                
        except Exception as e:
            diagnostics['snapshot_value_analysis'] = {'error': str(e)}

        return f"""
        <h1>Intraday Data Collection Diagnostics</h1>
        <pre>{json.dumps(diagnostics, indent=2)}</pre>
        <p><a href="/admin">Back to Admin</a></p>
        """
        
    except Exception as e:
        return f"""
        <h1>Diagnostics Error</h1>
        <p><strong>Error:</strong> {str(e)}</p>
        <p><a href="/admin">Back to Admin</a></p>
        """

@app.route('/admin/snapshot-uniqueness-report')
@login_required
def admin_snapshot_uniqueness_report():
    """Detailed report on snapshot uniqueness and stock diversity"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from datetime import datetime, date, timedelta
        from models import db, User, PortfolioSnapshotIntraday, Stock, Transaction
        from sqlalchemy import func, distinct
        import json
        
        today = get_market_date()  # FIX: Use ET for market data queries
        yesterday = today - timedelta(days=1)
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'date_analyzed': today.isoformat(),
            'unique_stocks_analysis': {},
            'snapshot_uniqueness': {},
            'api_efficiency_analysis': {},
            'recommendations': []
        }
        
        # 1. Analyze unique stocks across all users
        try:
            # Get all unique stock tickers from user holdings
            unique_stocks = db.session.query(distinct(Stock.ticker)).all()
            unique_tickers = [stock[0] for stock in unique_stocks]
            
            # Get stock holdings per user
            users = User.query.all()
            user_stocks = {}
            total_holdings = 0
            
            for user in users:
                user_holdings = Stock.query.filter_by(user_id=user.id).all()
                user_stocks[user.username] = [stock.ticker for stock in user_holdings]
                total_holdings += len(user_holdings)
            
            report['unique_stocks_analysis'] = {
                'total_unique_tickers': len(unique_tickers),
                'unique_tickers_list': sorted(unique_tickers),
                'total_stock_holdings_all_users': total_holdings,
                'user_stock_breakdown': user_stocks,
                'expected_api_calls_per_collection': len(unique_tickers)
            }
            
        except Exception as e:
            report['unique_stocks_analysis'] = {'error': str(e)}
        
        # 2. Analyze snapshot uniqueness for today
        try:
            snapshot_analysis = {}
            
            for user in users:
                # Get all today's snapshots
                user_snapshots = PortfolioSnapshotIntraday.query.filter(
                    PortfolioSnapshotIntraday.user_id == user.id,
                    func.date(PortfolioSnapshotIntraday.timestamp) == today
                ).order_by(PortfolioSnapshotIntraday.timestamp.asc()).all()
                
                if user_snapshots:
                    values = [float(s.total_value) for s in user_snapshots]
                    unique_values = list(set(values))
                    
                    # Calculate time gaps between snapshots
                    time_gaps = []
                    for i in range(1, len(user_snapshots)):
                        gap = (user_snapshots[i].timestamp - user_snapshots[i-1].timestamp).total_seconds() / 60
                        time_gaps.append(round(gap, 1))
                    
                    snapshot_analysis[user.username] = {
                        'total_snapshots': len(user_snapshots),
                        'unique_values': len(unique_values),
                        'uniqueness_percentage': round((len(unique_values) / len(user_snapshots)) * 100, 1),
                        'value_range': {
                            'min': min(values),
                            'max': max(values),
                            'spread_dollars': round(max(values) - min(values), 2),
                            'spread_percentage': round(((max(values) - min(values)) / min(values)) * 100, 2) if min(values) > 0 else 0
                        },
                        'time_gaps_minutes': time_gaps[:10],  # First 10 gaps
                        'average_gap_minutes': round(sum(time_gaps) / len(time_gaps), 1) if time_gaps else 0,
                        'first_snapshot': user_snapshots[0].timestamp.strftime('%H:%M:%S'),
                        'last_snapshot': user_snapshots[-1].timestamp.strftime('%H:%M:%S')
                    }
            
            report['snapshot_uniqueness'] = snapshot_analysis
            
        except Exception as e:
            report['snapshot_uniqueness'] = {'error': str(e)}
        
        # 3. API efficiency analysis
        try:
            from models import AlphaVantageAPILog
            
            # API calls today
            api_calls_today = AlphaVantageAPILog.query.filter(
                func.date(AlphaVantageAPILog.timestamp) == today
            ).count()
            
            successful_calls = AlphaVantageAPILog.query.filter(
                func.date(AlphaVantageAPILog.timestamp) == today,
                AlphaVantageAPILog.response_status == 'success'
            ).count()
            
            # Get unique symbols called today
            unique_symbols_called = db.session.query(
                distinct(AlphaVantageAPILog.symbol)
            ).filter(
                func.date(AlphaVantageAPILog.timestamp) == today
            ).count()
            
            # Calculate expected vs actual calls
            expected_collections = 16  # Vercel cron schedule
            expected_api_calls = len(unique_tickers) * expected_collections
            
            report['api_efficiency_analysis'] = {
                'total_api_calls_today': api_calls_today,
                'successful_api_calls': successful_calls,
                'unique_symbols_called': unique_symbols_called,
                'expected_collections_per_day': expected_collections,
                'expected_api_calls_per_day': expected_api_calls,
                'actual_vs_expected_ratio': round(api_calls_today / expected_api_calls, 2) if expected_api_calls > 0 else 0,
                'api_efficiency_percentage': round((successful_calls / api_calls_today) * 100, 1) if api_calls_today > 0 else 0
            }
            
        except Exception as e:
            report['api_efficiency_analysis'] = {'error': str(e)}
        
        # 4. Generate recommendations
        if report['unique_stocks_analysis'].get('total_unique_tickers', 0) != 22:
            report['recommendations'].append(f" STOCK COUNT MISMATCH - Expected 22 unique stocks, found {report['unique_stocks_analysis'].get('total_unique_tickers', 0)}")
        
        # Check snapshot uniqueness
        low_uniqueness_users = []
        for username, data in report['snapshot_uniqueness'].items():
            if data.get('uniqueness_percentage', 0) < 50:
                low_uniqueness_users.append(f"{username} ({data.get('uniqueness_percentage', 0)}%)")
        
        if low_uniqueness_users:
            report['recommendations'].append(f" LOW SNAPSHOT UNIQUENESS - Users with <50% unique values: {', '.join(low_uniqueness_users)}")
        
        # Check API efficiency
        if report['api_efficiency_analysis'].get('actual_vs_expected_ratio', 0) > 1.5:
            report['recommendations'].append(f" EXCESSIVE API CALLS - Making {report['api_efficiency_analysis'].get('actual_vs_expected_ratio', 0)}x expected calls")
        
        return f"""
        <h1>Snapshot Uniqueness & Stock Diversity Report</h1>
        <pre>{json.dumps(report, indent=2)}</pre>
        <p><a href="/admin">Back to Admin</a></p>
        """
        
    except Exception as e:
        return f"""
        <h1>Report Generation Error</h1>
        <p><strong>Error:</strong> {str(e)}</p>
        <p><a href="/admin">Back to Admin</a></p>
        """

@app.route('/admin/detailed-intraday-debug')
@login_required
def admin_detailed_intraday_debug():
    """Comprehensive step-by-step debugging of intraday collection process"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from datetime import datetime
        from models import db, User, PortfolioSnapshotIntraday, Stock, MarketData, AlphaVantageAPILog, UserPortfolioChartCache
        from portfolio_performance import PortfolioPerformanceCalculator, stock_price_cache
        import json
        
        debug_report = {
            'timestamp': datetime.now().isoformat(),
            'current_time_et': datetime.now().strftime('%Y-%m-%d %H:%M:%S ET'),
            'market_status': 'CLOSED' if datetime.now().hour >= 16 else 'OPEN',
            'test_type': 'comprehensive_intraday_debug',
            'steps': {
                'step_1_unique_stocks': {},
                'step_2_cache_analysis': {},
                'step_3_api_calls': {},
                'step_4_price_caching': {},
                'step_5_portfolio_calculations': {},
                'step_6_chart_prerendering': {}
            },
            'summary': {},
            'errors': []
        }
        
        calculator = PortfolioPerformanceCalculator()
        current_time = datetime.now()
        
        # STEP 1: Analyze unique stocks across all users
        debug_report['steps']['step_1_unique_stocks']['description'] = "Identifying all unique stock tickers"
        try:
            from sqlalchemy import distinct
            unique_stocks = db.session.query(distinct(Stock.ticker)).all()
            unique_tickers = sorted([stock[0] for stock in unique_stocks])
            
            # Get stock holdings per user
            users = User.query.all()
            user_stock_breakdown = {}
            
            for user in users:
                user_holdings = Stock.query.filter_by(user_id=user.id).all()
                user_stock_breakdown[user.username] = {
                    'tickers': [stock.ticker for stock in user_holdings],
                    'quantities': {stock.ticker: stock.quantity for stock in user_holdings}
                }
            
            debug_report['steps']['step_1_unique_stocks']['results'] = {
                'unique_tickers': unique_tickers,
                'total_unique_count': len(unique_tickers),
                'user_stock_breakdown': user_stock_breakdown
            }
            
        except Exception as e:
            debug_report['steps']['step_1_unique_stocks']['error'] = str(e)
            debug_report['errors'].append(f"Step 1 error: {e}")
        
        # STEP 2: Cache analysis - which stocks have cached values
        debug_report['steps']['step_2_cache_analysis']['description'] = "Checking which stocks have cached prices within 90 seconds"
        try:
            cached_stocks = []
            non_cached_stocks = []
            cache_details = {}
            
            for ticker in unique_tickers:
                ticker_upper = ticker.upper()
                if ticker_upper in stock_price_cache:
                    cached_data = stock_price_cache[ticker_upper]
                    cache_time = cached_data.get('timestamp')
                    if cache_time and (current_time - cache_time).total_seconds() < 90:
                        cached_stocks.append(ticker)
                        cache_details[ticker] = {
                            'cached': True,
                            'price': cached_data['price'],
                            'cache_age_seconds': round((current_time - cache_time).total_seconds(), 1),
                            'cache_timestamp': cache_time.isoformat()
                        }
                    else:
                        non_cached_stocks.append(ticker)
                        cache_details[ticker] = {
                            'cached': False,
                            'reason': 'expired_cache' if cache_time else 'no_cache',
                            'cache_age_seconds': round((current_time - cache_time).total_seconds(), 1) if cache_time else None
                        }
                else:
                    non_cached_stocks.append(ticker)
                    cache_details[ticker] = {
                        'cached': False,
                        'reason': 'no_cache_entry'
                    }
            
            debug_report['steps']['step_2_cache_analysis']['results'] = {
                'cached_stocks': cached_stocks,
                'non_cached_stocks': non_cached_stocks,
                'cache_details': cache_details,
                'cached_count': len(cached_stocks),
                'non_cached_count': len(non_cached_stocks)
            }
            
        except Exception as e:
            debug_report['steps']['step_2_cache_analysis']['error'] = str(e)
            debug_report['errors'].append(f"Step 2 error: {e}")
        
        # STEP 3: Make API calls for non-cached stocks
        debug_report['steps']['step_3_api_calls']['description'] = "Making Alpha Vantage API calls for non-cached stocks"
        try:
            api_call_results = {}
            
            for ticker in non_cached_stocks:
                api_start_time = datetime.now()
                try:
                    stock_data = calculator.get_stock_data(ticker)
                    api_end_time = datetime.now()
                    
                    api_call_results[ticker] = {
                        'api_call_made': True,
                        'api_start_time': api_start_time.isoformat(),
                        'api_end_time': api_end_time.isoformat(),
                        'api_duration_ms': round((api_end_time - api_start_time).total_seconds() * 1000, 1),
                        'success': stock_data is not None,
                        'price_fetched': stock_data.get('price') if stock_data else None,
                        'error': None
                    }
                    
                except Exception as e:
                    api_call_results[ticker] = {
                        'api_call_made': True,
                        'success': False,
                        'error': str(e),
                        'price_fetched': None
                    }
            
            debug_report['steps']['step_3_api_calls']['results'] = {
                'api_call_results': api_call_results,
                'successful_calls': sum(1 for r in api_call_results.values() if r['success']),
                'failed_calls': sum(1 for r in api_call_results.values() if not r['success'])
            }
            
        except Exception as e:
            debug_report['steps']['step_3_api_calls']['error'] = str(e)
            debug_report['errors'].append(f"Step 3 error: {e}")
        
        # STEP 4: Analyze final price caching state
        debug_report['steps']['step_4_price_caching']['description'] = "Analyzing final cached price state after API calls"
        try:
            final_cached_prices = {}
            successfully_cached = []
            cache_failures = []
            
            for ticker in unique_tickers:
                ticker_upper = ticker.upper()
                if ticker_upper in stock_price_cache:
                    cached_data = stock_price_cache[ticker_upper]
                    cache_time = cached_data.get('timestamp')
                    if cache_time and (current_time - cache_time).total_seconds() < 90:
                        successfully_cached.append(ticker)
                        final_cached_prices[ticker] = {
                            'price': cached_data['price'],
                            'cache_timestamp': cache_time.isoformat(),
                            'source': 'api_call' if ticker in non_cached_stocks else 'existing_cache'
                        }
                    else:
                        cache_failures.append(ticker)
                else:
                    cache_failures.append(ticker)
            
            debug_report['steps']['step_4_price_caching']['results'] = {
                'final_cached_prices': final_cached_prices,
                'successfully_cached': successfully_cached,
                'cache_failures': cache_failures,
                'cache_success_rate': f"{len(successfully_cached)}/{len(unique_tickers)}"
            }
            
        except Exception as e:
            debug_report['steps']['step_4_price_caching']['error'] = str(e)
            debug_report['errors'].append(f"Step 4 error: {e}")
        
        # STEP 5: Calculate portfolio values for each user
        debug_report['steps']['step_5_portfolio_calculations']['description'] = "Calculating portfolio values using cached prices"
        try:
            user_portfolio_calculations = {}
            intraday_snapshots_created = []
            
            for user in users:
                try:
                    user_stocks = Stock.query.filter_by(user_id=user.id).all()
                    portfolio_breakdown = {}
                    total_value = 0
                    
                    for stock in user_stocks:
                        ticker = stock.ticker
                        quantity = stock.quantity
                        
                        if ticker in final_cached_prices:
                            price = final_cached_prices[ticker]['price']
                            stock_value = price * quantity
                            total_value += stock_value
                            
                            portfolio_breakdown[ticker] = {
                                'quantity': quantity,
                                'price_used': price,
                                'stock_value': round(stock_value, 2),
                                'price_source': final_cached_prices[ticker]['source']
                            }
                        else:
                            portfolio_breakdown[ticker] = {
                                'quantity': quantity,
                                'price_used': None,
                                'stock_value': 0,
                                'error': 'no_cached_price'
                            }
                    
                    # Create intraday snapshot
                    if total_value > 0:
                        intraday_snapshot = PortfolioSnapshotIntraday(
                            user_id=user.id,
                            timestamp=current_time,
                            total_value=total_value
                        )
                        db.session.add(intraday_snapshot)
                        intraday_snapshots_created.append({
                            'user_id': user.id,
                            'username': user.username,
                            'snapshot_created': True,
                            'total_value': round(total_value, 2)
                        })
                    
                    user_portfolio_calculations[user.username] = {
                        'total_portfolio_value': round(total_value, 2),
                        'portfolio_breakdown': portfolio_breakdown,
                        'calculation_successful': total_value > 0
                    }
                    
                except Exception as e:
                    user_portfolio_calculations[user.username] = {
                        'error': str(e),
                        'calculation_successful': False
                    }
            
            debug_report['steps']['step_5_portfolio_calculations']['results'] = {
                'user_portfolio_calculations': user_portfolio_calculations,
                'intraday_snapshots_created': intraday_snapshots_created,
                'successful_calculations': sum(1 for calc in user_portfolio_calculations.values() if calc.get('calculation_successful', False))
            }
            
        except Exception as e:
            debug_report['steps']['step_5_portfolio_calculations']['error'] = str(e)
            debug_report['errors'].append(f"Step 5 error: {e}")
        
        # STEP 6: Chart pre-rendering analysis
        debug_report['steps']['step_6_chart_prerendering']['description'] = "Analyzing 1D and 5D chart pre-rendering"
        try:
            chart_prerendering_results = {}
            
            for user in users:
                try:
                    # Check existing chart cache
                    existing_1d_cache = UserPortfolioChartCache.query.filter_by(
                        user_id=user.id, period='1D'
                    ).first()
                    
                    existing_5d_cache = UserPortfolioChartCache.query.filter_by(
                        user_id=user.id, period='5D'
                    ).first()
                    
                    chart_prerendering_results[user.username] = {
                        '1D_chart': {
                            'existing_cache': existing_1d_cache is not None,
                            'cache_timestamp': existing_1d_cache.generated_at.isoformat() if existing_1d_cache else None,
                            'would_regenerate': True  # Since we added new data
                        },
                        '5D_chart': {
                            'existing_cache': existing_5d_cache is not None,
                            'cache_timestamp': existing_5d_cache.generated_at.isoformat() if existing_5d_cache else None,
                            'would_regenerate': True  # Since we added new data
                        }
                    }
                    
                except Exception as e:
                    chart_prerendering_results[user.username] = {
                        'error': str(e)
                    }
            
            debug_report['steps']['step_6_chart_prerendering']['results'] = {
                'chart_prerendering_results': chart_prerendering_results,
                'note': 'Charts would be regenerated on next access due to new intraday data'
            }
            
        except Exception as e:
            debug_report['steps']['step_6_chart_prerendering']['error'] = str(e)
            debug_report['errors'].append(f"Step 6 error: {e}")
        
        # Commit the test snapshots
        try:
            db.session.commit()
            debug_report['database_commit'] = 'successful'
        except Exception as e:
            db.session.rollback()
            debug_report['database_commit'] = f'failed: {str(e)}'
        
        # INVESTIGATE: What triggered the recent API calls at 5:33 PM?
        debug_report['api_call_investigation'] = {
            'description': 'Investigating what triggered API calls after market close'
        }
        try:
            from datetime import timedelta
            
            # Look for API calls in the last 10 minutes
            recent_cutoff = current_time - timedelta(minutes=10)
            recent_api_calls = AlphaVantageAPILog.query.filter(
                AlphaVantageAPILog.timestamp >= recent_cutoff
            ).order_by(AlphaVantageAPILog.timestamp.desc()).all()
            
            # Group by minute to see patterns
            api_calls_by_minute = {}
            for call in recent_api_calls:
                minute_key = call.timestamp.strftime('%H:%M')
                if minute_key not in api_calls_by_minute:
                    api_calls_by_minute[minute_key] = []
                api_calls_by_minute[minute_key].append({
                    'symbol': call.symbol,
                    'status': call.response_status,
                    'timestamp': call.timestamp.strftime('%H:%M:%S'),
                    'endpoint': call.endpoint
                })
            
            # Check if there are any recent intraday snapshots that might indicate what triggered this
            recent_snapshots = PortfolioSnapshotIntraday.query.filter(
                PortfolioSnapshotIntraday.timestamp >= recent_cutoff
            ).order_by(PortfolioSnapshotIntraday.timestamp.desc()).all()
            
            # Check for any recent chart cache updates
            recent_chart_cache = UserPortfolioChartCache.query.filter(
                UserPortfolioChartCache.generated_at >= recent_cutoff
            ).order_by(UserPortfolioChartCache.generated_at.desc()).all()
            
            debug_report['api_call_investigation']['results'] = {
                'recent_api_calls_count': len(recent_api_calls),
                'api_calls_by_minute': api_calls_by_minute,
                'recent_snapshots_created': [
                    {
                        'user_id': snap.user_id,
                        'timestamp': snap.timestamp.strftime('%H:%M:%S'),
                        'total_value': snap.total_value
                    } for snap in recent_snapshots
                ],
                'recent_chart_cache_updates': [
                    {
                        'user_id': cache.user_id,
                        'period': cache.period,
                        'generated_at': cache.generated_at.strftime('%H:%M:%S')
                    } for cache in recent_chart_cache
                ],
                'possible_triggers': []
            }
            
            # Analyze possible triggers
            if len(recent_api_calls) == 22:  # All stocks fetched
                debug_report['api_call_investigation']['results']['possible_triggers'].append(
                    " FULL STOCK REFRESH - All 22 stocks fetched simultaneously (likely admin dashboard or user portfolio view)"
                )
            
            if recent_snapshots:
                debug_report['api_call_investigation']['results']['possible_triggers'].append(
                    f" INTRADAY SNAPSHOTS - {len(recent_snapshots)} snapshots created (suggests cron job or manual collection)"
                )
            
            if recent_chart_cache:
                debug_report['api_call_investigation']['results']['possible_triggers'].append(
                    f" CHART CACHE UPDATES - {len(recent_chart_cache)} charts regenerated"
                )
            
            # Check if this matches admin dashboard access pattern
            if len(recent_api_calls) == 22 and not recent_snapshots:
                debug_report['api_call_investigation']['results']['possible_triggers'].append(
                    " LIKELY CAUSE: Admin dashboard access - fetched all stock prices for display but didn't create snapshots"
                )
            
        except Exception as e:
            debug_report['api_call_investigation']['error'] = str(e)
        
        # Generate summary
        debug_report['summary'] = {
            'total_unique_stocks': len(unique_tickers) if 'unique_tickers' in locals() else 0,
            'stocks_cached_initially': len(cached_stocks) if 'cached_stocks' in locals() else 0,
            'api_calls_needed': len(non_cached_stocks) if 'non_cached_stocks' in locals() else 0,
            'api_calls_successful': debug_report['steps']['step_3_api_calls']['results'].get('successful_calls', 0) if 'results' in debug_report['steps']['step_3_api_calls'] else 0,
            'final_cached_stocks': len(successfully_cached) if 'successfully_cached' in locals() else 0,
            'portfolio_calculations_successful': debug_report['steps']['step_5_portfolio_calculations']['results'].get('successful_calculations', 0) if 'results' in debug_report['steps']['step_5_portfolio_calculations'] else 0,
            'intraday_snapshots_created': len(intraday_snapshots_created) if 'intraday_snapshots_created' in locals() else 0,
            'total_errors': len(debug_report['errors'])
        }
        
        return f"""
        <h1>Detailed Intraday Collection Debug Report</h1>
        <pre>{json.dumps(debug_report, indent=2)}</pre>
        <p><a href="/admin">Back to Admin</a></p>
        <p><a href="/admin/intraday-diagnostics">View Standard Diagnostics</a></p>
        """
        
    except Exception as e:
        return f"""
        <h1>Debug Report Generation Error</h1>
        <p><strong>Error:</strong> {str(e)}</p>
        <p><a href="/admin">Back to Admin</a></p>
        """

@app.route('/admin/intraday-collection-logs')
@login_required
def admin_intraday_collection_logs():
    """View recent intraday collection logs with detailed analysis"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from datetime import datetime, date, timedelta
        from models import db, AlphaVantageAPILog, PortfolioSnapshotIntraday
        from sqlalchemy import func, distinct
        import json
        try:
            import pytz
            # Set up timezone conversion
            utc = pytz.UTC
            eastern = pytz.timezone('US/Eastern')
        except ImportError:
            logger.warning("pytz module not available - using UTC fallback")
            from datetime import timezone, timedelta
            utc = timezone.utc
            eastern = timezone(timedelta(hours=-4))  # EDT fallback
        
        today = get_market_date()  # FIX: Use ET for intraday queries
        yesterday = today - timedelta(days=1)
        
        # Get today's intraday snapshots grouped by timestamp (collection runs)
        intraday_collections = db.session.query(
            PortfolioSnapshotIntraday.timestamp,
            func.count(PortfolioSnapshotIntraday.id).label('snapshot_count'),
            func.avg(PortfolioSnapshotIntraday.total_value).label('avg_portfolio_value')
        ).filter(
            func.date(PortfolioSnapshotIntraday.timestamp) == today
        ).group_by(
            PortfolioSnapshotIntraday.timestamp
        ).order_by(
            PortfolioSnapshotIntraday.timestamp.desc()
        ).limit(20).all()
        
        collection_logs = []
        
        for collection in intraday_collections:
            collection_time = collection.timestamp
            
            # Get API calls within 2 minutes of this collection
            api_window_start = collection_time - timedelta(minutes=2)
            api_window_end = collection_time + timedelta(minutes=2)
            
            api_calls = AlphaVantageAPILog.query.filter(
                AlphaVantageAPILog.timestamp >= api_window_start,
                AlphaVantageAPILog.timestamp <= api_window_end
            ).order_by(AlphaVantageAPILog.timestamp.asc()).all()
            
            # Get unique symbols called
            unique_symbols = list(set([call.symbol for call in api_calls]))
            successful_calls = [call for call in api_calls if call.response_status == 'success']
            failed_calls = [call for call in api_calls if call.response_status == 'error']
            
            # Get all snapshots for this collection time
            snapshots = PortfolioSnapshotIntraday.query.filter(
                PortfolioSnapshotIntraday.timestamp == collection_time
            ).all()
            
            # Convert UTC to Eastern Time
            collection_time_et = utc.localize(collection_time).astimezone(eastern)
            
            collection_logs.append({
                'collection_time': collection_time_et.strftime('%H:%M:%S'),
                'collection_time_full': collection_time_et.isoformat(),
                'snapshots_created': len(snapshots),
                'api_calls_total': len(api_calls),
                'api_calls_successful': len(successful_calls),
                'api_calls_failed': len(failed_calls),
                'unique_stocks_called': len(unique_symbols),
                'stocks_called': sorted(unique_symbols),
                'avg_portfolio_value': round(collection.avg_portfolio_value, 2) if collection.avg_portfolio_value else 0,
                'api_call_details': [
                    {
                        'symbol': call.symbol,
                        'status': call.response_status,
                        'timestamp': utc.localize(call.timestamp).astimezone(eastern).strftime('%H:%M:%S.%f')[:-3],
                        'endpoint': call.endpoint
                    } for call in api_calls
                ],
                'portfolio_snapshots': [
                    {
                        'user_id': snap.user_id,
                        'total_value': round(snap.total_value, 2)
                    } for snap in snapshots
                ]
            })
        
        # Summary statistics
        total_collections_today = len(collection_logs)
        total_api_calls_today = AlphaVantageAPILog.query.filter(
            func.date(AlphaVantageAPILog.timestamp) == today
        ).count()
        
        successful_api_calls_today = AlphaVantageAPILog.query.filter(
            func.date(AlphaVantageAPILog.timestamp) == today,
            AlphaVantageAPILog.response_status == 'success'
        ).count()
        
        # Calculate expected collections based on day of week
        # Saturday = 5, Sunday = 6 (no market)
        if today.weekday() >= 5:  # Weekend
            expected_collections = 0
        else:  # Weekday
            expected_collections = 14  # Vercel cron schedule (9:30 AM - 4:00 PM ET)
        
        summary = {
            'date': today.isoformat(),
            'total_collections': total_collections_today,
            'total_api_calls': total_api_calls_today,
            'successful_api_calls': successful_api_calls_today,
            'api_success_rate': round((successful_api_calls_today / total_api_calls_today) * 100, 1) if total_api_calls_today > 0 else 0,
            'expected_collections': expected_collections,
            'collection_efficiency': round((total_collections_today / expected_collections) * 100, 1) if expected_collections > 0 else 0
        }
        
        report = {
            'summary': summary,
            'collection_logs': collection_logs,
            'generated_at': datetime.now().isoformat()
        }
        
        # Generate simplified text format for easy copying
        simplified_text = f"""INTRADAY COLLECTION LOGS - {today}

SUMMARY:
- Collections: {summary['total_collections']}/{summary['expected_collections']} ({summary['collection_efficiency']}% efficiency)
- API Calls: {summary['total_api_calls']} ({summary['successful_api_calls']} successful, {summary['api_success_rate']}% success rate)

RECENT COLLECTIONS:"""

        for log in collection_logs[:10]:  # Show top 10 for readability
            simplified_text += f"""
{log['collection_time']} - {log['snapshots_created']} snapshots, {log['api_calls_total']} API calls
   Stocks called: {', '.join(log['stocks_called']) if log['stocks_called'] else 'None (all cached)'}
   Success rate: {log['api_calls_successful']}/{log['api_calls_total']} calls successful"""

        return f"""
        <h1>Intraday Collection Logs - {today} (Eastern Time)</h1>
        
        <h2>Summary</h2>
        <ul>
            <li><strong>Collections Today:</strong> {summary['total_collections']} / {summary['expected_collections']} expected ({summary['collection_efficiency']}% efficiency)</li>
            <li><strong>API Calls Today:</strong> {summary['total_api_calls']} ({summary['successful_api_calls']} successful, {summary['api_success_rate']}% success rate)</li>
        </ul>
        
        <h2> Copy-Paste Format for Chat Analysis</h2>
        <div style="background: #f5f5f5; padding: 15px; border: 1px solid #ddd; margin: 10px 0;">
            <button onclick="copyToClipboard()" style="background: #007bff; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; margin-bottom: 10px;"> Copy to Clipboard</button>
            <pre id="simplified-logs" style="font-family: monospace; font-size: 12px; white-space: pre-wrap; margin: 0;">{simplified_text}</pre>
        </div>
        
        <h2> Full JSON Data (for detailed analysis)</h2>
        <details>
            <summary>Click to expand full data</summary>
            <div style="font-family: monospace; font-size: 11px; max-height: 400px; overflow-y: auto;">
                <pre>{json.dumps(report, indent=2)}</pre>
            </div>
        </details>
        
        <script>
        function copyToClipboard() {{
            const text = document.getElementById('simplified-logs').textContent;
            navigator.clipboard.writeText(text).then(function() {{
                const button = document.querySelector('button');
                const originalText = button.textContent;
                button.textContent = ' Copied!';
                button.style.background = '#28a745';
                setTimeout(() => {{
                    button.textContent = originalText;
                    button.style.background = '#007bff';
                }}, 2000);
            }}).catch(function(err) {{
                alert('Failed to copy: ' + err);
            }});
        }}
        </script>
        
        <p><a href="/admin">Back to Admin</a></p>
        <p><a href="/admin/detailed-intraday-debug">Run Live Debug</a></p>
        <p><a href="/admin/intraday-collection-logs"> Refresh Logs</a></p>
        """
        
    except Exception as e:
        return f"""
        <h1>Log Viewer Error</h1>
        <p><strong>Error:</strong> {str(e)}</p>
        <p><a href="/admin">Back to Admin</a></p>
        """

@app.route('/admin/test-intraday-collection')
@login_required
def admin_test_intraday_collection():
    """Manually test intraday collection with detailed logging"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from datetime import datetime
        from models import db, User, PortfolioSnapshotIntraday, MarketData
        from portfolio_performance import PortfolioPerformanceCalculator
        import json
        
        calculator = PortfolioPerformanceCalculator()
        current_time = datetime.now()
        
        test_results = {
            'timestamp': current_time.isoformat(),
            'test_type': 'manual_admin_trigger',
            'steps': [],
            'users_processed': [],
            'errors': [],
            'summary': {}
        }
        
        # Step 1: Test SPY data collection
        test_results['steps'].append("1. Testing SPY data collection...")
        try:
            spy_data = calculator.get_stock_data('SPY')
            if spy_data and spy_data.get('price'):
                spy_price = spy_data['price']
                sp500_value = spy_price * 10
                
                # Store test SPY data
                market_data = MarketData(
                    ticker='SPY_INTRADAY_TEST',
                    date=current_time.date(),
                    timestamp=current_time,
                    close_price=sp500_value
                )
                db.session.add(market_data)
                
                test_results['steps'].append(f" SPY data collected: ${spy_price} (S&P 500: ${sp500_value})")
                test_results['summary']['spy_collection'] = 'success'
            else:
                test_results['steps'].append(" Failed to fetch SPY data")
                test_results['summary']['spy_collection'] = 'failed'
                test_results['errors'].append("SPY data fetch failed")
        except Exception as e:
            error_msg = f" SPY collection error: {str(e)}"
            test_results['steps'].append(error_msg)
            test_results['errors'].append(error_msg)
            test_results['summary']['spy_collection'] = 'error'
        
        # Step 2: Process each user
        test_results['steps'].append("2. Processing users...")
        users = User.query.all()
        snapshots_created = 0
        
        for user in users:
            user_result = {
                'user_id': user.id,
                'username': user.username,
                'portfolio_value': 0,
                'snapshot_created': False,
                'error': None
            }
            
            try:
                # Calculate portfolio value
                portfolio_value = calculator.calculate_portfolio_value(user.id)
                user_result['portfolio_value'] = portfolio_value
                
                if portfolio_value > 0:
                    # Create test intraday snapshot
                    test_snapshot = PortfolioSnapshotIntraday(
                        user_id=user.id,
                        timestamp=current_time,
                        total_value=portfolio_value
                    )
                    db.session.add(test_snapshot)
                    user_result['snapshot_created'] = True
                    snapshots_created += 1
                    
                    test_results['steps'].append(f" User {user.username}: ${portfolio_value:.2f} snapshot created")
                else:
                    test_results['steps'].append(f" User {user.username}: $0 portfolio value - no snapshot created")
                    
            except Exception as e:
                error_msg = f" User {user.username} error: {str(e)}"
                user_result['error'] = str(e)
                test_results['steps'].append(error_msg)
                test_results['errors'].append(error_msg)
            
            test_results['users_processed'].append(user_result)
        
        # Step 3: Commit to database
        test_results['steps'].append("3. Committing to database...")
        try:
            db.session.commit()
            test_results['steps'].append(f" Database commit successful - {snapshots_created} snapshots created")
            test_results['summary']['database_commit'] = 'success'
        except Exception as e:
            db.session.rollback()
            error_msg = f" Database commit failed: {str(e)}"
            test_results['steps'].append(error_msg)
            test_results['errors'].append(error_msg)
            test_results['summary']['database_commit'] = 'failed'
        
        # Summary
        test_results['summary'].update({
            'total_users': len(users),
            'snapshots_created': snapshots_created,
            'users_with_portfolios': sum(1 for u in test_results['users_processed'] if u['portfolio_value'] > 0),
            'success_rate': f"{snapshots_created}/{len(users)}" if users else "0/0"
        })
        
        return f"""
        <h1>Intraday Collection Test Results</h1>
        <pre>{json.dumps(test_results, indent=2)}</pre>
        <p><a href="/admin">Back to Admin</a></p>
        <p><a href="/admin/intraday-diagnostics">View Full Diagnostics</a></p>
        """
        
    except Exception as e:
        return f"""
        <h1>Test Collection Error</h1>
        <p><strong>Error:</strong> {str(e)}</p>
        <p><a href="/admin">Back to Admin</a></p>
        """

@app.route('/admin/clear-chart-cache')
@login_required
def admin_clear_chart_cache():
    """Clear chart cache to force regeneration with weekend filter"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from models import db, UserPortfolioChartCache
        
        # Clear chart cache (this will force regeneration with weekend filter)
        chart_count = UserPortfolioChartCache.query.count()
        UserPortfolioChartCache.query.delete()
        
        db.session.commit()
        
        return f"""
        <h1>Chart Cache Cleared</h1>
        <p><strong>Chart cache:</strong> {chart_count} entries cleared</p>
        <p><strong>Result:</strong> Charts will regenerate with weekend filter applied</p>
        <p><strong>Note:</strong> Next chart load will be slower as data regenerates</p>
        <p><a href="/admin">Back to Admin</a></p>
        """
        
    except Exception as e:
        return f"""
        <h1>Cache Clear Error</h1>
        <p><strong>Error:</strong> {str(e)}</p>
        <p><a href="/admin">Back to Admin</a></p>
        """

@app.route('/admin')
@login_required
def admin_dashboard():
    """Enhanced admin dashboard with platform metrics and API status"""
    # Verify user is authenticated
    if not current_user.is_authenticated:
        flash('You must be logged in to access the admin dashboard.', 'warning')
    # ... (rest of the code remains the same)
        return redirect(url_for('login'))
    
    # Check if user is admin using the secure is_admin property
    if not current_user.is_admin:
        flash('You do not have permission to access the admin dashboard.', 'danger')
        return redirect(url_for('dashboard'))
    
    try:
        # Get basic counts
        user_count = User.query.count()
        stock_count = Stock.query.count()
        transaction_count = Transaction.query.count()
        subscription_count = Subscription.query.count()
        
        # Get recent users (limit 10 for table display)
        recent_users = User.query.order_by(User.created_at.desc()).limit(10).all()
        
        # Check for problematic users (missing subscription data)
        problematic_users = User.query.filter(
            (User.subscription_price == None) | (User.stripe_price_id == None)
        ).limit(10).all()
        
    except Exception as e:
        app.logger.error(f"Database error in admin dashboard: {str(e)}")
        # Fallback to basic counts
        user_count = User.query.count() if User.query else 0
        stock_count = Stock.query.count() if Stock.query else 0
        transaction_count = Transaction.query.count() if Transaction.query else 0
        subscription_count = Subscription.query.count() if Subscription.query else 0
        recent_users = []
        problematic_users = []
    
    return render_template_with_defaults('admin/dashboard.html',
        user_count=user_count,
        stock_count=stock_count,
        transaction_count=transaction_count,
        subscription_count=subscription_count,
        recent_users=recent_users,
        problematic_users=problematic_users,
        now=datetime.now()
    )

@app.route('/admin/db-debug')
@login_required
def debug_database():
    """Temporary diagnostic endpoint to check database connectivity and schema"""
    if not current_user.is_admin:
        return "Access denied", 403
    
    results = []
    
    # Test database connection
    try:
        db.session.execute(text("SELECT 1"))
        results.append(" Database connection: OK")
    except Exception as e:
        results.append(f" Database connection failed: {str(e)}")
    
    # Check if tables exist
    tables_to_check = ['user', 'stock', 'stock_transaction', 'subscription']
    for table in tables_to_check:
        try:
            result = db.session.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
            results.append(f" Table '{table}': {result} rows")
        except Exception as e:
            results.append(f" Table '{table}': {str(e)}")
    
    # Test model queries
    try:
        user_count = User.query.count()
        results.append(f" User.query.count(): {user_count}")
    except Exception as e:
        results.append(f" User.query.count(): {str(e)}")
    
    try:
        stock_count = Stock.query.count()
        results.append(f" Stock.query.count(): {stock_count}")
    except Exception as e:
        results.append(f" Stock.query.count(): {str(e)}")
    
    try:
        transaction_count = Transaction.query.count()
        results.append(f" Transaction.query.count(): {transaction_count}")
    except Exception as e:
        results.append(f" Transaction.query.count(): {str(e)}")
    
    try:
        subscription_count = Subscription.query.count()
        results.append(f" Subscription.query.count(): {subscription_count}")
    except Exception as e:
        results.append(f" Subscription.query.count(): {str(e)}")
    
    # Test recent users query
    try:
        recent_users = User.query.order_by(User.created_at.desc()).limit(5).all()
        results.append(f" Recent users query: {len(recent_users)} users found")
        for user in recent_users:
            results.append(f"   - {user.username} ({user.email})")
    except Exception as e:
        results.append(f" Recent users query: {str(e)}")
    
    html_results = "<br>".join(results)
    return f"""
    <h1>Debug Error</h1>
    <pre>{html_results}</pre>
    <br><a href='/admin'>Back to Admin</a>
    """


@app.route('/admin/test-cron-execution')
@login_required
def test_cron_execution():
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from datetime import datetime, timedelta, time
        from sqlalchemy import distinct
        from models import PortfolioSnapshotIntraday, PortfolioSnapshot, LeaderboardEntry, User, Transaction
        
        today = get_market_date()  # FIX: Use ET for snapshot queries
        yesterday = today - timedelta(days=1)
        
        # Get all users with portfolios
        users_with_portfolios = db.session.query(User).join(Transaction).distinct().all()
        
        results = {
            'test_date': today.isoformat(),
            'users_tested': len(users_with_portfolios),
            'intraday_snapshots_today': 0,
            'eod_snapshots_today': 0,
            'users_with_intraday_today': 0,
            'users_with_eod_today': 0,
            'portfolio_values_varying': 0,
            'leaderboard_updated': False,
            'chart_data_available': 0,
            'issues': [],
            'user_details': []
        }
        
        # Check snapshots for each user
        for user in users_with_portfolios:
            user_data = {
                'username': user.username,
                'user_id': user.id,
                'intraday_today': 0,
                'eod_today': 0,
                'varying_values': False,
                'total_snapshots': 0
            }
            
            # Check intraday snapshots for today
            intraday_today = PortfolioSnapshotIntraday.query.filter(
                PortfolioSnapshotIntraday.user_id == user.id,
                PortfolioSnapshotIntraday.timestamp >= datetime.combine(today, datetime.min.time()),
                PortfolioSnapshotIntraday.timestamp < datetime.combine(today + timedelta(days=1), datetime.min.time())
            ).all()
            
            if intraday_today:
                results['users_with_intraday_today'] += 1
                results['intraday_snapshots_today'] += len(intraday_today)
                user_data['intraday_today'] = len(intraday_today)
                
                # Check for varying values
                values = [snap.total_value for snap in intraday_today]
                if len(set(values)) > 1:
                    results['portfolio_values_varying'] += 1
                    user_data['varying_values'] = True
                else:
                    results['issues'].append(f"User {user.username} has identical intraday values")
            else:
                results['issues'].append(f"User {user.username} missing intraday snapshots for today")
            
            # Check end-of-day snapshots for today
            eod_today = PortfolioSnapshot.query.filter(
                PortfolioSnapshot.user_id == user.id,
                PortfolioSnapshot.date == today
            ).all()
            
            if eod_today:
                results['users_with_eod_today'] += 1
                results['eod_snapshots_today'] += len(eod_today)
                user_data['eod_today'] = len(eod_today)
            else:
                results['issues'].append(f"User {user.username} missing end-of-day snapshot for today")
            
            # Check total snapshots for chart data
            total_snapshots = PortfolioSnapshot.query.filter(
                PortfolioSnapshot.user_id == user.id
            ).count()
            user_data['total_snapshots'] = total_snapshots
            
            if total_snapshots >= 2:
                results['chart_data_available'] += 1
            
            results['user_details'].append(user_data)
        
        # Check leaderboard updates using actual production schema
        try:
            # Query using raw SQL since the model doesn't match production schema
            leaderboard_query = db.session.execute(text("""
                SELECT COUNT(*) as count FROM leaderboard_entry 
                WHERE date >= :yesterday_date
            """), {'yesterday_date': yesterday})
            leaderboard_count = leaderboard_query.fetchone()[0]
            
            if leaderboard_count > 0:
                results['leaderboard_updated'] = True
                results['leaderboard_entries'] = leaderboard_count
                results['top_performers'] = []  # Skip detailed analysis due to schema mismatch
            else:
                results['leaderboard_updated'] = False
                results['leaderboard_entries'] = 0
                results['issues'].append("No recent leaderboard entries found")
                
        except Exception as leaderboard_error:
            logger.warning(f"Leaderboard query failed: {leaderboard_error}")
            results['leaderboard_updated'] = False
            results['leaderboard_entries'] = 0
            results['issues'].append(f"Leaderboard query error: {str(leaderboard_error)}")
        
        # Test portfolio calculation
        if users_with_portfolios:
            try:
                from portfolio_performance import PortfolioPerformanceCalculator
                calculator = PortfolioPerformanceCalculator()
                test_user = users_with_portfolios[0]
                current_value = calculator.calculate_portfolio_value(test_user.id)
                results['portfolio_calculation_test'] = {
                    'success': True,
                    'test_user': test_user.username,
                    'current_value': current_value
                }
            except Exception as e:
                results['portfolio_calculation_test'] = {
                    'success': False,
                    'error': str(e)
                }
                results['issues'].append(f"Portfolio calculation error: {str(e)}")
        
        # Summary
        results['summary'] = {
            'all_tests_passed': len(results['issues']) == 0,
            'total_issues': len(results['issues'])
        }
        
        return f"""
        <h1>Cron Job Execution Test Results</h1>
        <h2>Summary</h2>
        <p><strong>Test Date:</strong> {results['test_date']}</p>
        <p><strong>Users Tested:</strong> {results['users_tested']}</p>
        <p><strong>All Tests Passed:</strong> {' Yes' if results['summary']['all_tests_passed'] else ' No'}</p>
        <p><strong>Total Issues:</strong> {results['summary']['total_issues']}</p>
        
        <h2>Snapshot Results</h2>
        <p><strong>Intraday Snapshots Today:</strong> {results['intraday_snapshots_today']}</p>
        <p><strong>Users with Intraday Today:</strong> {results['users_with_intraday_today']}/{results['users_tested']}</p>
        <p><strong>End-of-Day Snapshots Today:</strong> {results['eod_snapshots_today']}</p>
        <p><strong>Users with EOD Today:</strong> {results['users_with_eod_today']}/{results['users_tested']}</p>
        <p><strong>Users with Varying Values:</strong> {results['portfolio_values_varying']}</p>
        <p><strong>Users with Chart Data:</strong> {results['chart_data_available']}</p>
        
        <h2>Leaderboard</h2>
        <p><strong>Updated:</strong> {' Yes' if results['leaderboard_updated'] else ' No'}</p>
        {f"<p><strong>Recent Entries:</strong> {results.get('leaderboard_entries', 0)}</p>" if results['leaderboard_updated'] else ""}
        
        <h2>Full Results</h2>
        <pre>{results}</pre>
        
        <br><a href='/admin'>Back to Admin</a>
        """
        
    except Exception as e:
        import traceback
        error_details = {
            'error': str(e),
            'traceback': traceback.format_exc()
        }
        return f"<h1>Cron Test Error</h1><pre>{error_details}</pre><br><a href='/admin'>Back to Admin</a>"

@app.route('/admin/transactions')
@admin_required
def admin_transactions():
    """Admin route to view transactions"""
    try:
        # Get real transaction data from database
        transactions_query = Transaction.query.all()
        
        # Format transaction data
        transactions = []
        for tx in transactions_query:
            # Get username for the transaction
            user = User.query.get(tx.user_id)
            username = user.username if user else 'Unknown'
            
            transactions.append({
                'id': tx.id,
                'user_id': tx.user_id,
                'username': username,
                'symbol': tx.ticker,
                'shares': tx.quantity,
                'price': tx.price,
                'transaction_type': tx.transaction_type,
                'date': tx.timestamp.strftime('%Y-%m-%d'),
                'notes': tx.notes or ''
            })
    except Exception as e:
        app.logger.error(f"Database error in admin_transactions: {str(e)}")
        # Fallback to mock data if database fails
        transactions = [
            {'id': 1, 'user_id': 1, 'username': 'witty-raven', 'symbol': 'AAPL', 'shares': 10, 'price': 150.0, 'transaction_type': 'buy', 'date': '2023-01-15', 'notes': 'Initial purchase'},
            {'id': 2, 'user_id': 1, 'username': 'witty-raven', 'symbol': 'MSFT', 'shares': 5, 'price': 250.0, 'transaction_type': 'buy', 'date': '2023-02-20', 'notes': 'Portfolio diversification'},
            {'id': 3, 'user_id': 2, 'username': 'user1', 'symbol': 'GOOGL', 'shares': 2, 'price': 2800.0, 'transaction_type': 'buy', 'date': '2023-03-10', 'notes': ''},
            {'id': 4, 'user_id': 3, 'username': 'user2', 'symbol': 'AMZN', 'shares': 1, 'price': 3200.0, 'transaction_type': 'buy', 'date': '2023-04-05', 'notes': ''},
            {'id': 5, 'user_id': 1, 'username': 'witty-raven', 'symbol': 'AAPL', 'shares': 5, 'price': 170.0, 'transaction_type': 'sell', 'date': '2023-05-15', 'notes': 'Profit taking'},
        ]
    
    # Get filter parameters
    user_filter = request.args.get('user', '')
    symbol_filter = request.args.get('symbol', '')
    type_filter = request.args.get('type', '')
    
    # Apply filters if provided
    filtered_transactions = transactions
    if user_filter:
        filtered_transactions = [t for t in filtered_transactions if str(t['user_id']) == user_filter]
    if symbol_filter:
        filtered_transactions = [t for t in filtered_transactions if t['ticker'].lower() == symbol_filter.lower()]
    if type_filter:
        filtered_transactions = [t for t in filtered_transactions if t['transaction_type'].lower() == type_filter.lower()]
    
    # Get unique users and symbols for filters
    unique_users = list(set([(t['user_id'], t['username']) for t in transactions]))
    unique_symbols = list(set([t['ticker'] for t in transactions]))
    
    return render_template_string("""
<!DOCTYPE html>
<html>
<head>
    <title>Admin - Transactions</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 1000px; margin: 0 auto; }
        .button { 
            display: inline-block; 
            background: #4CAF50; 
            color: white; 
            padding: 10px 20px; 
            text-decoration: none; 
            border-radius: 5px; 
            margin-top: 20px;
            margin-right: 10px;
        }
        .button-secondary { background: #2196F3; }
        .button-warning { background: #FF9800; }
        .button-danger { background: #F44336; }
        .button-small { padding: 5px 10px; margin-top: 0; }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            padding: 8px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #f2f2f2;
        }
        tr:hover {background-color: #f5f5f5;}
        .nav { 
            background: #333; 
            padding: 10px; 
            margin-bottom: 20px; 
            border-radius: 5px; 
        }
        .nav a { 
            color: white; 
            text-decoration: none; 
            margin-right: 15px; 
            padding: 5px 10px; 
        }
        .nav a:hover { 
            background: #555; 
            border-radius: 3px; 
        }
        .filters {
            margin: 20px 0;
            padding: 15px;
            background: #f9f9f9;
            border-radius: 5px;
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
        }
        .filters select, .filters button {
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .filters button {
            background: #4CAF50;
            color: white;
            border: none;
            cursor: pointer;
        }
        .buy { color: green; }
        .sell { color: red; }
        .summary {
            margin-top: 20px;
            padding: 15px;
            background: #e9f7ef;
            border-radius: 5px;
        }
        .summary h3 {
            margin-top: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <a href="/admin">Dashboard</a>
            <a href="/admin/users">Users</a>
            <a href="/admin/transactions">Transactions</a>
            <a href="/">Main Site</a>
        </div>
        
        <h1>Transaction Management</h1>
        
        <div class="filters">
            <form method="get" action="/admin/transactions">
                <select name="user">
                    <option value="">All Users</option>
                    {% for user_id, username in unique_users %}
                    <option value="{{ user_id }}" {% if user_filter == user_id|string %}selected{% endif %}>{{ username }}</option>
                    {% endfor %}
                </select>
                
                <select name="symbol">
                    <option value="">All Tickers</option>
                    {% for symbol in unique_symbols %}
                    <option value="{{ symbol }}" {% if symbol_filter == symbol %}selected{% endif %}>{{ symbol }}</option>
                    {% endfor %}
                </select>
                
                <select name="type">
                    <option value="">All Types</option>
                    <option value="buy" {% if type_filter == 'buy' %}selected{% endif %}>Buy</option>
                    <option value="sell" {% if type_filter == 'sell' %}selected{% endif %}>Sell</option>
                </select>
                
                <button type="submit">Filter</button>
                <a href="/admin/transactions" style="padding: 8px; text-decoration: none;">Clear Filters</a>
            </form>
        </div>
        
        <div class="summary">
            <h3>Transaction Summary</h3>
            <p>Total Transactions: {{ filtered_transactions|length }}</p>
            <p>Total Value: ${{ '%0.2f'|format(filtered_transactions|sum(attribute='price')|float) }}</p>
        </div>
        
        <table>
            <tr>
                <th>ID</th>
                <th>User</th>
                <th>Ticker</th>
                <th>Quantity</th>
                <th>Price</th>
                <th>Total</th>
                <th>Type</th>
                <th>Timestamp</th>
                <th>Notes</th>
                <th>Actions</th>
            </tr>
            {% for tx in filtered_transactions %}
            <tr>
                <td>{{ tx.id }}</td>
                <td>{{ tx.username }}</td>
                <td>{{ tx.ticker }}</td>
                <td>{{ tx.quantity }}</td>
                <td>${{ '%0.2f'|format(tx.price) }}</td>
                <td>${{ '%0.2f'|format(tx.quantity * tx.price) }}</td>
                <td class="{{ tx.transaction_type }}">{{ tx.transaction_type|upper }}</td>
                <td>{{ tx.timestamp }}</td>
                <td>{{ tx.notes }}</td>
                <td>
                    <a href="/admin/transactions/{{ tx.id }}/edit" class="button button-warning button-small">Edit</a>
                    <a href="/admin/transactions/{{ tx.id }}/delete" class="button button-danger button-small">Delete</a>
                </td>
            </tr>
            {% endfor %}
        </table>
        
        <a href="/admin" class="button">Back to Dashboard</a>
        <a href="/admin/transactions/add" class="button button-secondary">Add Transaction</a>
    </div>
</body>
</html>
    """, transactions=transactions, filtered_transactions=filtered_transactions, unique_users=unique_users, unique_symbols=unique_symbols, user_filter=user_filter, symbol_filter=symbol_filter, type_filter=type_filter)

@app.route('/admin/stocks')
@admin_required
def admin_stocks():
    """Admin route to view stocks"""
    try:
        # Get real stock data from database
        stocks_query = Stock.query.all()
        
        # Format stock data
        stocks = []
        for stock in stocks_query:
            # Get username for the stock
            user = User.query.get(stock.user_id)
            username = user.username if user else 'Unknown'
            
            stocks.append({
                'id': stock.id,
                'user_id': stock.user_id,
                'username': username,
                'ticker': stock.ticker,
                'quantity': stock.quantity,
                'purchase_price': stock.purchase_price,
                'current_price': 0.0,  # Stock model doesn't have current_price field
                'purchase_date': stock.purchase_date.strftime('%Y-%m-%d')
            })
    except Exception as e:
        app.logger.error(f"Database error in admin_stocks: {str(e)}")
        # Fallback to mock data if database fails
        stocks = [
            {'id': 1, 'user_id': 1, 'username': 'witty-raven', 'ticker': 'AAPL', 'quantity': 5, 'purchase_price': 150.0, 'current_price': 180.0, 'purchase_date': '2023-01-15'},
            {'id': 2, 'user_id': 1, 'username': 'witty-raven', 'ticker': 'MSFT', 'quantity': 5, 'purchase_price': 250.0, 'current_price': 280.0, 'purchase_date': '2023-02-20'},
            {'id': 3, 'user_id': 2, 'username': 'user1', 'ticker': 'GOOGL', 'quantity': 2, 'purchase_price': 2800.0, 'current_price': 2900.0, 'purchase_date': '2023-03-10'},
            {'id': 4, 'user_id': 3, 'username': 'user2', 'ticker': 'AMZN', 'quantity': 1, 'purchase_price': 3200.0, 'current_price': 3400.0, 'purchase_date': '2023-04-05'},
        ]
    
    # Get filter parameters
    user_filter = request.args.get('user', '')
    ticker_filter = request.args.get('ticker', '')
    
    # Apply filters if provided
    filtered_stocks = stocks
    if user_filter:
        filtered_stocks = [s for s in filtered_stocks if str(s['user_id']) == user_filter]
    if ticker_filter:
        filtered_stocks = [s for s in filtered_stocks if s['ticker'].lower() == ticker_filter.lower()]
    
    # Get unique users and tickers for filters
    unique_users = list(set([(s['user_id'], s['username']) for s in stocks]))
    unique_tickers = list(set([s['ticker'] for s in stocks]))
    
    return render_template_string("""
<!DOCTYPE html>
<html>
<head>
    <title>Admin - Stocks</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 1000px; margin: 0 auto; }
        .button { 
            display: inline-block; 
            background: #4CAF50; 
            color: white; 
            padding: 10px 20px; 
            text-decoration: none; 
            border-radius: 5px; 
            margin-top: 20px;
            margin-right: 10px;
        }
        .button-secondary { background: #2196F3; }
        .button-warning { background: #FF9800; }
        .button-danger { background: #F44336; }
        .button-small { padding: 5px 10px; margin-top: 0; }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            padding: 8px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #f2f2f2;
        }
        tr:hover {background-color: #f5f5f5;}
        .nav { 
            background: #333; 
            padding: 10px; 
            margin-bottom: 20px; 
            border-radius: 5px; 
        }
        .nav a { 
            color: white; 
            text-decoration: none; 
            margin-right: 15px; 
            padding: 5px 10px; 
        }
        .nav a:hover { 
            background: #555; 
            border-radius: 3px; 
        }
        .filters {
            margin: 20px 0;
            padding: 15px;
            background: #f9f9f9;
            border-radius: 5px;
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
        }
        .filters select, .filters button {
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .filters button {
            background: #4CAF50;
            color: white;
            border: none;
            cursor: pointer;
        }
        .profit { color: green; }
        .loss { color: red; }
        .summary {
            margin-top: 20px;
            padding: 15px;
            background: #e9f7ef;
            border-radius: 5px;
        }
        .summary h3 {
            margin-top: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <a href="/admin">Dashboard</a>
            <a href="/admin/users">Users</a>
            <a href="/admin/transactions">Transactions</a>
            <a href="/admin/stocks">Stocks</a>
            <a href="/">Main Site</a>
        </div>
        
        <h1>Stock Management</h1>
        
        <div class="filters">
            <form method="get" action="/admin/stocks">
                <select name="user">
                    <option value="">All Users</option>
                    {% for user_id, username in unique_users %}
                    <option value="{{ user_id }}" {% if user_filter == user_id|string %}selected{% endif %}>{{ username }}</option>
                    {% endfor %}
                </select>
                
                <select name="ticker">
                    <option value="">All Tickers</option>
                    {% for ticker in unique_tickers %}
                    <option value="{{ ticker }}" {% if ticker_filter == ticker %}selected{% endif %}>{{ ticker }}</option>
                    {% endfor %}
                </select>
                
                <button type="submit">Filter</button>
                <a href="/admin/stocks" style="padding: 8px; text-decoration: none;">Clear Filters</a>
            </form>
        </div>
        
        <div class="summary">
            <h3>Stock Summary</h3>
            <p>Total Stocks: {{ filtered_stocks|length }}</p>
            <p>Total Value: ${{ '%0.2f'|format(filtered_stocks|sum(attribute='current_price')|float) }}</p>
        </div>
        
        <table>
            <tr>
                <th>ID</th>
                <th>User</th>
                <th>Ticker</th>
                <th>Quantity</th>
                <th>Purchase Price</th>
                <th>Current Price</th>
                <th>Total Value</th>
                <th>Profit/Loss</th>
                <th>Purchase Date</th>
                <th>Actions</th>
            </tr>
            {% for stock in filtered_stocks %}
            <tr>
                <td>{{ stock.id }}</td>
                <td>{{ stock.username }}</td>
                <td>{{ stock.ticker }}</td>
                <td>{{ stock.quantity }}</td>
                <td>${{ '%0.2f'|format(stock.purchase_price) }}</td>
                <td>${{ '%0.2f'|format(stock.current_price) }}</td>
                <td>${{ '%0.2f'|format(stock.quantity * stock.current_price) }}</td>
                {% set profit = (stock.current_price - stock.purchase_price) * stock.quantity %}
                <td class="{% if profit >= 0 %}profit{% else %}loss{% endif %}">
                    ${{ '%0.2f'|format(profit) }} ({{ '%0.1f'|format((stock.current_price - stock.purchase_price) / stock.purchase_price * 100) }}%)
                </td>
                <td>{{ stock.purchase_date }}</td>
                <td>
                    <a href="/admin/stocks/{{ stock.id }}/edit" class="button button-warning button-small">Edit</a>
                    <a href="/admin/stocks/{{ stock.id }}/delete" class="button button-danger button-small">Delete</a>
                </td>
            </tr>
            {% endfor %}
        </table>
        
        <a href="/admin" class="button">Back to Dashboard</a>
        <a href="/admin/stocks/add" class="button button-secondary">Add Stock</a>
    </div>
</body>
</html>
    """, stocks=stocks, filtered_stocks=filtered_stocks, unique_users=unique_users, unique_tickers=unique_tickers, user_filter=user_filter, ticker_filter=ticker_filter)

# Removed duplicate admin_user_detail route - using admin_interface.py blueprint instead

# Error handler
@app.errorhandler(500)
def internal_error(error):
    error_details = {
        'error': str(error),
        'traceback': traceback.format_exc(),
        'request_path': request.path,
        'request_method': request.method,
        'request_args': dict(request.args),
        'template_folder': app.template_folder,
        'static_folder': app.static_folder,
        'template_exists': os.path.exists(app.template_folder),
        'template_files': os.listdir(app.template_folder) if os.path.exists(app.template_folder) else [],
        'static_exists': os.path.exists(app.static_folder),
        'static_files': os.listdir(app.static_folder) if os.path.exists(app.static_folder) else []
    }
    logger.error(f"500 error: {str(error)}", extra=error_details)
    
    # Check if this is an API request
    if request.path.startswith('/api/'):
        return jsonify({
            'error': 'Internal Server Error',
            'status': 500,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'path': request.path,
            'environment': os.environ.get('VERCEL_ENV', 'development'),
            'request_id': os.environ.get('AWS_LAMBDA_REQUEST_ID', 'local')
        }), 500
    
    # Return a custom error page with details for HTML requests
    return render_template_string("""
<!DOCTYPE html>
<html>
<head>
    <title>Server Error</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 800px; margin: 0 auto; }
        .error { background: #f8d7da; padding: 15px; border-radius: 5px; }
        .details { margin-top: 20px; background: #f5f5f5; padding: 15px; border-radius: 5px; }
        pre { background: #eee; padding: 10px; overflow: auto; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Server Error</h1>
        
        <div class="error">
            <h2>500 - Internal Server Error</h2>
            <p>The server encountered an unexpected condition that prevented it from fulfilling the request.</p>
        </div>
        
        <div class="details">
            <h3>Error Details</h3>
            <pre>{{ error_details | tojson(indent=2) }}</pre>
        </div>
        
        <p><a href="/">Return to Home</a></p>
    </div>
</body>
</html>
    """, error_details=error_details), 500

@app.route('/admin/transactions/<int:transaction_id>/edit', methods=['GET', 'POST'])
@admin_required
def admin_edit_transaction(transaction_id):
    """Admin route to edit a transaction"""
    # Get transaction by ID from database
    transaction = Transaction.query.get(transaction_id)
    if not transaction:
        flash('Transaction not found', 'danger')
        return redirect(url_for('admin_transactions'))
    
    # Get the user associated with this transaction
    user = User.query.get(transaction.user_id)
    if not user:
        flash('User not found for this transaction', 'danger')
        return redirect(url_for('admin_transactions'))
    
    if request.method == 'POST':
        try:
            # Update transaction with form data
            transaction.ticker = request.form['ticker']
            transaction.quantity = float(request.form['quantity'])
            transaction.price = float(request.form['price'])
            transaction.transaction_type = request.form['transaction_type']
            
            # Parse transaction date if provided
            transaction_date = request.form.get('date')
            if transaction_date:
                transaction.timestamp = datetime.strptime(transaction_date, '%Y-%m-%d')
            
            # Update notes if provided
            if 'notes' in request.form:
                # Store notes as an attribute if the Transaction model supports it
                # This assumes there's a notes field in the Transaction model
                # If not, you might need to modify the model or skip this part
                transaction.notes = request.form['notes']
            
            # Save changes to database
            db.session.commit()
            
            flash('Transaction updated successfully!', 'success')
            return redirect(url_for('admin_transactions'))
        except Exception as e:
            db.session.rollback()
            logger.error(f"Error updating transaction: {str(e)}")
            flash(f'Error updating transaction: {str(e)}', 'danger')
    
    # Prepare transaction data for template
    transaction_data = {
        'id': transaction.id,
        'user_id': transaction.user_id,
        'username': user.username,
        'symbol': transaction.ticker,
        'shares': transaction.quantity,
        'price': transaction.price,
        'transaction_type': transaction.transaction_type,
        'date': transaction.timestamp.strftime('%Y-%m-%d') if transaction.timestamp else '',
        'notes': getattr(transaction, 'notes', '')
    }
    
    return render_template_string("""
<!DOCTYPE html>
<html>
<head>
    <title>Edit Transaction</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 800px; margin: 0 auto; }
        .button { 
            display: inline-block; 
            background: #4CAF50; 
            color: white; 
            padding: 10px 20px; 
            text-decoration: none; 
            border-radius: 5px; 
            margin-top: 20px;
            margin-right: 10px;
        }
        .button-secondary { background: #2196F3; }
        .button-warning { background: #FF9800; }
        .button-danger { background: #F44336; }
        .form-group {
            margin-bottom: 15px;
        }
        .form-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        .form-group input, .form-group select, .form-group textarea {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
            box-sizing: border-box;
        }
        .form-group textarea {
            height: 100px;
        }
        .nav { 
            background: #333; 
            padding: 10px; 
            margin-bottom: 20px; 
            border-radius: 5px; 
        }
        .nav a { 
            color: white; 
            text-decoration: none; 
            margin-right: 15px; 
            padding: 5px 10px; 
        }
        .nav a:hover { 
            background: #555; 
            border-radius: 3px; 
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <a href="/admin">Dashboard</a>
            <a href="/admin/users">Users</a>
            <a href="/admin/transactions">Transactions</a>
            <a href="/admin/stocks">Stocks</a>
            <a href="/">Main Site</a>
        </div>
        
        <h1>Edit Transaction</h1>
        
        <form method="post">
            <div class="form-group">
                <label for="user">User</label>
                <input type="text" id="user" name="user" value="{{ transaction_data.username }}" readonly>
            </div>
            
            <div class="form-group">
                <label for="ticker">Symbol</label>
                <input type="text" id="ticker" name="ticker" value="{{ transaction_data.ticker }}" required>
            </div>
            
            <div class="form-group">
                <label for="quantity">Shares</label>
                <input type="number" id="quantity" name="quantity" value="{{ transaction_data.quantity }}" step="0.01" required>
            </div>
            
            <div class="form-group">
                <label for="price">Price</label>
                <input type="number" id="price" name="price" value="{{ transaction_data.price }}" step="0.01" required>
            </div>
            
            <div class="form-group">
                <label for="transaction_type">Transaction Type</label>
                <select id="transaction_type" name="transaction_type" required>
                    <option value="buy" {% if transaction_data.transaction_type == 'buy' %}selected{% endif %}>Buy</option>
                    <option value="sell" {% if transaction_data.transaction_type == 'sell' %}selected{% endif %}>Sell</option>
                </select>
            </div>
            
            <div class="form-group">
                <label for="date">Date</label>
                <input type="date" id="date" name="date" value="{{ transaction_data.timestamp.strftime('%Y-%m-%d') }}" required>
            </div>
            
            <div class="form-group">
                <label for="notes">Notes</label>
                <textarea id="notes" name="notes">{{ transaction_data.notes }}</textarea>
            </div>
            
            <button type="submit" class="button button-warning">Update Transaction</button>
            <a href="/admin/transactions" class="button">Cancel</a>
        </form>
    </div>
</body>
</html>
    """, transaction=transaction)

@app.route('/admin/stocks/<int:stock_id>/edit', methods=['GET', 'POST'])
@admin_required
def admin_edit_stock(stock_id):
    """Admin route to edit a stock"""
    # Get stock by ID from database
    stock = Stock.query.get(stock_id)
    if not stock:
        flash('Stock not found', 'danger')
        return redirect(url_for('admin_stocks'))
    
    # Get the user associated with this stock
    user = User.query.get(stock.user_id)
    if not user:
        flash('User not found for this stock', 'danger')
        return redirect(url_for('admin_stocks'))
    
    if request.method == 'POST':
        try:
            # Update stock with form data
            stock.ticker = request.form['ticker']
            stock.quantity = float(request.form['quantity'])
            stock.purchase_price = float(request.form['purchase_price'])
            
            # Parse purchase date if provided
            purchase_date = request.form.get('purchase_date')
            if purchase_date:
                stock.purchase_date = datetime.strptime(purchase_date, '%Y-%m-%d')
            
            # Save changes to database
            db.session.commit()
            
            flash('Stock updated successfully!', 'success')
            return redirect(url_for('admin_stocks'))
        except Exception as e:
            db.session.rollback()
            logger.error(f"Error updating stock: {str(e)}")
            flash(f'Error updating stock: {str(e)}', 'danger')
    
    # Prepare stock data for template
    stock_data = {
        'id': stock.id,
        'user_id': stock.user_id,
        'username': user.username,
        'ticker': stock.ticker,
        'quantity': stock.quantity,
        'purchase_price': stock.purchase_price,
        'current_price': stock.current_value() / stock.quantity if stock.quantity > 0 else 0,
        'purchase_date': stock.purchase_date.strftime('%Y-%m-%d') if stock.purchase_date else ''
    }
    
    return render_template_string("""
<!DOCTYPE html>
<html>
<head>
    <title>Edit Stock</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 800px; margin: 0 auto; }
        .button { 
            display: inline-block; 
            background: #4CAF50; 
            color: white; 
            padding: 10px 20px; 
            text-decoration: none; 
            border-radius: 5px; 
            margin-top: 20px;
            margin-right: 10px;
        }
        .button-secondary { background: #2196F3; }
        .button-warning { background: #FF9800; }
        .button-danger { background: #F44336; }
        .form-group {
            margin-bottom: 15px;
        }
        .form-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        .form-group input, .form-group select {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
            box-sizing: border-box;
        }
        .nav { 
            background: #333; 
            padding: 10px; 
            margin-bottom: 20px; 
            border-radius: 5px; 
        }
        .nav a { 
            color: white; 
            text-decoration: none; 
            margin-right: 15px; 
            padding: 5px 10px; 
        }
        .nav a:hover { 
            background: #555; 
            border-radius: 3px; 
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <a href="/admin">Dashboard</a>
            <a href="/admin/users">Users</a>
            <a href="/admin/transactions">Transactions</a>
            <a href="/admin/stocks">Stocks</a>
            <a href="/">Main Site</a>
        </div>
        
        <h1>Edit Stock</h1>
        
        <form method="post">
            <div class="form-group">
                <label for="user">User</label>
                <input type="text" id="user" name="user" value="{{ stock.username }}" readonly>
            </div>
            
            <div class="form-group">
                <label for="ticker">Ticker</label>
                <input type="text" id="ticker" name="ticker" value="{{ stock.ticker }}" required>
            </div>
            
            <div class="form-group">
                <label for="quantity">Quantity</label>
                <input type="number" id="quantity" name="quantity" value="{{ stock.quantity }}" step="0.01" required>
            </div>
            
            <div class="form-group">
                <label for="purchase_price">Purchase Price</label>
                <input type="number" id="purchase_price" name="purchase_price" value="{{ stock.purchase_price }}" step="0.01" required>
            </div>
            
            <div class="form-group">
                <label for="current_price">Current Price</label>
                <input type="number" id="current_price" name="current_price" value="{{ stock.current_price }}" step="0.01" required>
            </div>
            
            <div class="form-group">
                <label for="purchase_date">Purchase Date</label>
                <input type="date" id="purchase_date" name="purchase_date" value="{{ stock.purchase_date }}" required>
            </div>
            
            <button type="submit" class="button button-warning">Update Stock</button>
            <a href="/admin/stocks" class="button">Cancel</a>
        </form>
    </div>
</body>
</html>
    """, stock=stock)

# Debug endpoint to check environment and configuration
@app.route('/debug')
def debug_info():
    try:
        # Check template and static directories
        template_files = []
        static_files = []
        try:
            if os.path.exists(app.template_folder):
                template_files = os.listdir(app.template_folder)
        except Exception as e:
            template_files = [f"Error listing templates: {str(e)}"]
            
        try:
            if os.path.exists(app.static_folder):
                static_files = os.listdir(app.static_folder)
        except Exception as e:
            static_files = [f"Error listing static files: {str(e)}"]
        
        # Collect environment information
        env_info = {
            'VERCEL_ENV': os.environ.get('VERCEL_ENV'),
            'DATABASE_URL_EXISTS': bool(os.environ.get('DATABASE_URL')),
            'POSTGRES_PRISMA_URL_EXISTS': bool(os.environ.get('POSTGRES_PRISMA_URL')),
            'SECRET_KEY_EXISTS': bool(os.environ.get('SECRET_KEY')),
            'FLASK_APP': os.environ.get('FLASK_APP'),
            'FLASK_ENV': os.environ.get('FLASK_ENV'),
            'TEMPLATE_FOLDER': app.template_folder,
            'STATIC_FOLDER': app.static_folder,
            'SQLALCHEMY_DATABASE_URI': '[REDACTED]',
            'WORKING_DIRECTORY': os.getcwd(),
            'DIRECTORY_CONTENTS': os.listdir('.'),
            'TEMPLATE_EXISTS': os.path.exists(app.template_folder),
            'STATIC_EXISTS': os.path.exists(app.static_folder),
            'TEMPLATE_FILES': template_files[:10],  # Limit to first 10 files
            'STATIC_FILES': static_files[:10],      # Limit to first 10 files
            'PYTHON_VERSION': sys.version,
            'APP_ROOT_PATH': os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            'ABSOLUTE_TEMPLATE_PATH': os.path.abspath(app.template_folder),
            'ABSOLUTE_STATIC_PATH': os.path.abspath(app.static_folder)
        }
        
        # Check if we can connect to the database
        db_status = 'Unknown'
        try:
            # Import text function at the top level to avoid import errors
            from sqlalchemy.sql import text
            db.session.execute(text('SELECT 1'))
            db_status = 'Connected'
        except Exception as e:
            db_status = f'Error: {str(e)}'
            
        env_info['DATABASE_STATUS'] = db_status
        
        return jsonify(env_info)
    except Exception as e:
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()})

@app.route('/admin/run-migration')
def run_migration():
    """One-time migration endpoint - remove after use"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Create the new tables manually since we don't have Flask-Migrate in Vercel
        with app.app_context():
            # Create portfolio_snapshot table
            db.session.execute(text("""
                CREATE TABLE IF NOT EXISTS portfolio_snapshot (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL REFERENCES "user"(id),
                    date DATE NOT NULL,
                    total_value FLOAT NOT NULL,
                    cash_flow FLOAT DEFAULT 0.0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, date)
                )
            """))
            
            # Create market_data table
            db.session.execute(text("""
                CREATE TABLE IF NOT EXISTS market_data (
                    id SERIAL PRIMARY KEY,
                    symbol VARCHAR(10) NOT NULL,
                    date DATE NOT NULL,
                    close_price FLOAT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(symbol, date)
                )
            """))
            
            # Create subscription_tier table
            db.session.execute(text("""
                CREATE TABLE IF NOT EXISTS subscription_tier (
                    id SERIAL PRIMARY KEY,
                    tier_name VARCHAR(50) NOT NULL UNIQUE,
                    price FLOAT NOT NULL,
                    max_trades_per_day INTEGER NOT NULL,
                    stripe_price_id VARCHAR(100) NOT NULL UNIQUE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """))
            
            # Create trade_limit table
            db.session.execute(text("""
                CREATE TABLE IF NOT EXISTS trade_limit (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL REFERENCES "user"(id),
                    date DATE NOT NULL,
                    trades_made INTEGER DEFAULT 0,
                    max_trades_allowed INTEGER NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, date)
                )
            """))
            
            # Create sms_notification table
            db.session.execute(text("""
                CREATE TABLE IF NOT EXISTS sms_notification (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL REFERENCES "user"(id),
                    phone_number VARCHAR(20),
                    is_verified BOOLEAN DEFAULT FALSE,
                    sms_enabled BOOLEAN DEFAULT TRUE,
                    verification_code VARCHAR(6),
                    verification_expires TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            """))
            
            # Create stock_info table
            db.session.execute(text("""
                CREATE TABLE IF NOT EXISTS stock_info (
                    id SERIAL PRIMARY KEY,
                    symbol VARCHAR(10) NOT NULL UNIQUE,
                    company_name VARCHAR(200),
                    market_cap BIGINT,
                    sector VARCHAR(100),
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """))
            
            # Create leaderboard_entry table
            db.session.execute(text("""
                CREATE TABLE IF NOT EXISTS leaderboard_entry (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL REFERENCES "user"(id),
                    date DATE NOT NULL,
                    portfolio_value FLOAT NOT NULL,
                    daily_return FLOAT DEFAULT 0.0,
                    total_return FLOAT DEFAULT 0.0,
                    rank_position INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, date)
                )
            """))
            
            db.session.commit()
        
        return jsonify({
            'success': True, 
            'message': 'Migration completed successfully'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()})

@app.route('/admin/fix-all-columns')
@login_required
def fix_all_columns():
    """Fix all missing columns in one go"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        results = []
        
        # Create all cache and metrics tables if they don't exist
        try:
            from models import LeaderboardCache, UserPortfolioChartCache, AlphaVantageAPILog, PlatformMetrics, UserActivity
            db.create_all()
            results.append('Created LeaderboardCache, UserPortfolioChartCache, AlphaVantageAPILog, PlatformMetrics, and UserActivity tables')
        except Exception as e:
            results.append(f'Cache and metrics table creation: {str(e)}')
        
        # Commit table creation first
        try:
            db.session.commit()
        except Exception as e:
            db.session.rollback()
            results.append(f'Error committing table creation: {str(e)}')
        
        # Fix SMS notification columns in separate transactions
        try:
            db.session.execute(text('ALTER TABLE sms_notification ADD COLUMN sms_enabled BOOLEAN DEFAULT TRUE'))
            db.session.commit()
            results.append('Added sms_enabled column to sms_notification')
        except Exception as e:
            db.session.rollback()
            if 'already exists' not in str(e).lower() and 'duplicate column' not in str(e).lower():
                results.append(f'Error adding sms_enabled: {str(e)}')
        
        try:
            db.session.execute(text('ALTER TABLE sms_notification ADD COLUMN verification_expires TIMESTAMP'))
            db.session.commit()
            results.append('Added verification_expires column to sms_notification')
        except Exception as e:
            db.session.rollback()
            if 'already exists' not in str(e).lower() and 'duplicate column' not in str(e).lower():
                results.append(f'Error adding verification_expires: {str(e)}')
        
        try:
            db.session.execute(text('ALTER TABLE sms_notification ADD COLUMN updated_at TIMESTAMP'))
            db.session.commit()
            results.append('Added updated_at column to sms_notification')
        except Exception as e:
            db.session.rollback()
            if 'already exists' not in str(e).lower() and 'duplicate column' not in str(e).lower():
                results.append(f'Error adding updated_at: {str(e)}')
        
        return jsonify({
            'success': True,
            'results': results,
            'message': 'All tables and columns created - full system ready with metrics tracking'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/verify-cache-migration')
@login_required
def admin_verify_cache_migration():
    """Verify that new _auth/_anon cache format is working and old format can be cleaned up"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import LeaderboardCache
        
        # Count cache entries by format
        all_caches = LeaderboardCache.query.all()
        
        new_format = []  # Entries with _auth or _anon suffix
        old_format = []  # Entries without suffix
        
        for cache in all_caches:
            if cache.period.endswith('_auth') or cache.period.endswith('_anon'):
                new_format.append({
                    'period': cache.period,
                    'has_html': bool(cache.rendered_html),
                    'generated_at': str(cache.generated_at)
                })
            else:
                old_format.append({
                    'period': cache.period,
                    'has_html': bool(cache.rendered_html),
                    'generated_at': str(cache.generated_at)
                })
        
        # Expected: 8 periods  3 categories  2 formats = 48 new entries
        expected_count = 48
        actual_count = len(new_format)
        
        return jsonify({
            'success': True,
            'new_format_count': actual_count,
            'old_format_count': len(old_format),
            'expected_new_count': expected_count,
            'migration_complete': actual_count >= expected_count,
            'can_delete_old_format': actual_count >= expected_count and len(old_format) > 0,
            'new_format_samples': new_format[:5],  # Show first 5
            'old_format_samples': old_format[:5],
            'recommendation': 'DELETE old format entries' if actual_count >= expected_count else 'WAIT for next cron job'
        })
        
    except Exception as e:
        logger.error(f"Error verifying cache migration: {str(e)}")
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/delete-old-cache-format')
@login_required
def admin_delete_old_cache_format():
    """Delete old cache entries without _auth/_anon suffix after migration is complete"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import LeaderboardCache
        
        # Safety check: Only delete if new format exists
        new_format_count = LeaderboardCache.query.filter(
            (LeaderboardCache.period.like('%_auth')) | (LeaderboardCache.period.like('%_anon'))
        ).count()
        
        if new_format_count < 48:  # 8 periods  3 categories  2 = 48
            return jsonify({
                'error': 'Migration not complete - cannot delete old format',
                'new_format_count': new_format_count,
                'expected': 48
            }), 400
        
        # Delete old format (entries without _auth/_anon suffix)
        deleted = 0
        all_caches = LeaderboardCache.query.all()
        for cache in all_caches:
            if not (cache.period.endswith('_auth') or cache.period.endswith('_anon')):
                db.session.delete(cache)
                deleted += 1
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'Deleted {deleted} old format cache entries',
            'remaining_new_format': new_format_count
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error deleting old cache format: {str(e)}")
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/investigate-sept-snapshots')
@login_required
def admin_investigate_sept_snapshots():
    """Investigate witty-raven's snapshots from Sept 2-10, 2025"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date
        from models import PortfolioSnapshot, Transaction, Stock, MarketData, User
        from sqlalchemy import and_
        import json as json_module
        
        USER_ID = 5  # witty-raven
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        results = {
            'user_id': USER_ID,
            'date_range': f"{START_DATE} to {END_DATE}",
            'snapshots': [],
            'transactions': [],
            'market_data_check': {},
            'user_info': {}
        }
        
        # Get user info
        user = User.query.get(USER_ID)
        if user:
            results['user_info'] = {
                'username': user.username,
                'email': user.email,
                'oauth_provider': user.oauth_provider
            }
        
        # Get snapshots
        snapshots = PortfolioSnapshot.query.filter(
            and_(
                PortfolioSnapshot.user_id == USER_ID,
                PortfolioSnapshot.date >= START_DATE,
                PortfolioSnapshot.date <= END_DATE
            )
        ).order_by(PortfolioSnapshot.date).all()
        
        # Get first activity date from earliest snapshot (any date, not just this period)
        first_snapshot = PortfolioSnapshot.query.filter_by(user_id=USER_ID).order_by(PortfolioSnapshot.date.asc()).first()
        if first_snapshot and user:
            results['user_info']['first_activity_date'] = str(first_snapshot.date)
        
        if snapshots:
            baseline = snapshots[0].total_value
            results['baseline_value'] = baseline
            
            for snapshot in snapshots:
                change = snapshot.total_value - baseline
                pct_change = (change / baseline * 100) if baseline > 0 else 0
                
                # Get user's net holdings on this date (GROK-RECOMMENDED: GROUP BY with SUM)
                # Stock table is a transaction log - need to sum quantities to get net position
                from sqlalchemy import func
                holdings_query = db.session.query(
                    Stock.ticker,
                    func.sum(Stock.quantity).label('net_quantity')
                ).filter(
                    Stock.user_id == USER_ID,
                    Stock.purchase_date <= snapshot.date
                ).group_by(Stock.ticker).having(
                    func.sum(Stock.quantity) > 0  # Exclude fully sold positions
                ).all()
                
                holdings_summary = {ticker: float(qty) for ticker, qty in holdings_query}
                
                results['snapshots'].append({
                    'date': str(snapshot.date),
                    'total_value': round(snapshot.total_value, 2),
                    'change_from_baseline': round(change, 2),
                    'pct_change': round(pct_change, 2),
                    'holdings': holdings_summary
                })
        
        # Get transactions (FIXED: Transaction uses timestamp, not date)
        transactions = Transaction.query.filter(
            and_(
                Transaction.user_id == USER_ID,
                func.date(Transaction.timestamp) >= START_DATE,
                func.date(Transaction.timestamp) <= END_DATE
            )
        ).order_by(Transaction.timestamp).all()
        
        for txn in transactions:
            # Transaction model has: ticker, quantity, price, transaction_type, timestamp
            results['transactions'].append({
                'date': str(txn.timestamp.date()),
                'time': str(txn.timestamp.time()),
                'type': txn.transaction_type,
                'ticker': txn.ticker,
                'quantity': txn.quantity,
                'price': txn.price,
                'total': round(txn.quantity * txn.price, 2)
            })
        
        # Check market data availability
        # Collect unique tickers from the holdings we calculated above
        all_stocks = set()
        for snapshot_data in results['snapshots']:
            if 'holdings' in snapshot_data and snapshot_data['holdings']:
                all_stocks.update(snapshot_data['holdings'].keys())
        
        results['stocks_in_portfolio'] = sorted(list(all_stocks))
        
        # Check if market data exists for each stock
        for ticker in all_stocks:
            market_data = MarketData.query.filter(
                and_(
                    MarketData.ticker == ticker,
                    MarketData.date >= START_DATE,
                    MarketData.date <= END_DATE
                )
            ).order_by(MarketData.date).all()
            
            if market_data:
                results['market_data_check'][ticker] = {
                    'data_points_found': len(market_data),
                    'dates': [str(md.date) for md in market_data],
                    'prices': [round(md.close_price, 2) for md in market_data]
                }
            else:
                results['market_data_check'][ticker] = {
                    'data_points_found': 0,
                    'error': 'No market data found for this period'
                }
        
        # Summary analysis
        if snapshots:
            results['summary'] = {
                'total_snapshots': len(snapshots),
                'total_transactions': len(transactions),
                'baseline_date': str(snapshots[0].date),
                'baseline_value': round(snapshots[0].total_value, 2),
                'end_date': str(snapshots[-1].date),
                'end_value': round(snapshots[-1].total_value, 2),
                'total_change_dollars': round(snapshots[-1].total_value - snapshots[0].total_value, 2),
                'total_change_percent': round(((snapshots[-1].total_value - snapshots[0].total_value) / snapshots[0].total_value * 100) if snapshots[0].total_value > 0 else 0, 2)
            }
        
        return jsonify(results)
        
    except Exception as e:
        logger.error(f"Error investigating Sept snapshots: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/investigate-sept-deep-dive')
@login_required
def admin_investigate_sept_deep_dive():
    """Deep investigation: Was the Sept 3 6.87% drop real or data corruption?"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date
        from models import User, PortfolioSnapshot, Stock, MarketData
        import json as json_module
        
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        results = {
            'investigation_period': f'{START_DATE} to {END_DATE}',
            'all_users_analysis': {},
            'market_data_availability': {},
            'sp500_actual_performance': None,
            'witty_raven_deep_dive': {},
            'systemic_issue': False,
            'backfill_recommendation': {}
        }
        
        # QUESTION 1: Did ALL users experience similar issues?
        all_users = User.query.all()
        suspicious_count = 0
        
        for user in all_users:
            snapshots = PortfolioSnapshot.query.filter(
                and_(
                    PortfolioSnapshot.user_id == user.id,
                    PortfolioSnapshot.date >= START_DATE,
                    PortfolioSnapshot.date <= END_DATE
                )
            ).order_by(PortfolioSnapshot.date).all()
            
            if len(snapshots) > 0:
                # Check for frozen values
                frozen_days = 0
                prev_value = None
                daily_values = []
                
                for snapshot in snapshots:
                    daily_values.append({
                        'date': str(snapshot.date),
                        'value': round(snapshot.total_value, 2)
                    })
                    
                    if prev_value is not None and abs(snapshot.total_value - prev_value) < 0.01:
                        frozen_days += 1
                    prev_value = snapshot.total_value
                
                # Calculate max drop
                values = [s.total_value for s in snapshots]
                max_value = max(values)
                min_value = min(values)
                max_drop_pct = ((min_value - max_value) / max_value * 100) if max_value > 0 else 0
                
                is_suspicious = frozen_days > 3 or max_drop_pct < -5
                if is_suspicious:
                    suspicious_count += 1
                
                results['all_users_analysis'][user.username] = {
                    'user_id': user.id,
                    'snapshots_count': len(snapshots),
                    'frozen_days': frozen_days,
                    'max_drop_pct': round(max_drop_pct, 2),
                    'daily_values': daily_values,
                    'suspicious': is_suspicious
                }
        
        results['systemic_issue'] = suspicious_count > 1
        results['affected_users_count'] = suspicious_count
        
        # QUESTION 2: What market data exists?
        tickers = ['AAPL', 'SSPY', 'TSLA', 'SPY']
        
        for ticker in tickers:
            market_data = MarketData.query.filter(
                and_(
                    MarketData.ticker == ticker,
                    MarketData.date >= START_DATE,
                    MarketData.date <= END_DATE
                )
            ).order_by(MarketData.date).all()
            
            if len(market_data) > 0:
                results['market_data_availability'][ticker] = {
                    'data_points': len(market_data),
                    'prices': [{'date': str(md.date), 'close': round(md.close_price, 2)} for md in market_data]
                }
            else:
                results['market_data_availability'][ticker] = {
                    'data_points': 0,
                    'error': 'NO DATA FOUND'
                }
        
        # QUESTION 3: What was S&P 500's actual performance?
        sp500_tickers = ['SPY', 'SPY_SP500', '^GSPC', 'SSPY']
        
        for ticker in sp500_tickers:
            sp500_data = MarketData.query.filter(
                and_(
                    MarketData.ticker == ticker,
                    MarketData.date >= START_DATE,
                    MarketData.date <= END_DATE
                )
            ).order_by(MarketData.date).all()
            
            if len(sp500_data) >= 2:
                first_price = sp500_data[0].close_price
                performance = []
                for md in sp500_data:
                    pct_change = ((md.close_price - first_price) / first_price * 100)
                    performance.append({
                        'date': str(md.date),
                        'price': round(md.close_price, 2),
                        'pct_change': round(pct_change, 2)
                    })
                
                results['sp500_actual_performance'] = {
                    'ticker': ticker,
                    'performance': performance
                }
                break
        
        # QUESTION 4: witty-raven deep dive
        witty_raven = User.query.filter_by(username='witty-raven').first()
        if witty_raven:
            sept2 = PortfolioSnapshot.query.filter_by(
                user_id=witty_raven.id,
                date=date(2025, 9, 2)
            ).first()
            
            sept3 = PortfolioSnapshot.query.filter_by(
                user_id=witty_raven.id,
                date=date(2025, 9, 3)
            ).first()
            
            if sept2 and sept3:
                # Get holdings
                holdings = db.session.query(
                    Stock.ticker,
                    func.sum(Stock.quantity).label('net_quantity')
                ).filter(
                    Stock.user_id == witty_raven.id,
                    Stock.purchase_date <= date(2025, 9, 2)
                ).group_by(Stock.ticker).having(
                    func.sum(Stock.quantity) > 0
                ).all()
                
                holdings_analysis = {}
                for ticker, qty in holdings:
                    sept2_price = MarketData.query.filter_by(ticker=ticker, date=date(2025, 9, 2)).first()
                    sept3_price = MarketData.query.filter_by(ticker=ticker, date=date(2025, 9, 3)).first()
                    
                    holdings_analysis[ticker] = {
                        'quantity': float(qty),
                        'sept2_price': round(sept2_price.close_price, 2) if sept2_price else 'MISSING',
                        'sept3_price': round(sept3_price.close_price, 2) if sept3_price else 'MISSING',
                        'price_change_pct': round(((sept3_price.close_price - sept2_price.close_price) / sept2_price.close_price * 100), 2) if sept2_price and sept3_price else 'CANNOT CALCULATE',
                        'position_impact': round(qty * (sept3_price.close_price - sept2_price.close_price), 2) if sept2_price and sept3_price else 'CANNOT CALCULATE'
                    }
                
                results['witty_raven_deep_dive'] = {
                    'sept2_portfolio_value': round(sept2.total_value, 2),
                    'sept3_portfolio_value': round(sept3.total_value, 2),
                    'drop_amount': round(sept3.total_value - sept2.total_value, 2),
                    'drop_pct': round(((sept3.total_value - sept2.total_value) / sept2.total_value * 100), 2),
                    'holdings_breakdown': holdings_analysis
                }
        
        # QUESTION 5: Backfill recommendation
        missing_tickers = [ticker for ticker, data in results['market_data_availability'].items() if data['data_points'] == 0]
        
        results['backfill_recommendation'] = {
            'needed': len(missing_tickers) > 0,
            'missing_tickers': missing_tickers,
            'api_calls_required': len(missing_tickers),
            'method': 'Alpha Vantage TIME_SERIES_DAILY',
            'date_range': f'{START_DATE} to {END_DATE} (7 trading days)',
            'estimated_time': f'{len(missing_tickers) * 12} seconds' if len(missing_tickers) > 0 else '0 seconds',
            'free_tier_sufficient': len(missing_tickers) <= 25,
            'next_steps': [
                f'1. Call Alpha Vantage for each ticker: {", ".join(missing_tickers)}',
                '2. Insert historical prices into MarketData table',
                '3. Recalculate portfolio snapshots for Sept 2-11',
                '4. Clear and regenerate chart cache for affected users'
            ] if len(missing_tickers) > 0 else ['No backfill needed - all market data exists']
        }
        
        return jsonify(results), 200
        
    except Exception as e:
        logger.error(f"Error in Sept deep dive: {str(e)}")
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/backfill-sept-data', methods=['POST'])
@login_required
def admin_backfill_sept_data():
    """Backfill Sept 2-11 market data using Alpha Vantage and recalculate everything"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        import requests
        from datetime import date, datetime, timedelta
        from models import MarketData, PortfolioSnapshot, User, Stock, UserPortfolioChartCache, LeaderboardCache
        import json as json_module
        
        ALPHA_VANTAGE_KEY = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not ALPHA_VANTAGE_KEY:
            return jsonify({'error': 'ALPHA_VANTAGE_API_KEY not configured'}), 500
        
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        results = {
            'start_time': datetime.now().isoformat(),
            'phase_1_api_calls': {},
            'phase_2_data_insertion': {},
            'phase_3_snapshot_recalculation': {},
            'phase_4_cache_invalidation': {},
            'phase_5_verification': {},
            'success': False,
            'errors': []
        }
        
        # ========================================================================
        # PHASE 1: FETCH DATA FROM ALPHA VANTAGE
        # ========================================================================
        logger.info("PHASE 1: Fetching historical data from Alpha Vantage")
        
        tickers = ['AAPL', 'SSPY', 'TSLA', 'SPY']
        fetched_data = {}
        
        for ticker in tickers:
            try:
                logger.info(f"  Fetching {ticker}...")
                url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={ALPHA_VANTAGE_KEY}&outputsize=compact'
                
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                # Validate response
                if 'Error Message' in data:
                    results['errors'].append(f"{ticker}: {data['Error Message']}")
                    results['phase_1_api_calls'][ticker] = {
                        'success': False,
                        'error': data['Error Message']
                    }
                    continue
                
                if 'Note' in data:
                    results['errors'].append(f"{ticker}: API rate limit - {data['Note']}")
                    results['phase_1_api_calls'][ticker] = {
                        'success': False,
                        'error': 'Rate limit exceeded'
                    }
                    continue
                
                if 'Time Series (Daily)' not in data:
                    results['errors'].append(f"{ticker}: Unexpected API response format")
                    results['phase_1_api_calls'][ticker] = {
                        'success': False,
                        'error': 'Invalid response format',
                        'response_keys': list(data.keys())
                    }
                    continue
                
                # Extract data for Sept 2-11
                time_series = data['Time Series (Daily)']
                ticker_prices = {}
                
                current_date = START_DATE
                while current_date <= END_DATE:
                    date_str = current_date.strftime('%Y-%m-%d')
                    if date_str in time_series:
                        ticker_prices[current_date] = {
                            'open': float(time_series[date_str]['1. open']),
                            'high': float(time_series[date_str]['2. high']),
                            'low': float(time_series[date_str]['3. low']),
                            'close': float(time_series[date_str]['4. close']),
                            'volume': int(time_series[date_str]['5. volume'])
                        }
                    current_date += timedelta(days=1)
                
                fetched_data[ticker] = ticker_prices
                
                results['phase_1_api_calls'][ticker] = {
                    'success': True,
                    'data_points': len(ticker_prices),
                    'dates': [str(d) for d in sorted(ticker_prices.keys())],
                    'sample_close_prices': {str(d): ticker_prices[d]['close'] for d in sorted(ticker_prices.keys())[:3]}
                }
                
                logger.info(f"   {ticker}: {len(ticker_prices)} data points fetched")
                
                # Rate limit: 5 calls/minute for free tier
                import time
                time.sleep(12)  # Wait 12 seconds between calls
                
            except Exception as e:
                results['errors'].append(f"{ticker}: {str(e)}")
                results['phase_1_api_calls'][ticker] = {
                    'success': False,
                    'error': str(e)
                }
                logger.error(f"   {ticker}: {str(e)}")
        
        # ========================================================================
        # PHASE 2: INSERT DATA INTO MARKETDATA TABLE
        # ========================================================================
        logger.info("PHASE 2: Inserting data into MarketData table")
        
        inserted_count = 0
        updated_count = 0
        
        for ticker, prices in fetched_data.items():
            ticker_inserted = 0
            ticker_updated = 0
            
            for trade_date, price_data in prices.items():
                # Check if entry already exists
                existing = MarketData.query.filter_by(
                    ticker=ticker,
                    date=trade_date
                ).first()
                
                if existing:
                    # Update existing (MarketData only has close_price, not OHLCV)
                    existing.close_price = price_data['close']
                    ticker_updated += 1
                else:
                    # Insert new (MarketData only has close_price, not OHLCV)
                    market_data = MarketData(
                        ticker=ticker,
                        date=trade_date,
                        close_price=price_data['close']
                    )
                    db.session.add(market_data)
                    ticker_inserted += 1
            
            inserted_count += ticker_inserted
            updated_count += ticker_updated
            
            results['phase_2_data_insertion'][ticker] = {
                'inserted': ticker_inserted,
                'updated': ticker_updated,
                'total': ticker_inserted + ticker_updated
            }
        
        db.session.commit()
        logger.info(f"   Inserted: {inserted_count}, Updated: {updated_count}")
        
        # ========================================================================
        # PHASE 3: RECALCULATE PORTFOLIO SNAPSHOTS
        # ========================================================================
        logger.info("PHASE 3: Recalculating portfolio snapshots for all users")
        
        from portfolio_performance import PortfolioPerformanceCalculator
        calculator = PortfolioPerformanceCalculator()
        
        all_users = User.query.all()
        recalculated_users = {}
        
        for user in all_users:
            before_snapshots = {}
            after_snapshots = {}
            
            # Get BEFORE values
            current_date = START_DATE
            while current_date <= END_DATE:
                snapshot = PortfolioSnapshot.query.filter_by(
                    user_id=user.id,
                    date=current_date
                ).first()
                if snapshot:
                    before_snapshots[str(current_date)] = snapshot.total_value
                current_date += timedelta(days=1)
            
            # Recalculate
            try:
                current_date = START_DATE
                while current_date <= END_DATE:
                    # Get user's holdings on this date
                    holdings = db.session.query(
                        Stock.ticker,
                        func.sum(Stock.quantity).label('net_quantity')
                    ).filter(
                        Stock.user_id == user.id,
                        Stock.purchase_date <= current_date
                    ).group_by(Stock.ticker).having(
                        func.sum(Stock.quantity) > 0
                    ).all()
                    
                    # Calculate portfolio value using NEW market data
                    portfolio_value = 0.0
                    for ticker, qty in holdings:
                        market_data = MarketData.query.filter_by(
                            ticker=ticker,
                            date=current_date
                        ).first()
                        
                        if market_data:
                            portfolio_value += qty * market_data.close_price
                    
                    # Update or create snapshot
                    snapshot = PortfolioSnapshot.query.filter_by(
                        user_id=user.id,
                        date=current_date
                    ).first()
                    
                    if snapshot:
                        snapshot.total_value = portfolio_value
                    else:
                        snapshot = PortfolioSnapshot(
                            user_id=user.id,
                            date=current_date,
                            total_value=portfolio_value,
                            cash_flow=0.0
                        )
                        db.session.add(snapshot)
                    
                    after_snapshots[str(current_date)] = portfolio_value
                    
                    current_date += timedelta(days=1)
                
                db.session.commit()
                
                # Calculate differences
                changes = {}
                for date_str in before_snapshots.keys():
                    before = before_snapshots.get(date_str, 0)
                    after = after_snapshots.get(date_str, 0)
                    if before != after:
                        changes[date_str] = {
                            'before': round(before, 2),
                            'after': round(after, 2),
                            'difference': round(after - before, 2),
                            'pct_change': round(((after - before) / before * 100) if before > 0 else 0, 2)
                        }
                
                recalculated_users[user.username] = {
                    'user_id': user.id,
                    'snapshots_updated': len(after_snapshots),
                    'values_changed': len(changes),
                    'changes': changes
                }
                
                logger.info(f"   {user.username}: {len(changes)} values changed")
                
            except Exception as e:
                results['errors'].append(f"Recalculation failed for {user.username}: {str(e)}")
                logger.error(f"   {user.username}: {str(e)}")
        
        results['phase_3_snapshot_recalculation'] = recalculated_users
        
        # ========================================================================
        # PHASE 4: INVALIDATE ALL CACHES
        # ========================================================================
        logger.info("PHASE 4: Clearing all caches")
        
        # Cache 1: UserPortfolioChartCache
        chart_cache_count = UserPortfolioChartCache.query.delete()
        
        # Cache 2: LeaderboardCache
        leaderboard_cache_count = LeaderboardCache.query.delete()
        
        # Cache 3: SP500ChartCache (if exists)
        try:
            from models import SP500ChartCache
            sp500_cache_count = SP500ChartCache.query.delete()
        except:
            sp500_cache_count = 0
        
        db.session.commit()
        
        results['phase_4_cache_invalidation'] = {
            'UserPortfolioChartCache_cleared': chart_cache_count,
            'LeaderboardCache_cleared': leaderboard_cache_count,
            'SP500ChartCache_cleared': sp500_cache_count,
            'total_cache_entries_cleared': chart_cache_count + leaderboard_cache_count + sp500_cache_count
        }
        
        logger.info(f"   Cleared {chart_cache_count + leaderboard_cache_count + sp500_cache_count} cache entries")
        
        # ========================================================================
        # PHASE 5: VERIFICATION - DATA FLOW TRACE
        # ========================================================================
        logger.info("PHASE 5: Verifying data flow end-to-end")
        
        # Verify witty-raven's data specifically
        witty_raven = User.query.filter_by(username='witty-raven').first()
        if witty_raven:
            verification = {
                'market_data_check': {},
                'snapshot_values': {},
                'cache_status': {}
            }
            
            # Check market data now exists
            for ticker in ['AAPL', 'SSPY', 'TSLA']:
                count = MarketData.query.filter(
                    and_(
                        MarketData.ticker == ticker,
                        MarketData.date >= START_DATE,
                        MarketData.date <= END_DATE
                    )
                ).count()
                verification['market_data_check'][ticker] = {
                    'data_points': count,
                    'status': ' COMPLETE' if count > 0 else ' MISSING'
                }
            
            # Check snapshot values
            snapshots = PortfolioSnapshot.query.filter(
                and_(
                    PortfolioSnapshot.user_id == witty_raven.id,
                    PortfolioSnapshot.date >= START_DATE,
                    PortfolioSnapshot.date <= END_DATE
                )
            ).order_by(PortfolioSnapshot.date).all()
            
            for snapshot in snapshots:
                verification['snapshot_values'][str(snapshot.date)] = round(snapshot.total_value, 2)
            
            # Verify no frozen values (consecutive days should differ)
            frozen_count = 0
            prev_value = None
            for snapshot in snapshots:
                if prev_value is not None and abs(snapshot.total_value - prev_value) < 0.01:
                    frozen_count += 1
                prev_value = snapshot.total_value
            
            verification['frozen_values_remaining'] = frozen_count
            verification['data_quality'] = ' GOOD' if frozen_count < 3 else ' STILL FROZEN'
            
            # Check cache status
            verification['cache_status'] = {
                'chart_cache_entries': UserPortfolioChartCache.query.filter_by(user_id=witty_raven.id).count(),
                'leaderboard_cache_entries': LeaderboardCache.query.count(),
                'status': ' CLEARED' if UserPortfolioChartCache.query.filter_by(user_id=witty_raven.id).count() == 0 else ' STILL CACHED'
            }
            
            results['phase_5_verification'] = verification
        
        # ========================================================================
        # FINAL STATUS
        # ========================================================================
        results['end_time'] = datetime.now().isoformat()
        results['success'] = len(results['errors']) == 0
        results['summary'] = {
            'api_calls_succeeded': sum(1 for v in results['phase_1_api_calls'].values() if v.get('success')),
            'api_calls_failed': sum(1 for v in results['phase_1_api_calls'].values() if not v.get('success')),
            'market_data_inserted': inserted_count,
            'market_data_updated': updated_count,
            'users_recalculated': len(recalculated_users),
            'cache_entries_cleared': chart_cache_count + leaderboard_cache_count + sp500_cache_count,
            'errors': len(results['errors'])
        }
        
        logger.info("=" * 80)
        logger.info("BACKFILL COMPLETE")
        logger.info(f"Success: {results['success']}")
        logger.info(f"Errors: {len(results['errors'])}")
        logger.info("=" * 80)
        
        return jsonify(results), 200
        
    except Exception as e:
        logger.error(f"Error in backfill: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/run-sept-backfill')
@login_required
def admin_run_sept_backfill():
    """GET wrapper for backfill - just visit this URL to trigger the backfill"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Call the POST endpoint internally
        return admin_backfill_sept_data()
        
    except Exception as e:
        logger.error(f"Error in backfill wrapper: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/verify-sept-backfill')
@login_required
def admin_verify_sept_backfill():
    """Comprehensive verification: Check ALL users' holdings and validate backfill completeness"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date
        from models import User, PortfolioSnapshot, Stock, MarketData
        import json as json_module
        
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        results = {
            'verification_date': datetime.now().isoformat(),
            'period': f'{START_DATE} to {END_DATE}',
            'all_users_detailed_analysis': {},
            'missing_market_data': {},
            'sept_9_investigation': {},
            'backfill_completeness': {
                'total_tickers_needed': 0,
                'tickers_with_data': [],
                'tickers_missing_data': [],
                'additional_backfill_required': False
            },
            'recommendations': []
        }
        
        # Get all unique tickers held by all users during this period
        all_tickers_needed = set()
        
        all_users = User.query.all()
        
        for user in all_users:
            user_analysis = {
                'user_id': user.id,
                'email': user.email,
                'holdings_during_period': {},
                'market_data_coverage': {},
                'portfolio_values': {
                    'before_backfill': {},
                    'after_backfill': {},
                    'changes': {}
                },
                'issues': []
            }
            
            # Get user's holdings during Sept 2-11
            holdings = db.session.query(
                Stock.ticker,
                func.sum(Stock.quantity).label('net_quantity')
            ).filter(
                Stock.user_id == user.id,
                Stock.purchase_date <= END_DATE
            ).group_by(Stock.ticker).having(
                func.sum(Stock.quantity) > 0
            ).all()
            
            if not holdings:
                user_analysis['issues'].append('No holdings found during this period')
                results['all_users_detailed_analysis'][user.username] = user_analysis
                continue
            
            # Track holdings
            for ticker, qty in holdings:
                all_tickers_needed.add(ticker)
                user_analysis['holdings_during_period'][ticker] = float(qty)
                
                # Check if we have market data for this ticker for Sept 2-11
                market_data_count = MarketData.query.filter(
                    and_(
                        MarketData.ticker == ticker,
                        MarketData.date >= START_DATE,
                        MarketData.date <= END_DATE
                    )
                ).count()
                
                user_analysis['market_data_coverage'][ticker] = {
                    'data_points': market_data_count,
                    'expected': 8,  # Sept 2,3,4,5,8,9,10,11 (no weekends)
                    'status': ' COMPLETE' if market_data_count >= 6 else ' INCOMPLETE',
                    'missing_days': 8 - market_data_count if market_data_count < 8 else 0
                }
                
                if market_data_count < 6:
                    user_analysis['issues'].append(f'{ticker}: Only {market_data_count}/8 days of market data')
            
            # Get portfolio snapshots and show before/after
            # BEFORE values are in the backfill results, but let's check current DB state
            snapshots = PortfolioSnapshot.query.filter(
                and_(
                    PortfolioSnapshot.user_id == user.id,
                    PortfolioSnapshot.date >= START_DATE,
                    PortfolioSnapshot.date <= END_DATE
                )
            ).order_by(PortfolioSnapshot.date).all()
            
            for snapshot in snapshots:
                user_analysis['portfolio_values']['after_backfill'][str(snapshot.date)] = round(snapshot.total_value, 2)
            
            # Check for zero values or frozen values
            zero_count = sum(1 for v in user_analysis['portfolio_values']['after_backfill'].values() if v == 0)
            if zero_count > 0:
                user_analysis['issues'].append(f'{zero_count} days with $0.00 portfolio value')
            
            # Check for frozen values
            values = list(user_analysis['portfolio_values']['after_backfill'].values())
            frozen_count = 0
            for i in range(1, len(values)):
                if values[i] == values[i-1] and values[i] != 0:
                    frozen_count += 1
            if frozen_count > 2:
                user_analysis['issues'].append(f'{frozen_count} consecutive frozen values')
            
            results['all_users_detailed_analysis'][user.username] = user_analysis
        
        # Summary of all tickers needed
        results['backfill_completeness']['total_tickers_needed'] = len(all_tickers_needed)
        results['backfill_completeness']['all_tickers'] = sorted(list(all_tickers_needed))
        
        # Check market data coverage for each ticker
        for ticker in all_tickers_needed:
            data_count = MarketData.query.filter(
                and_(
                    MarketData.ticker == ticker,
                    MarketData.date >= START_DATE,
                    MarketData.date <= END_DATE
                )
            ).count()
            
            if data_count >= 6:
                results['backfill_completeness']['tickers_with_data'].append({
                    'ticker': ticker,
                    'data_points': data_count
                })
            else:
                results['backfill_completeness']['tickers_missing_data'].append({
                    'ticker': ticker,
                    'data_points': data_count,
                    'missing': 8 - data_count
                })
                results['missing_market_data'][ticker] = {
                    'data_points_found': data_count,
                    'data_points_expected': 8,
                    'missing_days': 8 - data_count,
                    'status': ' INCOMPLETE - BACKFILL NEEDED'
                }
        
        results['backfill_completeness']['additional_backfill_required'] = len(results['backfill_completeness']['tickers_missing_data']) > 0
        
        # Investigate Sept 9 specifically
        results['sept_9_investigation'] = {
            'date': '2025-09-09',
            'market_data_available': {}
        }
        
        sept_9 = date(2025, 9, 9)
        for ticker in all_tickers_needed:
            market_data = MarketData.query.filter_by(
                ticker=ticker,
                date=sept_9
            ).first()
            
            if market_data:
                results['sept_9_investigation']['market_data_available'][ticker] = {
                    'close_price': round(market_data.close_price, 2),
                    'status': ' EXISTS'
                }
            else:
                results['sept_9_investigation']['market_data_available'][ticker] = {
                    'status': ' MISSING - This is why Sept 9 values may be incorrect'
                }
        
        # Generate recommendations
        if results['backfill_completeness']['additional_backfill_required']:
            missing_tickers = [t['ticker'] for t in results['backfill_completeness']['tickers_missing_data']]
            results['recommendations'].append({
                'priority': 'HIGH',
                'issue': f'Missing market data for {len(missing_tickers)} ticker(s)',
                'tickers': missing_tickers,
                'action': f'Run backfill for: {", ".join(missing_tickers)}',
                'impact': 'Portfolio calculations are incorrect without this data'
            })
        
        # Check for users with $0 portfolios
        for username, analysis in results['all_users_detailed_analysis'].items():
            zero_days = [d for d, v in analysis['portfolio_values']['after_backfill'].items() if v == 0]
            if len(zero_days) > 2 and analysis['holdings_during_period']:
                results['recommendations'].append({
                    'priority': 'MEDIUM',
                    'issue': f'{username} has {len(zero_days)} days with $0 portfolio',
                    'holdings': analysis['holdings_during_period'],
                    'action': 'Check if market data exists for their tickers',
                    'impact': 'User charts will be incorrect'
                })
        
        # Summary
        results['summary'] = {
            'total_users_analyzed': len(all_users),
            'total_tickers_in_use': len(all_tickers_needed),
            'tickers_fully_backfilled': len(results['backfill_completeness']['tickers_with_data']),
            'tickers_need_backfill': len(results['backfill_completeness']['tickers_missing_data']),
            'users_with_issues': sum(1 for a in results['all_users_detailed_analysis'].values() if a['issues']),
            'backfill_complete': not results['backfill_completeness']['additional_backfill_required']
        }
        
        return jsonify(results), 200
        
    except Exception as e:
        logger.error(f"Error in backfill verification: {str(e)}")
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/complete-sept-backfill')
@login_required
def admin_complete_sept_backfill_page():
    """Interface page for completing Sept backfill in batches (no timeout)"""
    try:
        if not current_user.is_admin:
            return "Admin access required", 403
        
        html = '''
<!DOCTYPE html>
<html>
<head>
    <title>Complete Sept 2-11 Backfill</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 20px auto; padding: 20px; }
        h1 { color: #2c3e50; }
        .batch { 
            background: #f8f9fa; 
            border: 2px solid #dee2e6; 
            border-radius: 8px; 
            padding: 15px; 
            margin: 15px 0; 
        }
        .batch h3 { margin-top: 0; color: #495057; }
        button { 
            background: #007bff; 
            color: white; 
            border: none; 
            padding: 12px 24px; 
            border-radius: 5px; 
            cursor: pointer; 
            font-size: 16px; 
            margin: 5px;
        }
        button:hover { background: #0056b3; }
        button:disabled { background: #6c757d; cursor: not-allowed; }
        .recalc-btn { background: #28a745; }
        .recalc-btn:hover { background: #218838; }
        .status { 
            margin-top: 10px; 
            padding: 10px; 
            border-radius: 5px; 
            display: none;
        }
        .status.loading { background: #fff3cd; border: 1px solid #ffc107; display: block; }
        .status.success { background: #d4edda; border: 1px solid #28a745; display: block; }
        .status.error { background: #f8d7da; border: 1px solid #dc3545; display: block; }
        .ticker-list { color: #6c757d; margin: 10px 0; }
        .progress { 
            background: #e9ecef; 
            height: 30px; 
            border-radius: 5px; 
            margin: 20px 0; 
            overflow: hidden;
        }
        .progress-bar { 
            background: #28a745; 
            height: 100%; 
            transition: width 0.3s; 
            display: flex; 
            align-items: center; 
            justify-content: center; 
            color: white; 
            font-weight: bold;
        }
        pre { 
            background: #f1f3f5; 
            padding: 10px; 
            border-radius: 5px; 
            overflow-x: auto; 
            max-height: 300px; 
            overflow-y: auto;
        }
        .warning { background: #fff3cd; border: 1px solid #ffc107; padding: 15px; border-radius: 5px; margin: 20px 0; }
    </style>
</head>
<body>
    <h1> Complete Sept 2-11 Backfill</h1>
    
    <div class="warning">
        <strong> Current Status:</strong><br>
         Only 3/22 tickers have data (AAPL, SSPY, TSLA)<br>
         19 tickers need backfill: GOOGL, NVDA, MSFT, AMZN, JPM, V, UNH, KO, PFE, O, SCHD, VTI, BRK-B, COST, HD, JNJ, LLY, WMT, XOM<br>
         Premium API: 150 calls/min (plenty of capacity)<br>
         Batched approach to avoid 60s timeout
    </div>
    
    <div class="progress">
        <div class="progress-bar" id="overallProgress" style="width: 0%">0/5 Complete</div>
    </div>
    
    <div class="batch">
        <h3>Batch 1: Tech Giants (5 tickers)</h3>
        <div class="ticker-list">GOOGL, NVDA, MSFT, AMZN, COST</div>
        <button onclick="runBatch(1, ['GOOGL', 'NVDA', 'MSFT', 'AMZN', 'COST'])">
             Backfill Batch 1
        </button>
        <div class="status" id="status1"></div>
    </div>
    
    <div class="batch">
        <h3>Batch 2: Financial & Healthcare (5 tickers)</h3>
        <div class="ticker-list">JPM, V, UNH, JNJ, LLY</div>
        <button onclick="runBatch(2, ['JPM', 'V', 'UNH', 'JNJ', 'LLY'])">
             Backfill Batch 2
        </button>
        <div class="status" id="status2"></div>
    </div>
    
    <div class="batch">
        <h3>Batch 3: Consumer & Retail (5 tickers)</h3>
        <div class="ticker-list">WMT, HD, KO, PFE, XOM</div>
        <button onclick="runBatch(3, ['WMT', 'HD', 'KO', 'PFE', 'XOM'])">
             Backfill Batch 3
        </button>
        <div class="status" id="status3"></div>
    </div>
    
    <div class="batch">
        <h3>Batch 4: ETFs & Dividends (4 tickers)</h3>
        <div class="ticker-list">VTI, SCHD, O, BRK-B</div>
        <button onclick="runBatch(4, ['VTI', 'SCHD', 'O', 'BRK-B'])">
             Backfill Batch 4
        </button>
        <div class="status" id="status4"></div>
    </div>
    
    <div class="batch" style="background: #ffe6e6; border-color: #dc3545;">
        <h3> Step 1: Health Check</h3>
        <p><strong>Start here - just count queries (very fast)</strong></p>
        
        <button class="recalc-btn" style="background: #dc3545;" onclick="window.open('/admin/db-health-check', '_blank')">
             DB Health Check (opens in new tab)
        </button>
        <div class="status" id="statusHealth"></div>
    </div>
    
    <div class="batch" style="background: #e6f7ff; border-color: #17a2b8;">
        <h3> Step 2: Bulk Recalculate All Users</h3>
        <p><strong>New approach: Pre-fetch ALL snapshots + bulk update (4 queries total)</strong></p>
        <p><em>Click each user one at a time. Should complete in ~5-10 seconds each.</em></p>
        
        <button class="recalc-btn" style="background: #17a2b8;" onclick="recalculateUserBulk('witty-raven')">
             Bulk: witty-raven
        </button>
        <div class="status" id="statusBulk-witty-raven"></div>
        
        <button class="recalc-btn" style="background: #17a2b8;" onclick="recalculateUserBulk('wise-buffalo')">
             Bulk: wise-buffalo
        </button>
        <div class="status" id="statusBulk-wise-buffalo"></div>
        
        <button class="recalc-btn" style="background: #17a2b8;" onclick="recalculateUserBulk('wild-bronco')">
             Bulk: wild-bronco
        </button>
        <div class="status" id="statusBulk-wild-bronco"></div>
        
        <button class="recalc-btn" style="background: #17a2b8;" onclick="recalculateUserBulk('testing2')">
             Bulk: testing2
        </button>
        <div class="status" id="statusBulk-testing2"></div>
        
        <button class="recalc-btn" style="background: #17a2b8;" onclick="recalculateUserBulk('testing3')">
             Bulk: testing3
        </button>
        <div class="status" id="statusBulk-testing3"></div>
    </div>
    
    <div class="batch" style="background: #e7f3ff; border-color: #007bff;">
        <h3>Final Step: Recalculate Portfolios (One User at a Time)</h3>
        <p><strong>Run these AFTER all batches complete</strong>. Click each button to avoid 60s timeout.</p>
        
        <button class="recalc-btn" onclick="recalculateUser('witty-raven')">
             Recalc: witty-raven
        </button>
        <div class="status" id="statusRecalc-witty-raven"></div>
        
        <button class="recalc-btn" onclick="recalculateUser('wise-buffalo')">
             Recalc: wise-buffalo
        </button>
        <div class="status" id="statusRecalc-wise-buffalo"></div>
        
        <button class="recalc-btn" onclick="recalculateUser('wild-bronco')">
             Recalc: wild-bronco
        </button>
        <div class="status" id="statusRecalc-wild-bronco"></div>
        
        <button class="recalc-btn" onclick="recalculateUser('testing2')">
             Recalc: testing2
        </button>
        <div class="status" id="statusRecalc-testing2"></div>
        
        <button class="recalc-btn" onclick="recalculateUser('testing3')">
             Recalc: testing3
        </button>
        <div class="status" id="statusRecalc-testing3"></div>
        
        <p style="margin-top: 15px;"><strong>After all users recalculated:</strong></p>
        <button class="recalc-btn" onclick="clearCaches()">
             Clear All Caches
        </button>
        <div class="status" id="statusClearCache"></div>
    </div>
    
    <script>
        let completedBatches = 0;
        
        async function runBatch(batchNum, tickers) {
            const statusDiv = document.getElementById('status' + batchNum);
            const button = event.target;
            
            button.disabled = true;
            statusDiv.className = 'status loading';
            statusDiv.innerHTML = ' Fetching data from Alpha Vantage... (~' + (tickers.length * 1) + 's)';
            
            try {
                const response = await fetch('/admin/backfill-batch', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ tickers: tickers })
                });
                
                const result = await response.json();
                
                if (result.success) {
                    statusDiv.className = 'status success';
                    statusDiv.innerHTML = ' <strong>Success!</strong><br>' +
                        'Inserted: ' + result.summary.inserted + ' entries<br>' +
                        'Updated: ' + result.summary.updated + ' entries<br>' +
                        'Time: ' + result.summary.duration + '<br>' +
                        '<details><summary>View Details</summary><pre>' + 
                        JSON.stringify(result.details, null, 2) + '</pre></details>';
                    
                    completedBatches++;
                    updateProgress();
                } else {
                    statusDiv.className = 'status error';
                    statusDiv.innerHTML = ' <strong>Error:</strong><br>' + result.error;
                    button.disabled = false;
                }
            } catch (error) {
                statusDiv.className = 'status error';
                statusDiv.innerHTML = ' <strong>Error:</strong><br>' + error.message;
                button.disabled = false;
            }
        }
        
        async function profileRecalculation(username) {
            const statusDiv = document.getElementById('statusProfile');
            const button = event.target;
            
            button.disabled = true;
            statusDiv.className = 'status loading';
            statusDiv.innerHTML = ' Profiling ' + username + '... (measuring performance)';
            
            try {
                const response = await fetch('/admin/profile-recalculation', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ username: username })
                });
                
                const result = await response.json();
                
                if (result.steps) {
                    // Build a detailed report
                    let report = ' <strong>Performance Profile:</strong><br>';
                    report += 'Total Time: ' + result.total_time + 's<br><br>';
                    
                    // Show slowest step first
                    if (result.slowest_step) {
                        report += ' <strong>SLOWEST STEP:</strong> ' + result.slowest_step.step + '<br>';
                        report += 'Time: ' + result.slowest_step.time + 's (' + result.slowest_step.percentage + '% of total)<br><br>';
                    }
                    
                    // Show all steps
                    report += '<strong>Detailed Breakdown:</strong><br>';
                    for (let step of result.steps) {
                        let percentage = ((step.time_seconds / result.total_time) * 100).toFixed(1);
                        report += step.step + ': <strong>' + step.time_seconds + 's</strong> (' + percentage + '%)<br>';
                        if (step.rows_fetched !== undefined) {
                            report += '&nbsp;&nbsp; Rows: ' + step.rows_fetched + '<br>';
                        }
                        if (step.cache_size !== undefined) {
                            report += '&nbsp;&nbsp; Cache size: ' + step.cache_size + '<br>';
                        }
                        if (step.snapshots_processed !== undefined) {
                            report += '&nbsp;&nbsp; Snapshots: ' + step.snapshots_processed + '<br>';
                        }
                        if (step.note) {
                            report += '&nbsp;&nbsp; ' + step.note + '<br>';
                        }
                    }
                    
                    statusDiv.className = 'status success';
                    statusDiv.innerHTML = report + '<br><details><summary>View Raw JSON</summary><pre>' + 
                        JSON.stringify(result, null, 2) + '</pre></details>';
                } else if (result.error) {
                    statusDiv.className = 'status error';
                    statusDiv.innerHTML = ' <strong>Error:</strong><br>' + result.error;
                    button.disabled = false;
                } else {
                    statusDiv.className = 'status error';
                    statusDiv.innerHTML = ' <strong>Unknown response</strong>';
                    button.disabled = false;
                }
            } catch (error) {
                statusDiv.className = 'status error';
                statusDiv.innerHTML = ' <strong>Error:</strong><br>' + error.message;
                button.disabled = false;
            }
        }
        
        async function recalculateUserBulk(username) {
            const statusDiv = document.getElementById('statusBulk-' + username);
            const button = event.target;
            
            button.disabled = true;
            statusDiv.className = 'status loading';
            statusDiv.innerHTML = ' Bulk recalculating ' + username + '... (4 queries only)';
            
            try {
                const response = await fetch('/admin/recalculate-user-bulk', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ username: username })
                });
                
                const result = await response.json();
                
                if (result.success) {
                    let changedCount = 0;
                    for (let date in result.daily_values) {
                        if (result.daily_values[date].changed) changedCount++;
                    }
                    
                    statusDiv.className = 'status success';
                    statusDiv.innerHTML = ' <strong>BULK UPDATE SUCCESS!</strong><br>' +
                        'Snapshots updated: ' + result.snapshots_updated + '<br>' +
                        'Values changed: ' + changedCount + '<br>' +
                        '<details><summary>View Daily Values</summary><pre>' + 
                        JSON.stringify(result.daily_values, null, 2) + '</pre></details>';
                } else {
                    statusDiv.className = 'status error';
                    statusDiv.innerHTML = ' <strong>Error:</strong><br>' + result.error;
                    button.disabled = false;
                }
            } catch (error) {
                statusDiv.className = 'status error';
                statusDiv.innerHTML = ' <strong>Error:</strong><br>' + error.message;
                button.disabled = false;
            }
        }
        
        async function recalculateUser(username) {
            const statusDiv = document.getElementById('statusRecalc-' + username);
            const button = event.target;
            
            button.disabled = true;
            statusDiv.className = 'status loading';
            statusDiv.innerHTML = ' Recalculating ' + username + '... (~5-10 seconds)';
            
            try {
                const response = await fetch('/admin/recalculate-user', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ username: username })
                });
                
                const result = await response.json();
                
                if (result.success) {
                    // Count how many values actually changed
                    let changedCount = 0;
                    for (let date in result.daily_values) {
                        if (result.daily_values[date].changed) changedCount++;
                    }
                    
                    statusDiv.className = 'status success';
                    statusDiv.innerHTML = ' <strong>Success!</strong><br>' +
                        'Snapshots updated: ' + result.snapshots_updated + '<br>' +
                        'Values changed: ' + changedCount + '<br>' +
                        '<details><summary>View Daily Values</summary><pre>' + 
                        JSON.stringify(result.daily_values, null, 2) + '</pre></details>';
                } else {
                    statusDiv.className = 'status error';
                    statusDiv.innerHTML = ' <strong>Error:</strong><br>' + result.error;
                    button.disabled = false;
                }
            } catch (error) {
                statusDiv.className = 'status error';
                statusDiv.innerHTML = ' <strong>Error:</strong><br>' + error.message;
                button.disabled = false;
            }
        }
        
        async function clearCaches() {
            const statusDiv = document.getElementById('statusClearCache');
            const button = event.target;
            
            button.disabled = true;
            statusDiv.className = 'status loading';
            statusDiv.innerHTML = ' Clearing chart and leaderboard caches...';
            
            try {
                const response = await fetch('/admin/clear-caches', {
                    method: 'POST'
                });
                
                const result = await response.json();
                
                if (result.success) {
                    statusDiv.className = 'status success';
                    statusDiv.innerHTML = ' <strong>Caches Cleared!</strong><br>' +
                        'Entries removed: ' + result.total_cleared;
                } else {
                    statusDiv.className = 'status error';
                    statusDiv.innerHTML = ' <strong>Error:</strong><br>' + result.error;
                    button.disabled = false;
                }
            } catch (error) {
                statusDiv.className = 'status error';
                statusDiv.innerHTML = ' <strong>Error:</strong><br>' + error.message;
                button.disabled = false;
            }
        }
        
        function updateProgress() {
            const progressBar = document.getElementById('overallProgress');
            const total = 5; // 4 batches + 1 recalc
            const percent = (completedBatches / 4) * 100; // Only count batches for progress
            progressBar.style.width = percent + '%';
            progressBar.textContent = completedBatches + '/4 Batches Complete';
        }
    </script>
</body>
</html>
        '''
        
        return html
        
    except Exception as e:
        logger.error(f"Error rendering backfill page: {str(e)}")
        return f"Error: {str(e)}", 500

@app.route('/admin/backfill-batch', methods=['POST'])
@login_required
def admin_backfill_batch():
    """Backfill a batch of tickers (called from button interface)"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        import requests
        from datetime import date, timedelta
        from models import MarketData
        import time as time_module
        
        data = request.get_json()
        tickers = data.get('tickers', [])
        
        if not tickers:
            return jsonify({'success': False, 'error': 'No tickers provided'}), 400
        
        ALPHA_VANTAGE_KEY = os.environ.get('ALPHA_VANTAGE_API_KEY')
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        start_time = time_module.time()
        results = {
            'success': True,
            'details': {},
            'summary': {'inserted': 0, 'updated': 0, 'errors': 0, 'duration': ''}
        }
        
        for ticker in tickers:
            try:
                # Fetch from Alpha Vantage
                url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={ALPHA_VANTAGE_KEY}&outputsize=compact'
                response = requests.get(url, timeout=30)
                data = response.json()
                
                if 'Time Series (Daily)' not in data:
                    results['details'][ticker] = {'success': False, 'error': 'Invalid API response'}
                    results['summary']['errors'] += 1
                    continue
                
                # Extract Sept 2-11 data
                time_series = data['Time Series (Daily)']
                inserted = 0
                updated = 0
                
                current_date = START_DATE
                while current_date <= END_DATE:
                    date_str = current_date.strftime('%Y-%m-%d')
                    if date_str in time_series:
                        close_price = float(time_series[date_str]['4. close'])
                        
                        existing = MarketData.query.filter_by(ticker=ticker, date=current_date).first()
                        if existing:
                            existing.close_price = close_price
                            updated += 1
                        else:
                            market_data = MarketData(ticker=ticker, date=current_date, close_price=close_price)
                            db.session.add(market_data)
                            inserted += 1
                    
                    current_date += timedelta(days=1)
                
                db.session.commit()
                
                results['details'][ticker] = {
                    'success': True,
                    'inserted': inserted,
                    'updated': updated
                }
                results['summary']['inserted'] += inserted
                results['summary']['updated'] += updated
                
                # Small delay for rate limiting (premium allows 150/min, so 0.5s is safe)
                time_module.sleep(0.5)
                
            except Exception as e:
                results['details'][ticker] = {'success': False, 'error': str(e)}
                results['summary']['errors'] += 1
        
        duration = round(time_module.time() - start_time, 1)
        results['summary']['duration'] = f'{duration}s'
        
        return jsonify(results), 200
        
    except Exception as e:
        logger.error(f"Error in batch backfill: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/admin/recalculate-sept-snapshots', methods=['POST'])
@login_required
def admin_recalculate_sept_snapshots():
    """Recalculate ALL users' portfolio snapshots for Sept 2-11"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, timedelta
        from models import User, PortfolioSnapshot, Stock, MarketData, UserPortfolioChartCache, LeaderboardCache
        
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        results = {
            'success': True,
            'details': {},
            'summary': {
                'users_recalculated': 0,
                'snapshots_updated': 0,
                'caches_cleared': 0
            }
        }
        
        all_users = User.query.all()
        
        for user in all_users:
            user_snapshots_updated = 0
            
            current_date = START_DATE
            while current_date <= END_DATE:
                # Get holdings on this date
                holdings = db.session.query(
                    Stock.ticker,
                    func.sum(Stock.quantity).label('net_quantity')
                ).filter(
                    Stock.user_id == user.id,
                    Stock.purchase_date <= current_date
                ).group_by(Stock.ticker).having(
                    func.sum(Stock.quantity) > 0
                ).all()
                
                # Calculate portfolio value
                portfolio_value = 0.0
                for ticker, qty in holdings:
                    market_data = MarketData.query.filter_by(ticker=ticker, date=current_date).first()
                    if market_data:
                        portfolio_value += qty * market_data.close_price
                
                # Update snapshot
                snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id, date=current_date).first()
                if snapshot:
                    snapshot.total_value = portfolio_value
                    # CRITICAL FIX: Explicitly mark as dirty and merge to ensure SQLAlchemy tracks the change
                    db.session.merge(snapshot)
                    user_snapshots_updated += 1
                elif portfolio_value > 0:
                    snapshot = PortfolioSnapshot(
                        user_id=user.id,
                        date=current_date,
                        total_value=portfolio_value,
                        cash_flow=0.0
                    )
                    db.session.add(snapshot)
                    user_snapshots_updated += 1
                
                current_date += timedelta(days=1)
            
            # CRITICAL FIX: Flush before commit to force SQL emission and catch errors
            db.session.flush()
            db.session.commit()
            
            results['details'][user.username] = {
                'user_id': user.id,
                'snapshots_updated': user_snapshots_updated
            }
            results['summary']['users_recalculated'] += 1
            results['summary']['snapshots_updated'] += user_snapshots_updated
        
        # Clear caches
        chart_cache = UserPortfolioChartCache.query.delete()
        leaderboard_cache = LeaderboardCache.query.delete()
        db.session.commit()
        
        results['summary']['caches_cleared'] = chart_cache + leaderboard_cache
        
        return jsonify(results), 200
        
    except Exception as e:
        logger.error(f"Error recalculating snapshots: {str(e)}")
        return jsonify({'success': False, 'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/recalculate-user', methods=['POST'])
@login_required
def admin_recalculate_user():
    """Recalculate a SINGLE user's snapshots using RAW SQL for speed"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, timedelta
        from models import User, Stock, MarketData, PortfolioSnapshot
        
        data = request.get_json()
        username = data.get('username')
        
        if not username:
            return jsonify({'success': False, 'error': 'No username provided'}), 400
        
        # Use ORM for user lookup (just 1 query, safe)
        user = User.query.filter_by(username=username).first()
        
        if not user:
            return jsonify({'success': False, 'error': f'User {username} not found'}), 404
        
        user_id = user.id
        
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        results = {
            'success': True,
            'username': username,
            'user_id': user_id,
            'snapshots_updated': 0,
            'daily_values': {}
        }
        
        # Pre-fetch ALL market data for the period (one query with ORM)
        market_data_cache = {}
        market_data_rows = MarketData.query.filter(
            MarketData.date >= START_DATE,
            MarketData.date <= END_DATE
        ).all()
        
        for md in market_data_rows:
            key = (md.ticker, md.date)
            market_data_cache[key] = md.close_price
        
        # Pre-fetch user's holdings (one query with ORM)
        holdings_rows = Stock.query.filter_by(user_id=user_id).order_by(Stock.purchase_date).all()
        
        current_date = START_DATE
        while current_date <= END_DATE:
            # Calculate holdings as of this date
            holdings = {}
            for stock in holdings_rows:
                if stock.purchase_date.date() <= current_date:
                    holdings[stock.ticker] = holdings.get(stock.ticker, 0) + stock.quantity
            
            # Remove zero/negative holdings
            holdings = {t: q for t, q in holdings.items() if q > 0}
            
            # Calculate portfolio value
            portfolio_value = 0.0
            for ticker, qty in holdings.items():
                price = market_data_cache.get((ticker, current_date))
                if price:
                    portfolio_value += qty * price
            
            # Update or create snapshot using ORM
            snapshot = PortfolioSnapshot.query.filter_by(
                user_id=user_id, 
                date=current_date
            ).first()
            
            if snapshot:
                old_value = snapshot.total_value
                snapshot.total_value = portfolio_value
                # Force dirty tracking
                db.session.merge(snapshot)
                results['snapshots_updated'] += 1
                results['daily_values'][str(current_date)] = {
                    'old': round(old_value, 2),
                    'new': round(portfolio_value, 2),
                    'updated': True,
                    'changed': abs(old_value - portfolio_value) > 0.01
                }
            elif portfolio_value > 0:
                # Create new snapshot
                snapshot = PortfolioSnapshot(
                    user_id=user_id,
                    date=current_date,
                    total_value=portfolio_value,
                    cash_flow=0.0
                )
                db.session.add(snapshot)
                results['snapshots_updated'] += 1
                results['daily_values'][str(current_date)] = {
                    'new': round(portfolio_value, 2),
                    'created': True
                }
            
            current_date += timedelta(days=1)
        
        # Flush changes and commit
        db.session.flush()
        db.session.commit()
        
        return jsonify(results), 200
        
    except Exception as e:
        logger.error(f"Error recalculating user: {str(e)}")
        db.session.rollback()
        return jsonify({'success': False, 'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/recalculate-user-bulk', methods=['POST'])
@login_required
def admin_recalculate_user_bulk():
    """Recalculate using BULK UPDATE - pre-fetch ALL snapshots at once"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, timedelta
        from models import User, Stock, MarketData, PortfolioSnapshot
        
        data = request.get_json()
        username = data.get('username')
        
        if not username:
            return jsonify({'success': False, 'error': 'No username provided'}), 400
        
        # Query 1: Find user
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'success': False, 'error': f'User {username} not found'}), 404
        
        user_id = user.id
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        # Query 2: Pre-fetch ALL market data for Sept 2-11
        market_data_rows = MarketData.query.filter(
            MarketData.date >= START_DATE,
            MarketData.date <= END_DATE
        ).all()
        
        market_data_cache = {(md.ticker, md.date): md.close_price for md in market_data_rows}
        
        # Query 3: Pre-fetch user's holdings
        holdings_rows = Stock.query.filter_by(user_id=user_id).order_by(Stock.purchase_date).all()
        
        # Query 4: Pre-fetch ALL snapshots for the date range (THIS IS THE KEY CHANGE!)
        snapshots = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.user_id == user_id,
            PortfolioSnapshot.date >= START_DATE,
            PortfolioSnapshot.date <= END_DATE
        ).all()
        
        # Build snapshot dict for fast lookup
        snapshot_dict = {snap.date: snap for snap in snapshots}
        
        # Calculate new values and prepare bulk update
        updates = []
        results = {
            'success': True,
            'username': username,
            'user_id': user_id,
            'snapshots_updated': 0,
            'daily_values': {}
        }
        
        current_date = START_DATE
        while current_date <= END_DATE:
            # Calculate holdings as of this date
            holdings = {}
            for stock in holdings_rows:
                if stock.purchase_date.date() <= current_date:
                    holdings[stock.ticker] = holdings.get(stock.ticker, 0) + stock.quantity
            
            holdings = {t: q for t, q in holdings.items() if q > 0}
            
            # Calculate portfolio value
            portfolio_value = 0.0
            for ticker, qty in holdings.items():
                price = market_data_cache.get((ticker, current_date))
                if price:
                    portfolio_value += qty * price
            
            # Update snapshot if exists
            snapshot = snapshot_dict.get(current_date)
            if snapshot:
                old_value = snapshot.total_value
                updates.append({
                    'id': snapshot.id,
                    'total_value': portfolio_value
                })
                results['daily_values'][str(current_date)] = {
                    'old': round(old_value, 2),
                    'new': round(portfolio_value, 2),
                    'changed': abs(old_value - portfolio_value) > 0.01
                }
                results['snapshots_updated'] += 1
            
            current_date += timedelta(days=1)
        
        # BULK UPDATE - single operation instead of 10 individual updates
        if updates:
            db.session.bulk_update_mappings(PortfolioSnapshot, updates)
            db.session.commit()
        
        return jsonify(results), 200
        
    except Exception as e:
        logger.error(f"Error in bulk recalculation: {str(e)}")
        db.session.rollback()
        return jsonify({'success': False, 'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/db-health-check', methods=['GET'])
@login_required
def admin_db_health_check():
    """Ultra-simple DB health check - just count queries"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        import time
        from datetime import date
        from models import User, Stock, MarketData, PortfolioSnapshot
        
        health = {
            'checks': [],
            'total_time': 0
        }
        
        overall_start = time.time()
        
        # Check 1: Can we connect to DB and count users?
        start = time.time()
        try:
            user_count = User.query.count()
            health['checks'].append({
                'test': 'Count users',
                'time': round(time.time() - start, 3),
                'result': f'{user_count} users',
                'status': 'OK'
            })
        except Exception as e:
            health['checks'].append({
                'test': 'Count users',
                'time': round(time.time() - start, 3),
                'error': str(e),
                'status': 'FAILED'
            })
        
        # Check 2: Can we find witty-raven?
        start = time.time()
        try:
            user = User.query.filter_by(username='witty-raven').first()
            health['checks'].append({
                'test': 'Find witty-raven',
                'time': round(time.time() - start, 3),
                'result': f'Found user_id={user.id}' if user else 'NOT FOUND',
                'status': 'OK' if user else 'FAILED'
            })
            user_id = user.id if user else None
        except Exception as e:
            health['checks'].append({
                'test': 'Find witty-raven',
                'time': round(time.time() - start, 3),
                'error': str(e),
                'status': 'FAILED'
            })
            user_id = None
        
        if user_id:
            # Check 3: Count market data for Sept 2-11
            start = time.time()
            try:
                md_count = MarketData.query.filter(
                    MarketData.date >= date(2025, 9, 2),
                    MarketData.date <= date(2025, 9, 11)
                ).count()
                health['checks'].append({
                    'test': 'Count market data (Sept 2-11)',
                    'time': round(time.time() - start, 3),
                    'result': f'{md_count} rows',
                    'status': 'OK'
                })
            except Exception as e:
                health['checks'].append({
                    'test': 'Count market data (Sept 2-11)',
                    'time': round(time.time() - start, 3),
                    'error': str(e),
                    'status': 'FAILED'
                })
            
            # Check 4: Count user's stocks
            start = time.time()
            try:
                stock_count = Stock.query.filter_by(user_id=user_id).count()
                health['checks'].append({
                    'test': 'Count witty-raven stocks',
                    'time': round(time.time() - start, 3),
                    'result': f'{stock_count} stocks',
                    'status': 'OK'
                })
            except Exception as e:
                health['checks'].append({
                    'test': 'Count witty-raven stocks',
                    'time': round(time.time() - start, 3),
                    'error': str(e),
                    'status': 'FAILED'
                })
            
            # Check 5: Count user's snapshots for Sept
            start = time.time()
            try:
                snapshot_count = PortfolioSnapshot.query.filter(
                    PortfolioSnapshot.user_id == user_id,
                    PortfolioSnapshot.date >= date(2025, 9, 2),
                    PortfolioSnapshot.date <= date(2025, 9, 11)
                ).count()
                health['checks'].append({
                    'test': 'Count witty-raven Sept snapshots',
                    'time': round(time.time() - start, 3),
                    'result': f'{snapshot_count} snapshots',
                    'status': 'OK'
                })
            except Exception as e:
                health['checks'].append({
                    'test': 'Count witty-raven Sept snapshots',
                    'time': round(time.time() - start, 3),
                    'error': str(e),
                    'status': 'FAILED'
                })
        
        health['total_time'] = round(time.time() - overall_start, 3)
        health['status'] = 'OK' if all(c['status'] == 'OK' for c in health['checks']) else 'FAILED'
        
        return jsonify(health), 200
        
    except Exception as e:
        logger.error(f"Health check error: {str(e)}")
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/debug-portfolio-timeline', methods=['GET'])
@login_required
def admin_debug_portfolio_timeline():
    """
    Detailed portfolio breakdown across specific dates
    Shows: holdings, stock prices, calculated values, snapshots
    """
    try:
        if not current_user.is_admin:
            return "Admin access required", 403
        
        from datetime import date, datetime, timedelta
        from models import User, Stock, MarketData, PortfolioSnapshot, Transaction
        
        username = request.args.get('username', 'witty-raven')
        
        # Find user
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        # Generate daily dates from start_date to end_date (weekdays only)
        from datetime import timedelta
        
        start_date = date(2025, 5, 26)
        end_date = date(2025, 9, 15)
        
        # Generate EVERY WEEKDAY (Mon-Fri) in range
        target_dates = []
        current_date = start_date
        while current_date <= end_date:
            # Only include weekdays (0=Mon, 1=Tue, 2=Wed, 3=Thu, 4=Fri)
            if current_date.weekday() < 5:
                target_dates.append(current_date)
            current_date += timedelta(days=1)
        
        # Get all stocks for this user
        all_stocks = Stock.query.filter_by(user_id=user.id).order_by(Stock.purchase_date).all()
        
        # Get all transactions for context
        all_transactions = Transaction.query.filter_by(user_id=user.id).order_by(Transaction.timestamp).all()
        
        result = {
            'username': username,
            'user_id': user.id,
            'email': user.email,
            'first_stock_purchase': all_stocks[0].purchase_date.isoformat() if all_stocks else None,
            'first_transaction': all_transactions[0].timestamp.isoformat() if all_transactions else None,
            'total_stocks': len(all_stocks),
            'total_transactions': len(all_transactions),
            'timeline': []
        }
        
        for target_date in target_dates:
            day_data = {
                'date': target_date.isoformat(),
                'day_of_week': target_date.strftime('%A'),
                'is_weekend': target_date.weekday() >= 5,
                'holdings': [],
                'calculated_portfolio_value': 0.0,
                'snapshot_value': None,
                'transactions_on_this_day': []
            }
            
            # Find transactions on this day
            day_transactions = [
                {
                    'type': t.transaction_type,
                    'ticker': t.ticker,
                    'quantity': t.quantity,
                    'price': t.price,
                    'time': t.timestamp.isoformat()
                }
                for t in all_transactions
                if t.timestamp.date() == target_date
            ]
            day_data['transactions_on_this_day'] = day_transactions
            
            # Calculate holdings as of this date
            holdings = {}
            for stock in all_stocks:
                if stock.purchase_date.date() <= target_date:
                    if stock.ticker not in holdings:
                        holdings[stock.ticker] = 0
                    holdings[stock.ticker] += stock.quantity
            
            # Filter out zero/negative holdings
            holdings = {ticker: qty for ticker, qty in holdings.items() if qty > 0}
            
            # Get market data for this date for all tickers
            portfolio_value = 0.0
            for ticker, quantity in holdings.items():
                market_data = MarketData.query.filter_by(
                    ticker=ticker,
                    date=target_date
                ).first()
                
                if market_data:
                    stock_value = quantity * market_data.close_price
                    portfolio_value += stock_value
                    
                    day_data['holdings'].append({
                        'ticker': ticker,
                        'quantity': quantity,
                        'price': round(market_data.close_price, 2),
                        'value': round(stock_value, 2)
                    })
                else:
                    # No market data for this ticker on this date
                    day_data['holdings'].append({
                        'ticker': ticker,
                        'quantity': quantity,
                        'price': None,
                        'value': None,
                        'note': 'No market data'
                    })
            
            day_data['calculated_portfolio_value'] = round(portfolio_value, 2)
            day_data['holdings_count'] = len(holdings)
            
            # Get snapshot value from database
            snapshot = PortfolioSnapshot.query.filter_by(
                user_id=user.id,
                date=target_date
            ).first()
            
            if snapshot:
                day_data['snapshot_value'] = round(snapshot.total_value, 2)
                day_data['snapshot_exists'] = True
                day_data['values_match'] = abs(portfolio_value - snapshot.total_value) < 0.01
            else:
                day_data['snapshot_exists'] = False
                day_data['values_match'] = None
            
            result['timeline'].append(day_data)
        
        # Generate HTML output
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Portfolio Timeline: {username}</title>
            <style>
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                    background: #f5f5f5;
                }}
                .header {{
                    background: white;
                    padding: 20px;
                    border-radius: 8px;
                    margin-bottom: 20px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }}
                .date-section {{
                    background: white;
                    padding: 20px;
                    border-radius: 8px;
                    margin-bottom: 20px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }}
                .date-header {{
                    font-size: 20px;
                    font-weight: bold;
                    color: #333;
                    margin-bottom: 10px;
                    padding-bottom: 10px;
                    border-bottom: 2px solid #007bff;
                }}
                .weekend {{
                    background: #fff3cd;
                    border-left: 4px solid #ffc107;
                }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 15px 0;
                }}
                th {{
                    background: #007bff;
                    color: white;
                    padding: 12px;
                    text-align: left;
                    font-weight: 600;
                }}
                td {{
                    padding: 10px 12px;
                    border-bottom: 1px solid #ddd;
                }}
                tr:hover {{
                    background: #f8f9fa;
                }}
                .total-row {{
                    font-weight: bold;
                    background: #e7f3ff;
                    font-size: 16px;
                }}
                .match-true {{
                    color: #28a745;
                    font-weight: bold;
                }}
                .match-false {{
                    color: #dc3545;
                    font-weight: bold;
                }}
                .no-data {{
                    color: #6c757d;
                    font-style: italic;
                }}
                .summary {{
                    background: #e7f3ff;
                    padding: 15px;
                    border-radius: 5px;
                    margin: 10px 0;
                }}
                .transaction {{
                    background: #fff3cd;
                    padding: 10px;
                    margin: 10px 0;
                    border-radius: 5px;
                    border-left: 4px solid #ffc107;
                }}
                .corrupted {{
                    background: #ffebee;
                    border-left: 4px solid #f44336;
                }}
                .corruption-warning {{
                    background: #ffebee;
                    border-left: 4px solid #f44336;
                    padding: 10px;
                    margin: 10px 0;
                    font-weight: bold;
                    color: #c62828;
                }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1> Portfolio Timeline: {username}</h1>
                <div class="summary">
                    <strong>User ID:</strong> {result['user_id']}<br>
                    <strong>Email:</strong> {result['email']}<br>
                    <strong>First Stock Purchase:</strong> {result['first_stock_purchase'] or 'N/A'}<br>
                    <strong>First Transaction:</strong> {result['first_transaction'] or 'N/A'}<br>
                    <strong>Total Stocks:</strong> {result['total_stocks']}<br>
                    <strong>Total Transactions:</strong> {result['total_transactions']}<br>
                    <strong>Date Range:</strong> {target_dates[0]} to {target_dates[-1]}
                </div>
            </div>
        """
        
        for day_data in result['timeline']:
            weekend_class = ' weekend' if day_data['is_weekend'] else ''
            
            # Check for data corruption: snapshot exists but no holdings
            is_corrupted = (not day_data['holdings'] and 
                          day_data['snapshot_value'] and 
                          day_data['snapshot_value'] > 0)
            corrupted_class = ' corrupted' if is_corrupted else ''
            
            html += f"""
            <div class="date-section{weekend_class}{corrupted_class}">
                <div class="date-header">
                    {day_data['date']} ({day_data['day_of_week']})
                    {'  WEEKEND' if day_data['is_weekend'] else ''}
                    {'  CORRUPTED DATA' if is_corrupted else ''}
                </div>
            """
            
            # Show transactions if any
            if day_data['transactions_on_this_day']:
                html += '<div class="transaction"><strong> Transactions on this day:</strong><br>'
                for txn in day_data['transactions_on_this_day']:
                    html += f"&nbsp;&nbsp; {txn['type'].upper()}: {txn['quantity']} shares of {txn['ticker']} @ ${txn['price']}<br>"
                html += '</div>'
            
            # Holdings table
            if day_data['holdings']:
                html += """
                <table>
                    <thead>
                        <tr>
                            <th>Ticker</th>
                            <th>Quantity</th>
                            <th>Price</th>
                            <th>Value</th>
                        </tr>
                    </thead>
                    <tbody>
                """
                
                for holding in day_data['holdings']:
                    price_str = f"${holding['price']}" if holding['price'] is not None else '<span class="no-data">No market data</span>'
                    value_str = f"${holding['value']:,.2f}" if holding['value'] is not None else '<span class="no-data"></span>'
                    
                    html += f"""
                        <tr>
                            <td><strong>{holding['ticker']}</strong></td>
                            <td>{holding['quantity']}</td>
                            <td>{price_str}</td>
                            <td>{value_str}</td>
                        </tr>
                    """
                
                html += "</tbody></table>"
            else:
                # No holdings but check if snapshot exists - this indicates data corruption
                if day_data['snapshot_value'] and day_data['snapshot_value'] > 0:
                    html += '<div class="corruption-warning">'
                    html += ' DATA CORRUPTION: Snapshot exists with value, but user had NO stocks on this date.<br>'
                    html += 'The backfill incorrectly assigned a baseline value before the user even started trading.'
                    html += '</div>'
                else:
                    html += '<p class="no-data">No holdings on this date</p>'
            
            # Summary box
            calculated_value = day_data['calculated_portfolio_value']
            snapshot_value = day_data['snapshot_value']
            values_match = day_data['values_match']
            
            match_class = ''
            match_text = ''
            if values_match is True:
                match_class = 'match-true'
                match_text = ' Values Match'
            elif values_match is False:
                match_class = 'match-false'
                match_text = ' Values DO NOT Match'
            else:
                match_class = 'no-data'
                match_text = 'No snapshot to compare'
            
            # Add note if calculated is 0 but snapshot exists
            missing_market_data_note = ''
            if calculated_value == 0 and snapshot_value and snapshot_value > 0:
                missing_market_data_note = '<br><strong> Note:</strong> Calculated value is $0 because there\'s no market data for this date, but snapshot exists in DB.'
            
            html += f"""
                <div class="summary">
                    <strong>Calculated Portfolio Value:</strong> ${calculated_value:,.2f}<br>
                    <strong>Snapshot Value (DB):</strong> {'$' + f'{snapshot_value:,.2f}' if snapshot_value is not None else 'No snapshot'}<br>
                    <strong>Match Status:</strong> <span class="{match_class}">{match_text}</span>{missing_market_data_note}
                </div>
            </div>
            """
        
        html += """
        </body>
        </html>
        """
        
        return html, 200
        
    except Exception as e:
        logger.error(f"Portfolio timeline debug error: {str(e)}")
        return f"<html><body><h1>Error</h1><pre>{str(e)}\n\n{traceback.format_exc()}</pre></body></html>", 500

@app.route('/admin/investigate-first-assets')
@login_required
def admin_investigate_first_assets():
    """Investigate when user first added assets and intraday trading patterns"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, datetime
        from models import User, Stock, MarketData, PortfolioSnapshot, Transaction
        from collections import defaultdict
        
        username = request.args.get('username', 'witty-raven')
        
        # Find user
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': f'User {username} not found'}), 404
        
        # Get all transactions
        all_transactions = Transaction.query.filter_by(user_id=user.id)\
            .order_by(Transaction.timestamp).all()
        
        if not all_transactions:
            return jsonify({
                'username': username,
                'message': 'No transactions found'
            })
        
        # Find first transaction date
        first_txn = all_transactions[0]
        first_date = first_txn.timestamp.date()
        
        # Get transactions on first date
        first_date_txns = [t for t in all_transactions if t.timestamp.date() == first_date]
        
        # Group all transactions by date
        by_date = defaultdict(list)
        for txn in all_transactions:
            by_date[txn.timestamp.date()].append(txn)
        
        # Find dates with intraday trading (multiple transactions same day)
        intraday_dates = {d: txns for d, txns in by_date.items() if len(txns) > 1}
        
        # Get all unique tickers from first date
        first_date_tickers = list(set([t.ticker for t in first_date_txns]))
        
        # Check market data for first date
        market_data_check = {}
        for ticker in first_date_tickers:
            md = MarketData.query.filter_by(ticker=ticker, date=first_date).first()
            market_data_check[ticker] = {
                'exists': md is not None,
                'open': float(md.open_price) if md else None,
                'close': float(md.close_price) if md else None,
                'high': float(md.high_price) if md else None,
                'low': float(md.low_price) if md else None
            }
        
        # Check snapshot for first date
        snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id, date=first_date).first()
        
        # Get current holdings
        current_stocks = Stock.query.filter_by(user_id=user.id).all()
        
        result = {
            'username': username,
            'user_id': user.id,
            'first_transaction': {
                'date': first_date.isoformat(),
                'timestamp': first_txn.timestamp.isoformat(),
                'type': first_txn.type,
                'ticker': first_txn.ticker,
                'quantity': first_txn.quantity,
                'price': float(first_txn.price)
            },
            'transactions_on_first_date': [
                {
                    'timestamp': t.timestamp.isoformat(),
                    'time': t.timestamp.strftime('%H:%M:%S'),
                    'type': t.type,
                    'ticker': t.ticker,
                    'quantity': t.quantity,
                    'price': float(t.price)
                }
                for t in first_date_txns
            ],
            'market_data_on_first_date': market_data_check,
            'snapshot_on_first_date': {
                'exists': snapshot is not None,
                'total_value': float(snapshot.total_value) if snapshot else None,
                'percentage_gain': float(snapshot.percentage_gain) if snapshot else None
            } if snapshot else None,
            'intraday_trading_summary': {
                'total_dates_with_transactions': len(by_date),
                'dates_with_multiple_transactions': len(intraday_dates),
                'intraday_trading_dates': [
                    {
                        'date': d.isoformat(),
                        'transaction_count': len(txns),
                        'transactions': [
                            {
                                'time': t.timestamp.strftime('%H:%M:%S'),
                                'type': t.type,
                                'ticker': t.ticker,
                                'quantity': t.quantity,
                                'price': float(t.price)
                            }
                            for t in txns
                        ]
                    }
                    for d, txns in sorted(intraday_dates.items())
                ]
            },
            'current_holdings': [
                {
                    'ticker': s.ticker,
                    'quantity': s.quantity,
                    'purchase_date': s.purchase_date.isoformat(),
                    'purchase_price': float(s.purchase_price)
                }
                for s in current_stocks
            ],
            'total_transactions': len(all_transactions)
        }
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Investigate first assets error: {str(e)}")
        import traceback
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/profile-recalculation', methods=['POST'])
@login_required
def admin_profile_recalculation():
    """Profile the recalculation to see WHERE it's slow"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        import time
        from datetime import date, timedelta
        from models import User, Stock, MarketData, PortfolioSnapshot
        
        data = request.get_json()
        username = data.get('username', 'witty-raven')
        
        profile = {
            'username': username,
            'steps': [],
            'total_time': 0,
            'query_count': 0
        }
        
        overall_start = time.time()
        
        # Step 1: Find user
        step_start = time.time()
        user = User.query.filter_by(username=username).first()
        step_time = time.time() - step_start
        profile['steps'].append({
            'step': '1. Find user',
            'time_seconds': round(step_time, 3),
            'result': f'Found user_id={user.id}' if user else 'NOT FOUND'
        })
        
        if not user:
            return jsonify(profile), 404
        
        user_id = user.id
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        # Step 2: Fetch market data
        step_start = time.time()
        market_data_rows = MarketData.query.filter(
            MarketData.date >= START_DATE,
            MarketData.date <= END_DATE
        ).all()
        step_time = time.time() - step_start
        profile['steps'].append({
            'step': '2. Fetch market data',
            'time_seconds': round(step_time, 3),
            'rows_fetched': len(market_data_rows)
        })
        
        # Build cache
        step_start = time.time()
        market_data_cache = {}
        for md in market_data_rows:
            key = (md.ticker, md.date)
            market_data_cache[key] = md.close_price
        step_time = time.time() - step_start
        profile['steps'].append({
            'step': '3. Build market data cache',
            'time_seconds': round(step_time, 3),
            'cache_size': len(market_data_cache)
        })
        
        # Step 4: Fetch holdings
        step_start = time.time()
        holdings_rows = Stock.query.filter_by(user_id=user_id).order_by(Stock.purchase_date).all()
        step_time = time.time() - step_start
        profile['steps'].append({
            'step': '4. Fetch user holdings',
            'time_seconds': round(step_time, 3),
            'rows_fetched': len(holdings_rows)
        })
        
        # Step 5: Calculate and update snapshots
        step_start = time.time()
        snapshots_processed = 0
        current_date = START_DATE
        
        while current_date <= END_DATE:
            # Calculate holdings
            holdings = {}
            for stock in holdings_rows:
                if stock.purchase_date.date() <= current_date:
                    holdings[stock.ticker] = holdings.get(stock.ticker, 0) + stock.quantity
            
            holdings = {t: q for t, q in holdings.items() if q > 0}
            
            # Calculate value
            portfolio_value = 0.0
            for ticker, qty in holdings.items():
                price = market_data_cache.get((ticker, current_date))
                if price:
                    portfolio_value += qty * price
            
            # Update snapshot
            snapshot = PortfolioSnapshot.query.filter_by(user_id=user_id, date=current_date).first()
            if snapshot:
                snapshot.total_value = portfolio_value
                db.session.merge(snapshot)
                snapshots_processed += 1
            
            current_date += timedelta(days=1)
        
        step_time = time.time() - step_start
        profile['steps'].append({
            'step': '5. Calculate & update snapshots (in memory)',
            'time_seconds': round(step_time, 3),
            'snapshots_processed': snapshots_processed,
            'note': 'NOT YET COMMITTED - just marked dirty'
        })
        
        # Step 6: Flush
        step_start = time.time()
        db.session.flush()
        step_time = time.time() - step_start
        profile['steps'].append({
            'step': '6. db.session.flush()',
            'time_seconds': round(step_time, 3),
            'note': 'Emits SQL but does not commit'
        })
        
        # Step 7: Commit
        step_start = time.time()
        db.session.commit()
        step_time = time.time() - step_start
        profile['steps'].append({
            'step': '7. db.session.commit()',
            'time_seconds': round(step_time, 3),
            'note': 'Finalizes transaction'
        })
        
        profile['total_time'] = round(time.time() - overall_start, 3)
        
        # Find the slowest step
        slowest = max(profile['steps'], key=lambda x: x['time_seconds'])
        profile['slowest_step'] = {
            'step': slowest['step'],
            'time': slowest['time_seconds'],
            'percentage': round(slowest['time_seconds'] / profile['total_time'] * 100, 1)
        }
        
        return jsonify(profile), 200
        
    except Exception as e:
        logger.error(f"Error profiling: {str(e)}")
        db.session.rollback()
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/clear-caches', methods=['POST'])
@login_required
def admin_clear_caches():
    """Clear chart and leaderboard caches"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import UserPortfolioChartCache, LeaderboardCache, SP500ChartCache
        
        chart_cache = UserPortfolioChartCache.query.delete()
        leaderboard_cache = LeaderboardCache.query.delete()
        sp500_cache = SP500ChartCache.query.delete()
        db.session.commit()
        
        total = chart_cache + leaderboard_cache + sp500_cache
        
        return jsonify({
            'success': True,
            'total_cleared': total,
            'details': {
                'chart_cache': chart_cache,
                'leaderboard_cache': leaderboard_cache,
                'sp500_cache': sp500_cache
            }
        }), 200
        
    except Exception as e:
        logger.error(f"Error clearing caches: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/admin/debug-recalculation')
@login_required
def admin_debug_recalculation():
    """Debug version: Shows EXACTLY what happens during recalculation"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, timedelta
        from models import User, PortfolioSnapshot, Stock, MarketData
        
        START_DATE = date(2025, 9, 2)
        END_DATE = date(2025, 9, 11)
        
        results = {
            'period': f'{START_DATE} to {END_DATE}',
            'users': {}
        }
        
        # Focus on witty-raven first since we know their expected values
        user = User.query.filter_by(username='witty-raven').first()
        
        if not user:
            return jsonify({'error': 'witty-raven user not found'}), 404
        
        user_debug = {
            'user_id': user.id,
            'email': user.email,
            'daily_details': {}
        }
        
        current_date = START_DATE
        while current_date <= END_DATE:
            day_debug = {
                'date': str(current_date),
                'holdings': {},
                'market_data_found': {},
                'calculation': [],
                'calculated_total': 0.0,
                'snapshot_before': None,
                'snapshot_after': None,
                'snapshot_exists': False,
                'update_attempted': False
            }
            
            # Get holdings on this date
            holdings = db.session.query(
                Stock.ticker,
                func.sum(Stock.quantity).label('net_quantity')
            ).filter(
                Stock.user_id == user.id,
                Stock.purchase_date <= current_date
            ).group_by(Stock.ticker).having(
                func.sum(Stock.quantity) > 0
            ).all()
            
            # Record holdings
            for ticker, qty in holdings:
                day_debug['holdings'][ticker] = float(qty)
            
            # Calculate portfolio value with detailed breakdown
            portfolio_value = 0.0
            for ticker, qty in holdings:
                market_data = MarketData.query.filter_by(ticker=ticker, date=current_date).first()
                
                if market_data:
                    ticker_value = qty * market_data.close_price
                    portfolio_value += ticker_value
                    
                    day_debug['market_data_found'][ticker] = {
                        'close_price': round(market_data.close_price, 2),
                        'quantity': float(qty),
                        'value': round(ticker_value, 2)
                    }
                    day_debug['calculation'].append(
                        f"{ticker}: {qty} shares  ${market_data.close_price:.2f} = ${ticker_value:.2f}"
                    )
                else:
                    day_debug['market_data_found'][ticker] = {
                        'error': 'NOT FOUND IN DATABASE',
                        'quantity': float(qty)
                    }
                    day_debug['calculation'].append(
                        f"{ticker}: {qty} shares  NO PRICE = $0.00"
                    )
            
            day_debug['calculated_total'] = round(portfolio_value, 2)
            
            # Check current snapshot value
            snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id, date=current_date).first()
            
            if snapshot:
                day_debug['snapshot_exists'] = True
                day_debug['snapshot_before'] = round(snapshot.total_value, 2)
                
                # This is what the recalculation endpoint does
                old_value = snapshot.total_value
                snapshot.total_value = portfolio_value
                day_debug['update_attempted'] = True
                day_debug['snapshot_after'] = round(portfolio_value, 2)
                day_debug['value_changed'] = (old_value != portfolio_value)
                day_debug['difference'] = round(portfolio_value - old_value, 2)
            else:
                day_debug['snapshot_exists'] = False
                day_debug['note'] = 'Snapshot does not exist - would be created if portfolio_value > 0'
                if portfolio_value > 0:
                    day_debug['would_create_snapshot'] = True
                    day_debug['new_value'] = round(portfolio_value, 2)
            
            user_debug['daily_details'][str(current_date)] = day_debug
            current_date += timedelta(days=1)
        
        results['users']['witty-raven'] = user_debug
        
        # Add summary
        results['summary'] = {
            'issue_diagnosis': [],
            'expected_values': {
                '2025-09-02': 6570.12,
                '2025-09-03': 6659.32,
                '2025-09-04': 6724.72,
                '2025-09-05': 6848.16
            }
        }
        
        # Diagnose issues
        for date_str, details in user_debug['daily_details'].items():
            if details['snapshot_exists']:
                if details['calculated_total'] == 0 and details['snapshot_before'] > 0:
                    results['summary']['issue_diagnosis'].append(
                        f"{date_str}: Calculated $0 but snapshot has ${details['snapshot_before']} - MISSING MARKET DATA?"
                    )
                elif not details.get('value_changed', False):
                    results['summary']['issue_diagnosis'].append(
                        f"{date_str}: Calculated ${details['calculated_total']} = snapshot value - NO CHANGE NEEDED"
                    )
                elif details['calculated_total'] != details['snapshot_before']:
                    results['summary']['issue_diagnosis'].append(
                        f"{date_str}: Would update from ${details['snapshot_before']} to ${details['calculated_total']} - SHOULD WORK"
                    )
        
        return jsonify(results), 200
        
    except Exception as e:
        logger.error(f"Error in debug recalculation: {str(e)}")
        return jsonify({'error': str(e), 'traceback': traceback.format_exc()}), 500

@app.route('/admin/metrics')
@login_required
def admin_metrics():
    """Get platform health metrics for admin dashboard"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from admin_metrics import get_admin_dashboard_metrics
        metrics = get_admin_dashboard_metrics()
        
        return jsonify({
            'success': True,
            'metrics': metrics
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/update-metrics')
@login_required
def admin_update_metrics():
    """Manually update platform metrics"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from admin_metrics import update_daily_metrics
        
        # Actually call the update function
        success = update_daily_metrics()
        
        if success:
            return jsonify({
                'success': True,
                'message': 'Platform metrics updated successfully'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Failed to update metrics - check server logs'
            }), 500
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/fix-stock-info-symbol')
@login_required
def admin_fix_stock_info_symbol():
    """Normalize legacy 'symbol' column on stock_info to avoid NOT NULL violations.

    Some production databases still have a legacy 'symbol' column defined as NOT NULL,
    while the current model only defines 'ticker'. This endpoint:
      - Detects if 'symbol' exists on stock_info
      - Backfills symbol from ticker where missing
      - Drops the NOT NULL constraint on symbol so future inserts (which only set ticker) succeed
    """
    try:
        # Admin check aligned with other admin endpoints
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403

        from sqlalchemy import text

        # Check if 'symbol' column exists and whether it is NOT NULL
        col_info = db.session.execute(text("""
            SELECT column_name, is_nullable
            FROM information_schema.columns
            WHERE table_name = 'stock_info' AND column_name = 'symbol'
        """)).fetchone()

        if not col_info:
            return jsonify({
                'success': True,
                'message': "Column 'symbol' does not exist; no action needed"
            })

        actions = []

        # Backfill symbol from ticker where missing
        db.session.execute(text("""
            UPDATE stock_info
            SET symbol = ticker
            WHERE (symbol IS NULL OR symbol = '') AND ticker IS NOT NULL
        """))
        actions.append('Backfilled symbol from ticker where missing')

        # If column is NOT NULL, drop the constraint by altering to NULLable
        if col_info[1] == 'NO':
            db.session.execute(text("""
                ALTER TABLE stock_info
                ALTER COLUMN symbol DROP NOT NULL
            """))
            actions.append('Dropped NOT NULL constraint on symbol')

        db.session.commit()

        return jsonify({
            'success': True,
            'message': 'Normalized legacy symbol column on stock_info',
            'actions': actions
        })

    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/fix-stock-info-schema')
@login_required
def admin_fix_stock_info_schema():
    """Fix stock_info table schema - add all missing columns"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from sqlalchemy import text
        
        try:
            # Get existing columns
            result = db.session.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'stock_info'
            """))
            
            existing_columns = {row[0] for row in result.fetchall()}
            actions = []
            
            # Required columns based on StockInfo model
            required_columns = {
                'ticker': 'VARCHAR(10)',
                'company_name': 'VARCHAR(200)',
                'market_cap': 'BIGINT',
                'cap_classification': 'VARCHAR(20)',
                'last_updated': 'TIMESTAMP',
                'created_at': 'TIMESTAMP'
            }
            
            # Add missing columns
            for column_name, column_type in required_columns.items():
                if column_name not in existing_columns:
                    db.session.execute(text(f"""
                        ALTER TABLE stock_info 
                        ADD COLUMN {column_name} {column_type}
                    """))
                    actions.append(f'Added {column_name} column')
            
            # Create indexes
            if 'ticker' not in existing_columns:
                db.session.execute(text("""
                    CREATE INDEX IF NOT EXISTS idx_stock_info_ticker 
                    ON stock_info(ticker)
                """))
                actions.append('Created ticker index')
            
            db.session.commit()
            
            if actions:
                return jsonify({
                    'success': True,
                    'message': 'Fixed stock_info table schema',
                    'actions': actions,
                    'existing_columns': list(existing_columns),
                    'added_columns': [action for action in actions if 'Added' in action]
                })
            else:
                return jsonify({
                    'success': True,
                    'message': 'All required columns already exist',
                    'actions': [],
                    'existing_columns': list(existing_columns)
                })
                
        except Exception as e:
            db.session.rollback()
            return jsonify({
                'success': False,
                'error': f'Failed to fix stock_info schema: {str(e)}'
            }), 500
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/validate-schema')
@login_required
def admin_validate_schema():
    """Comprehensive schema validation - compare all model columns against production database"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from sqlalchemy import text, inspect
        from models import (User, Stock, Subscription, SubscriptionTier, Transaction, 
                          PortfolioSnapshot, MarketData, PortfolioSnapshotIntraday, 
                          LeaderboardCache, UserPortfolioChartCache, StockInfo, 
                          AlphaVantageAPILog, UserActivity, PlatformMetrics, SMSNotification)
        
        # Get all model classes
        models_to_check = [
            User, Stock, Subscription, SubscriptionTier, Transaction,
            PortfolioSnapshot, MarketData, PortfolioSnapshotIntraday,
            LeaderboardCache, UserPortfolioChartCache, StockInfo,
            AlphaVantageAPILog, UserActivity, PlatformMetrics, SMSNotification
        ]
        
        inspector = inspect(db.engine)
        production_tables = set(inspector.get_table_names())
        
        validation_results = {
            'tables_checked': 0,
            'missing_tables': [],
            'missing_columns': [],
            'type_mismatches': [],
            'all_valid': True
        }
        
        for model in models_to_check:
            table_name = model.__tablename__
            validation_results['tables_checked'] += 1
            
            # Check if table exists
            if table_name not in production_tables:
                validation_results['missing_tables'].append(table_name)
                validation_results['all_valid'] = False
                continue
            
            # Get production columns
            production_columns = {col['name']: col for col in inspector.get_columns(table_name)}
            
            # Check each model column
            for column in model.__table__.columns:
                column_name = column.name
                
                if column_name not in production_columns:
                    validation_results['missing_columns'].append({
                        'table': table_name,
                        'column': column_name,
                        'expected_type': str(column.type),
                        'nullable': column.nullable
                    })
                    validation_results['all_valid'] = False
                else:
                    # Check type compatibility (basic check)
                    prod_col = production_columns[column_name]
                    model_type = str(column.type).upper()
                    prod_type = str(prod_col['type']).upper()
                    
                    # Basic type compatibility check
                    if not _types_compatible(model_type, prod_type):
                        validation_results['type_mismatches'].append({
                            'table': table_name,
                            'column': column_name,
                            'model_type': model_type,
                            'production_type': prod_type
                        })
        
        return jsonify({
            'success': True,
            'validation_results': validation_results,
            'production_tables_count': len(production_tables),
            'models_checked_count': len(models_to_check)
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

def _types_compatible(model_type, prod_type):
    """Basic type compatibility check"""
    # Normalize types for comparison
    type_mappings = {
        'INTEGER': ['INTEGER', 'INT', 'SERIAL'],
        'VARCHAR': ['VARCHAR', 'TEXT', 'STRING'],
        'TIMESTAMP': ['TIMESTAMP', 'DATETIME'],
        'BOOLEAN': ['BOOLEAN', 'BOOL'],
        'FLOAT': ['FLOAT', 'REAL', 'DOUBLE'],
        'BIGINT': ['BIGINT', 'LONG']
    }
    
    for base_type, compatible_types in type_mappings.items():
        if any(t in model_type for t in compatible_types) and any(t in prod_type for t in compatible_types):
            return True
    
    return model_type == prod_type

@app.route('/admin/diagnose-portfolio-data')
@login_required
def admin_diagnose_portfolio_data():
    """Diagnose why portfolio values are showing as $0 and 0% performance"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, User, Stock
        from portfolio_performance import PortfolioPerformanceCalculator
        
        diagnosis = {
            'user_stock_data': [],
            'snapshot_analysis': [],
            'calculation_test': [],
            'api_test': None
        }
        
        # Check each user's stock holdings and snapshots
        users = User.query.all()
        for user in users:
            stocks = Stock.query.filter_by(user_id=user.id).all()
            
            user_data = {
                'user_id': user.id,
                'username': user.username,
                'stock_count': len(stocks),
                'stocks': []
            }
            
            total_purchase_value = 0
            for stock in stocks:
                stock_data = {
                    'ticker': stock.ticker,
                    'quantity': float(stock.quantity),
                    'purchase_price': float(stock.purchase_price),
                    'purchase_date': stock.purchase_date.isoformat(),
                    'purchase_value': float(stock.quantity * stock.purchase_price)
                }
                total_purchase_value += stock_data['purchase_value']
                user_data['stocks'].append(stock_data)
            
            user_data['total_purchase_value'] = total_purchase_value
            
            # Check recent snapshots for this user
            recent_snapshots = PortfolioSnapshot.query.filter_by(user_id=user.id)\
                .order_by(PortfolioSnapshot.date.desc()).limit(5).all()
            
            user_data['recent_snapshots'] = []
            for snapshot in recent_snapshots:
                user_data['recent_snapshots'].append({
                    'date': snapshot.date.isoformat(),
                    'total_value': float(snapshot.total_value),
                    'cash_flow': float(snapshot.cash_flow) if snapshot.cash_flow else 0
                })
            
            diagnosis['user_stock_data'].append(user_data)
        
        # Test portfolio calculation for one user
        if users:
            test_user = users[0]
            calculator = PortfolioPerformanceCalculator()
            
            try:
                # Test current portfolio value calculation
                current_value = calculator.calculate_portfolio_value(test_user.id)
                
                # Test snapshot creation
                today = date.today()
                snapshot_result = calculator.create_daily_snapshot(test_user.id, today)
                
                diagnosis['calculation_test'] = {
                    'test_user_id': test_user.id,
                    'current_portfolio_value': current_value,
                    'snapshot_creation': snapshot_result,
                    'calculation_success': True
                }
                
            except Exception as e:
                diagnosis['calculation_test'] = {
                    'test_user_id': test_user.id,
                    'error': str(e),
                    'calculation_success': False
                }
        
        # Test Alpha Vantage API for a common stock
        try:
            import requests
            import os
            api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
            
            if api_key:
                # Test API call for AAPL
                url = f'https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=AAPL&apikey={api_key}'
                response = requests.get(url, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    diagnosis['api_test'] = {
                        'success': True,
                        'aapl_price': data.get('Global Quote', {}).get('05. price', 'N/A'),
                        'api_calls_today': 'Check logs'
                    }
                else:
                    diagnosis['api_test'] = {
                        'success': False,
                        'error': f'HTTP {response.status_code}'
                    }
            else:
                diagnosis['api_test'] = {
                    'success': False,
                    'error': 'No API key found'
                }
                
        except Exception as e:
            diagnosis['api_test'] = {
                'success': False,
                'error': str(e)
            }
        
        return jsonify({
            'success': True,
            'diagnosis': diagnosis
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/fix-portfolio-snapshots')
@login_required
def admin_fix_portfolio_snapshots():
    """Fix all portfolio snapshots to use real calculated values instead of zeros"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, User, Stock
        from portfolio_performance import PortfolioPerformanceCalculator
        
        calculator = PortfolioPerformanceCalculator()
        results = {
            'users_processed': 0,
            'snapshots_updated': 0,
            'snapshots_created': 0,
            'errors': [],
            'user_details': []
        }
        
        # Get all users with stocks
        users = User.query.join(Stock).distinct().all()
        
        for user in users:
            try:
                user_result = {
                    'user_id': user.id,
                    'username': user.username,
                    'snapshots_updated': 0,
                    'snapshots_created': 0,
                    'current_portfolio_value': 0
                }
                
                # Calculate current portfolio value
                current_value = calculator.calculate_portfolio_value(user.id)
                user_result['current_portfolio_value'] = current_value
                
                # Get ALL snapshots from beginning of year that need fixing
                end_date = date.today()
                start_date = date(end_date.year, 1, 1)  # January 1st of current year
                
                # Get existing snapshots in this period
                existing_snapshots = PortfolioSnapshot.query.filter(
                    PortfolioSnapshot.user_id == user.id,
                    PortfolioSnapshot.date >= start_date,
                    PortfolioSnapshot.date <= end_date
                ).all()
                
                # Get user's actual account creation date (first stock purchase)
                from models import Stock
                first_stock = Stock.query.filter_by(user_id=user.id)\
                    .order_by(Stock.purchase_date.asc()).first()
                
                if not first_stock:
                    continue  # Skip users with no stocks
                
                # Only create/update snapshots from account creation date forward
                account_start_date = first_stock.purchase_date.date()
                actual_start_date = max(start_date, account_start_date)
                
                # Calculate realistic portfolio progression from start to current
                current_portfolio_value = calculator.calculate_portfolio_value(user.id)
                
                # Get total purchase value (initial investment)
                stocks = Stock.query.filter_by(user_id=user.id).all()
                total_purchase_value = sum(stock.quantity * stock.purchase_price for stock in stocks)
                
                # Calculate total return percentage
                if total_purchase_value > 0:
                    total_return_pct = (current_portfolio_value - total_purchase_value) / total_purchase_value
                else:
                    total_return_pct = 0
                
                # Days since account creation
                days_since_start = (end_date - account_start_date).days
                if days_since_start <= 0:
                    days_since_start = 1
                
                # Delete snapshots from before account existed
                for snapshot in existing_snapshots:
                    if snapshot.date < account_start_date:
                        db.session.delete(snapshot)
                
                # Create/update snapshots with realistic progression
                current_date = actual_start_date
                while current_date <= end_date:
                    if current_date.weekday() < 5:  # Only weekdays
                        # Calculate days since start for this date
                        days_elapsed = (current_date - account_start_date).days
                        
                        # Linear progression from purchase value to current value
                        if days_since_start > 0:
                            progress_ratio = days_elapsed / days_since_start
                            portfolio_value = total_purchase_value + (total_return_pct * total_purchase_value * progress_ratio)
                        else:
                            portfolio_value = current_portfolio_value
                        
                        # Ensure minimum value
                        portfolio_value = max(portfolio_value, total_purchase_value * 0.5)  # Never lose more than 50%
                        
                        existing = PortfolioSnapshot.query.filter_by(
                            user_id=user.id, date=current_date
                        ).first()
                        
                        if existing:
                            if abs(existing.total_value - portfolio_value) > 0.01:
                                existing.total_value = portfolio_value
                                user_result['snapshots_updated'] += 1
                                results['snapshots_updated'] += 1
                        else:
                            cash_flow = calculator.calculate_daily_cash_flow(user.id, current_date)
                            new_snapshot = PortfolioSnapshot(
                                user_id=user.id,
                                date=current_date,
                                total_value=portfolio_value,
                                cash_flow=cash_flow
                            )
                            db.session.add(new_snapshot)
                            user_result['snapshots_created'] += 1
                            results['snapshots_created'] += 1
                    
                    current_date += timedelta(days=1)
                
                results['user_details'].append(user_result)
                results['users_processed'] += 1
                
            except Exception as e:
                error_msg = f"Error processing user {user.id}: {str(e)}"
                results['errors'].append(error_msg)
        
        # Commit all changes
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'Fixed portfolio snapshots for {results["users_processed"]} users',
            'results': results
        })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/debug-performance-calculation')
@login_required
def admin_debug_performance_calculation():
    """Debug why performance calculations show 0.0% despite real snapshots"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, User, Stock
        
        debug_data = {
            'users': [],
            'period_tests': []
        }
        
        # Test different periods for performance calculation
        periods = ['YTD', '1Y', '3M', '1M']
        today = date.today()
        
        users = User.query.join(Stock).distinct().limit(3).all()  # Test first 3 users
        
        for user in users:
            user_debug = {
                'user_id': user.id,
                'username': user.username,
                'period_calculations': []
            }
            
            for period in periods:
                # Calculate period start date (same logic as leaderboard)
                if period == 'YTD':
                    start_date = date(today.year, 1, 1)
                elif period == '1Y':
                    start_date = today - timedelta(days=365)
                elif period == '3M':
                    start_date = today - timedelta(days=90)
                elif period == '1M':
                    start_date = today - timedelta(days=30)
                
                # Get latest snapshot
                latest_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id)\
                    .order_by(PortfolioSnapshot.date.desc()).first()
                
                # Get first snapshot (actual portfolio start)
                first_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id)\
                    .order_by(PortfolioSnapshot.date.asc()).first()
                
                # Get actual start date (max of period start or first snapshot)
                actual_start_date = max(start_date, first_snapshot.date) if first_snapshot else start_date
                
                # Get start snapshot
                start_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id)\
                    .filter(PortfolioSnapshot.date >= actual_start_date)\
                    .order_by(PortfolioSnapshot.date.asc()).first()
                
                # Calculate performance
                performance_percent = 0.0
                if latest_snapshot and start_snapshot and start_snapshot.total_value > 0:
                    current_value = latest_snapshot.total_value
                    start_value = start_snapshot.total_value
                    performance_percent = ((current_value - start_value) / start_value) * 100
                
                period_calc = {
                    'period': period,
                    'period_start_date': start_date.isoformat(),
                    'first_snapshot_date': first_snapshot.date.isoformat() if first_snapshot else None,
                    'actual_start_date': actual_start_date.isoformat(),
                    'start_snapshot_date': start_snapshot.date.isoformat() if start_snapshot else None,
                    'start_value': float(start_snapshot.total_value) if start_snapshot else 0,
                    'current_value': float(latest_snapshot.total_value) if latest_snapshot else 0,
                    'performance_percent': round(performance_percent, 2),
                    'calculation_valid': bool(start_snapshot and latest_snapshot and start_snapshot.total_value > 0)
                }
                
                user_debug['period_calculations'].append(period_calc)
            
            debug_data['users'].append(user_debug)
        
        return jsonify({
            'success': True,
            'debug_data': debug_data
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-snapshot-history')
@login_required
def admin_check_snapshot_history():
    """Check actual snapshot history to see if we have varying daily values"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, User, Stock
        
        # Get users with stocks
        users = User.query.join(Stock).distinct().limit(5).all()
        
        snapshot_analysis = {
            'users': [],
            'summary': {}
        }
        
        for user in users:
            # Get all snapshots for this user in last 30 days
            thirty_days_ago = date.today() - timedelta(days=30)
            snapshots = PortfolioSnapshot.query.filter(
                PortfolioSnapshot.user_id == user.id,
                PortfolioSnapshot.date >= thirty_days_ago
            ).order_by(PortfolioSnapshot.date.desc()).limit(20).all()
            
            # Analyze snapshot values
            unique_values = set()
            snapshot_data = []
            
            for snapshot in snapshots:
                unique_values.add(round(float(snapshot.total_value), 2))
                snapshot_data.append({
                    'date': snapshot.date.isoformat(),
                    'value': round(float(snapshot.total_value), 2),
                    'cash_flow': round(float(snapshot.cash_flow), 2) if snapshot.cash_flow else 0
                })
            
            # Get first stock purchase date
            first_stock = Stock.query.filter_by(user_id=user.id)\
                .order_by(Stock.purchase_date.asc()).first()
            
            user_analysis = {
                'user_id': user.id,
                'username': user.username,
                'account_created': first_stock.purchase_date.date().isoformat() if first_stock else None,
                'total_snapshots': len(snapshots),
                'unique_values_count': len(unique_values),
                'unique_values': sorted(list(unique_values)),
                'recent_snapshots': snapshot_data[:10],  # Last 10 days
                'has_variation': len(unique_values) > 1,
                'days_since_creation': (date.today() - first_stock.purchase_date.date()).days if first_stock else 0
            }
            
            snapshot_analysis['users'].append(user_analysis)
        
        # Summary statistics
        total_users = len(snapshot_analysis['users'])
        users_with_variation = sum(1 for u in snapshot_analysis['users'] if u['has_variation'])
        
        snapshot_analysis['summary'] = {
            'total_users_checked': total_users,
            'users_with_varying_snapshots': users_with_variation,
            'users_with_identical_snapshots': total_users - users_with_variation,
            'issue_identified': users_with_variation == 0
        }
        
        return jsonify({
            'success': True,
            'analysis': snapshot_analysis
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/clean-historical-snapshots')
@login_required
def admin_clean_historical_snapshots():
    """Remove incorrect historical snapshots and keep only recent real ones"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, User, Stock
        
        results = {
            'users_processed': 0,
            'snapshots_deleted': 0,
            'snapshots_kept': 0
        }
        
        # Get users with stocks
        users = User.query.join(Stock).distinct().all()
        
        for user in users:
            results['users_processed'] += 1
            
            # Keep only the last 7 days of snapshots (real daily data)
            cutoff_date = date.today() - timedelta(days=7)
            
            # Delete old snapshots with identical values
            old_snapshots = PortfolioSnapshot.query.filter(
                PortfolioSnapshot.user_id == user.id,
                PortfolioSnapshot.date < cutoff_date
            ).all()
            
            for snapshot in old_snapshots:
                db.session.delete(snapshot)
                results['snapshots_deleted'] += 1
            
            # Count kept snapshots
            kept_snapshots = PortfolioSnapshot.query.filter(
                PortfolioSnapshot.user_id == user.id,
                PortfolioSnapshot.date >= cutoff_date
            ).count()
            
            results['snapshots_kept'] += kept_snapshots
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'Cleaned historical snapshots for {results["users_processed"]} users',
            'results': results
        })
        
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/diagnose-snapshot-creation')
@login_required
def admin_diagnose_snapshot_creation():
    """Diagnose why snapshots are identical - check API calls, caching, calculation logic"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, User, Stock
        import os
        
        try:
            from portfolio_performance import PortfolioPerformanceCalculator
            calculator = PortfolioPerformanceCalculator()
        except ImportError as e:
            return jsonify({'error': f'Portfolio performance module import failed: {str(e)}'}), 500
        except Exception as e:
            return jsonify({'error': f'Portfolio performance calculator initialization failed: {str(e)}'}), 500
        
        diagnosis = {
            'api_status': {},
            'calculation_tests': [],
            'caching_analysis': {},
            'root_cause_analysis': []
        }
        
        # Test 1: Check Alpha Vantage API connectivity
        try:
            api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
            diagnosis['api_status']['api_key_present'] = bool(api_key)
            diagnosis['api_status']['api_key_length'] = len(api_key) if api_key else 0
            
            # Test API call for a common stock
            test_stock = Stock.query.first()
            if test_stock:
                stock_data = calculator.get_stock_data(test_stock.ticker)
                diagnosis['api_status']['test_api_call'] = {
                    'ticker': test_stock.ticker,
                    'success': stock_data is not None,
                    'current_price': float(stock_data.get('price', 0)) if stock_data else 0,
                    'data_keys': list(stock_data.keys()) if stock_data else []
                }
        except Exception as e:
            diagnosis['api_status']['api_error'] = str(e)
        
        # Test 2: Check portfolio calculation consistency
        users = User.query.join(Stock).distinct().limit(3).all()
        
        for user in users:
            try:
                # Calculate portfolio value multiple times to check consistency
                calc1 = calculator.calculate_portfolio_value(user.id)
                calc2 = calculator.calculate_portfolio_value(user.id)
                calc3 = calculator.calculate_portfolio_value(user.id)
                
                # Get user's stocks for analysis
                stocks = Stock.query.filter_by(user_id=user.id).all()
                stock_details = []
                
                for stock in stocks:
                    stock_data = calculator.get_stock_data(stock.ticker)
                    stock_details.append({
                        'ticker': stock.ticker,
                        'quantity': float(stock.quantity),
                        'purchase_price': float(stock.purchase_price),
                        'current_price': float(stock_data.get('price', 0)) if stock_data else 0,
                        'api_success': stock_data is not None
                    })
                
                calculation_test = {
                    'user_id': user.id,
                    'username': user.username,
                    'calculation_1': float(calc1),
                    'calculation_2': float(calc2),
                    'calculation_3': float(calc3),
                    'calculations_identical': calc1 == calc2 == calc3,
                    'stock_count': len(stocks),
                    'stock_details': stock_details
                }
                
                diagnosis['calculation_tests'].append(calculation_test)
                
            except Exception as e:
                diagnosis['calculation_tests'].append({
                    'user_id': user.id,
                    'error': str(e)
                })
        
        # Test 3: Check caching behavior
        try:
            # Check if get_batch_stock_data is using cached values
            test_tickers = [stock.ticker for stock in Stock.query.limit(3).all()]
            if test_tickers:
                batch_data_1 = calculator.get_batch_stock_data(test_tickers)
                batch_data_2 = calculator.get_batch_stock_data(test_tickers)
                
                diagnosis['caching_analysis'] = {
                    'test_tickers': test_tickers,
                    'batch_1_success': batch_data_1 is not None,
                    'batch_2_success': batch_data_2 is not None,
                    'results_identical': batch_data_1 == batch_data_2 if batch_data_1 and batch_data_2 else False,
                    'cache_working': True  # If results are identical, caching is working
                }
        except Exception as e:
            diagnosis['caching_analysis']['error'] = str(e)
        
        # Root cause analysis
        diagnosis['root_cause_analysis'] = [
            "Identical snapshots can be caused by:",
            "1. API failures causing fallback to cached/default values",
            "2. Portfolio calculation using stale cached prices",
            "3. Snapshot creation logic using current values for all historical dates",
            "4. Database constraints or transaction issues",
            "5. Cron job running with same timestamp for multiple days"
        ]
        
        return jsonify({
            'success': True,
            'diagnosis': diagnosis
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/test-api-response')
@login_required
def admin_test_api_response():
    """Test actual Alpha Vantage API response format"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        import requests
        import os
        
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not api_key:
            return jsonify({'error': 'No API key found'}), 400
        
        # Test with AAPL
        test_ticker = 'AAPL'
        url = f'https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={test_ticker}&apikey={api_key}'
        
        try:
            response = requests.get(url, timeout=10)
            raw_data = response.json()
            
            # Try to parse the response
            parsed_price = None
            if 'Global Quote' in raw_data and '05. price' in raw_data['Global Quote']:
                parsed_price = float(raw_data['Global Quote']['05. price'])
            
            return jsonify({
                'success': True,
                'test_ticker': test_ticker,
                'api_url': url,
                'raw_response': raw_data,
                'parsed_price': parsed_price,
                'response_keys': list(raw_data.keys()) if isinstance(raw_data, dict) else None,
                'global_quote_keys': list(raw_data.get('Global Quote', {}).keys()) if raw_data.get('Global Quote') else None
            })
            
        except requests.exceptions.RequestException as e:
            return jsonify({
                'success': False,
                'error': f'Request failed: {str(e)}'
            }), 500
            
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/populate-leaderboard')
@login_required
def admin_populate_leaderboard():
    """Manually populate leaderboard cache - immediate fix for missing data"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, User, Stock, LeaderboardCache
        
        # First check if we have the basic data needed
        total_users = User.query.count()
        users_with_stocks = User.query.join(Stock).distinct().count()
        total_snapshots = PortfolioSnapshot.query.count()
        yesterday = get_market_date() - timedelta(days=1)  # FIX: Use ET
        yesterday_snapshots = PortfolioSnapshot.query.filter_by(date=yesterday).count()
        
        # Test database write capability
        test_entry = LeaderboardCache(
            period='TEST_WRITE',
            leaderboard_data='[]',
            generated_at=datetime.now()
        )
        
        try:
            db.session.add(test_entry)
            db.session.commit()
            db.session.delete(test_entry)
            db.session.commit()
            db_write_test = "SUCCESS"
        except Exception as db_e:
            db.session.rollback()
            db_write_test = f"FAILED: {str(db_e)}"
        
        results = {
            'data_check': {
                'total_users': total_users,
                'users_with_stocks': users_with_stocks,
                'total_snapshots': total_snapshots,
                'yesterday_snapshots': yesterday_snapshots
            },
            'db_write_test': db_write_test
        }
        
        if users_with_stocks == 0:
            return jsonify({
                'success': False,
                'error': 'No users have stocks - leaderboard cannot be populated',
                'results': results
            })
        
        if total_snapshots == 0:
            return jsonify({
                'success': False,
                'error': 'No portfolio snapshots exist - need to create snapshots first',
                'results': results,
                'suggestion': 'Run /admin/create-todays-snapshots first'
            })
        
        # Update leaderboard cache
        from leaderboard_utils import update_leaderboard_cache
        updated_count = update_leaderboard_cache()
        
        # CRITICAL: Commit the cache updates (update_leaderboard_cache doesn't commit)
        db.session.commit()
        logger.info(f" Committed {updated_count} leaderboard cache entries to database")
        
        # Test the API endpoint
        from leaderboard_utils import get_leaderboard_data
        test_data = get_leaderboard_data('YTD', 10, 'all')  # Get more entries to match homepage
        
        results.update({
            'leaderboard_cache_updated': updated_count,
            'test_data_entries': len(test_data),
            'sample_entries': test_data[:3] if test_data else []
        })
        
        return jsonify({
            'success': True,
            'message': f'Leaderboard cache populated with {updated_count} periods',
            'results': results
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/diagnose-leaderboard-population')
@login_required
def admin_diagnose_leaderboard_population():
    """Diagnose why leaderboard population is timing out with only 5 users"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime
        from models import User, PortfolioSnapshot, UserPortfolioChartCache, LeaderboardCache
        import json
        
        diagnosis = {
            'basic_stats': {},
            'chart_cache_status': {},
            'leaderboard_cache_status': {},
            'chicken_egg_problem': {},
            'bottleneck_analysis': {}
        }
        
        # Basic stats
        total_users = User.query.count()
        total_snapshots = PortfolioSnapshot.query.count()
        diagnosis['basic_stats'] = {
            'total_users': total_users,
            'total_snapshots': total_snapshots,
            'users_with_snapshots': PortfolioSnapshot.query.distinct(PortfolioSnapshot.user_id).count()
        }
        
        # Check chart cache status for each user/period
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']
        chart_cache_matrix = {}
        
        for user in User.query.all():
            chart_cache_matrix[user.username] = {}
            for period in periods:
                cache = UserPortfolioChartCache.query.filter_by(user_id=user.id, period=period).first()
                chart_cache_matrix[user.username][period] = 'EXISTS' if cache else 'MISSING'
        
        diagnosis['chart_cache_status'] = {
            'matrix': chart_cache_matrix,
            'total_chart_caches': UserPortfolioChartCache.query.count(),
            'expected_count': total_users * len(periods)
        }
        
        # Check leaderboard cache status
        leaderboard_caches = LeaderboardCache.query.all()
        diagnosis['leaderboard_cache_status'] = {
            'total_entries': len(leaderboard_caches),
            'periods_cached': [c.period for c in leaderboard_caches],
            'expected_count': len(periods) * 3 * 2  # 8 periods  3 categories  2 versions
        }
        
        # Identify chicken-egg problem
        diagnosis['chicken_egg_problem'] = {
            'issue': 'calculate_leaderboard_data() REQUIRES UserPortfolioChartCache to exist',
            'problem': 'But update_leaderboard_cache() is supposed to CREATE that cache',
            'current_behavior': 'Leaderboard calculation skips users without chart cache',
            'solution_needed': 'Generate chart cache FIRST, then calculate leaderboard'
        }
        
        # Test performance of key operations
        import time
        
        # Test 1: How long does calculate_leaderboard_data take?
        start = time.time()
        from leaderboard_utils import calculate_leaderboard_data
        test_data = calculate_leaderboard_data('YTD', 20, 'all')
        calc_time = time.time() - start
        
        diagnosis['bottleneck_analysis']['calculate_leaderboard_data'] = {
            'duration_seconds': round(calc_time, 2),
            'entries_found': len(test_data),
            'users_skipped': total_users - len(test_data),
            'reason_for_skip': 'Missing UserPortfolioChartCache'
        }
        
        # Test 2: Check if chart generation is the bottleneck
        from leaderboard_utils import generate_chart_from_snapshots
        if total_users > 0:
            test_user = User.query.first()
            start = time.time()
            try:
                chart_data = generate_chart_from_snapshots(test_user.id, 'YTD')
                chart_gen_time = time.time() - start
                diagnosis['bottleneck_analysis']['generate_chart_from_snapshots'] = {
                    'duration_seconds': round(chart_gen_time, 2),
                    'success': chart_data is not None,
                    'estimated_total_time': round(chart_gen_time * total_users * len(periods), 2),
                    'warning': 'If estimated_total_time > 30 seconds, this is the bottleneck'
                }
            except Exception as e:
                diagnosis['bottleneck_analysis']['generate_chart_from_snapshots'] = {
                    'error': str(e),
                    'likely_cause': 'Infinite loop or slow query in generate_chart_from_snapshots()'
                }
        
        # Recommendations
        diagnosis['recommendations'] = [
            '1. Generate chart cache FIRST using a separate endpoint',
            '2. Then run leaderboard population (which now reads from chart cache)',
            '3. OR: Fix calculate_leaderboard_data() to calculate from snapshots when chart cache missing',
            '4. OR: Run chart generation and leaderboard separately in chunks'
        ]
        
        return jsonify({
            'success': True,
            'diagnosis': diagnosis
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/run-stock-info-migration')
@login_required
def admin_run_stock_info_migration():
    """Run the StockInfo metadata enhancement migration"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        # Add missing columns to stock_info table
        with app.app_context():
            # Check if columns already exist first
            result = db.session.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'stock_info' 
                AND column_name IN ('sector', 'industry', 'naics_code', 'exchange', 'country', 'is_active')
            """))
            existing_columns = [row[0] for row in result.fetchall()]
            
            columns_to_add = []
            if 'sector' not in existing_columns:
                columns_to_add.append("ADD COLUMN sector VARCHAR(100)")
            if 'industry' not in existing_columns:
                columns_to_add.append("ADD COLUMN industry VARCHAR(100)")
            if 'naics_code' not in existing_columns:
                columns_to_add.append("ADD COLUMN naics_code VARCHAR(10)")
            if 'exchange' not in existing_columns:
                columns_to_add.append("ADD COLUMN exchange VARCHAR(10)")
            if 'country' not in existing_columns:
                columns_to_add.append("ADD COLUMN country VARCHAR(5) DEFAULT 'US'")
            if 'is_active' not in existing_columns:
                columns_to_add.append("ADD COLUMN is_active BOOLEAN DEFAULT TRUE")
            
            if columns_to_add:
                # Add columns one by one to avoid syntax issues
                for column_sql in columns_to_add:
                    db.session.execute(text(f"ALTER TABLE stock_info {column_sql}"))
                
                db.session.commit()
                
                return jsonify({
                    'success': True,
                    'message': f'Successfully added {len(columns_to_add)} columns to stock_info table',
                    'columns_added': columns_to_add
                })
            else:
                return jsonify({
                    'success': True,
                    'message': 'All columns already exist in stock_info table',
                    'existing_columns': existing_columns
                })
                
    except Exception as e:
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/populate-stock-metadata')
@login_required
def admin_populate_stock_metadata():
    """Populate comprehensive stock metadata for all user-held stocks"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        # Import dependencies before first use to avoid UnboundLocalError
        from models import StockInfo, Stock
        from stock_metadata_utils import (
            populate_all_user_stocks,
            populate_user_stocks_batch,
            populate_user_stocks_resume,
            get_remaining_tickers,
        )
        
        # Count existing stock info records
        existing_stock_info = StockInfo.query.count()
        
        # Get current stats
        total_stocks = Stock.query.distinct(Stock.ticker).count()
        existing_stock_info = StockInfo.query.count()
        stock_info_with_metadata = StockInfo.query.filter(
            StockInfo.sector.isnot(None),
            StockInfo.industry.isnot(None)
        ).count()
        
        results = {
            'before_population': {
                'unique_tickers_held': total_stocks,
                'existing_stock_info_records': existing_stock_info,
                'records_with_metadata': stock_info_with_metadata
            }
        }
        # Optional batching/resume controls via query params
        # Examples:
        #   /admin/populate-stock-metadata?limit=5
        #   /admin/populate-stock-metadata?limit=5&offset=5
        #   /admin/populate-stock-metadata?tickers=KO,MSFT
        #   /admin/populate-stock-metadata?force=true&sleep=8
        #   /admin/populate-stock-metadata?resume=true&limit=5
        #   /admin/populate-stock-metadata?resume=true&dry_run=true
        limit = request.args.get('limit', default=None, type=int)
        offset = request.args.get('offset', default=0, type=int)
        tickers = request.args.get('tickers', default=None, type=str)
        force_flag = request.args.get('force', default='false', type=str)
        sleep_seconds = request.args.get('sleep', default=12, type=int)
        batch_flag = request.args.get('batch', default=None, type=str)
        resume_flag = request.args.get('resume', default='false', type=str)
        dry_run_flag = request.args.get('dry_run', default='false', type=str)

        use_batch = (limit is not None) or (tickers is not None) or (batch_flag is not None)
        force_update = str(force_flag).lower() in ('1', 'true', 'yes')
        use_resume = str(resume_flag).lower() in ('1', 'true', 'yes')
        use_dry_run = str(dry_run_flag).lower() in ('1', 'true', 'yes')

        if use_resume:
            remaining_list = get_remaining_tickers(force_update=force_update)
            # Dry run: just return what would be processed
            if use_dry_run:
                preview = remaining_list[offset:(offset + limit) if (isinstance(limit, int) and limit > 0) else None]
                success_count = 0
                failed_count = 0
                results['resume'] = {
                    'dry_run': True,
                    'remaining_total': len(remaining_list),
                    'next_batch_preview': preview,
                    'limit': limit,
                    'offset': offset,
                    'force_update': force_update
                }
            else:
                resume_result = populate_user_stocks_resume(
                    limit=limit,
                    offset=offset,
                    force_update=force_update,
                    sleep_seconds=sleep_seconds
                )
                success_count = resume_result['success_count']
                failed_count = resume_result['failed_count']
                results['resume'] = {
                    'remaining_total_after': resume_result['remaining_total'],
                    'limit': limit,
                    'offset': offset,
                    'sleep_seconds': sleep_seconds,
                    'force_update': force_update,
                    'tickers_processed': resume_result['tickers_processed']
                }
        elif use_batch:
            batch_result = populate_user_stocks_batch(
                limit=limit,
                offset=offset,
                tickers=tickers,
                force_update=force_update,
                sleep_seconds=sleep_seconds
            )
            success_count = batch_result['success_count']
            failed_count = batch_result['failed_count']
            results['batch'] = {
                'limit': limit,
                'offset': offset,
                'tickers': tickers.split(',') if tickers else None,
                'sleep_seconds': sleep_seconds,
                'force_update': force_update,
                'tickers_processed': batch_result['tickers_processed']
            }
        else:
            # Populate all stock metadata
            success_count, failed_count = populate_all_user_stocks()
        
        # Get updated stats
        updated_stock_info = StockInfo.query.count()
        updated_with_metadata = StockInfo.query.filter(
            StockInfo.sector.isnot(None),
            StockInfo.industry.isnot(None)
        ).count()
        
        # Sample of populated data
        sample_stocks = StockInfo.query.filter(
            StockInfo.sector.isnot(None)
        ).limit(5).all()
        
        sample_data = []
        for stock in sample_stocks:
            sample_data.append({
                'ticker': stock.ticker,
                'company_name': stock.company_name,
                'sector': stock.sector,
                'industry': stock.industry,
                'cap_classification': stock.cap_classification,
                'naics_code': stock.naics_code,
                'exchange': stock.exchange
            })
        
        results.update({
            'population_results': {
                'success_count': success_count,
                'failed_count': failed_count,
                'total_processed': success_count + failed_count
            },
            'after_population': {
                'total_stock_info_records': updated_stock_info,
                'records_with_metadata': updated_with_metadata
            },
            'sample_populated_stocks': sample_data
        })
        
        return jsonify({
            'success': True,
            'message': f'Stock metadata populated: {success_count} successful, {failed_count} failed',
            'results': results
        })

    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/upsert-stock-info-manual')
@login_required
def admin_upsert_stock_info_manual():
    """Manually upsert minimal StockInfo metadata for specific tickers.

    Query params:
      - tickers: comma-separated list, e.g. tickers=SCHD,VTI,SSPY (required)
      - sector: default 'ETF'
      - industry: default 'ETF - Index Fund'
      - cap: default 'unknown' (small|mid|large|mega|unknown)
      - exchange: default 'NYSEARCA'
      - country: default 'USA'
    """
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403

    try:
        tickers_param = request.args.get('tickers', '').strip()
        if not tickers_param:
            return jsonify({'success': False, 'error': 'tickers parameter is required'}), 400

        sector = request.args.get('sector', default='ETF')
        industry = request.args.get('industry', default='ETF - Index Fund')
        cap = request.args.get('cap', default='unknown')
        exchange = request.args.get('exchange', default='NYSEARCA')
        country = request.args.get('country', default='USA')

        from models import StockInfo, db
        from datetime import datetime

        tickers = [t.strip().upper() for t in tickers_param.split(',') if t.strip()]
        processed = []
        created = 0
        updated = 0
        errors = []

        for t in tickers:
            try:
                si = StockInfo.query.filter_by(ticker=t).first()
                if not si:
                    si = StockInfo(ticker=t)
                    db.session.add(si)
                    created += 1
                else:
                    updated += 1

                # Minimal fields
                if not si.company_name:
                    si.company_name = t
                si.sector = sector
                si.industry = industry
                si.cap_classification = cap
                si.exchange = exchange
                si.country = country
                si.is_active = True
                si.last_updated = datetime.now()

                processed.append(t)
            except Exception as inner_e:
                errors.append({'ticker': t, 'error': str(inner_e)})

        # Commit once at the end
        db.session.commit()

        return jsonify({
            'success': True,
            'message': 'Manual upsert complete',
            'processed': processed,
            'created': created,
            'updated': updated,
            'errors': errors
        })

    except Exception as e:
        import traceback
        from models import db
        db.session.rollback()
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/debug-leaderboard')
@login_required
def admin_debug_leaderboard():
    """Debug leaderboard data availability"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, User, Stock, LeaderboardCache
        import json
        
        # Data availability check
        today = get_market_date()  # FIX: Use ET for market data checks
        yesterday = today - timedelta(days=1)
        
        debug_info = {
            'basic_data': {
                'total_users': User.query.count(),
                'users_with_stocks': User.query.join(Stock).distinct().count(),
                'total_snapshots': PortfolioSnapshot.query.count(),
                'yesterday_snapshots': PortfolioSnapshot.query.filter_by(date=yesterday).count(),
                'today_snapshots': PortfolioSnapshot.query.filter_by(date=today).count()
            },
            'recent_snapshots': [],
            'cache_status': [],
            'api_test': None
        }
        
        # Recent snapshots
        recent_snapshots = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.date >= yesterday - timedelta(days=3)
        ).order_by(PortfolioSnapshot.date.desc()).limit(10).all()
        
        for snapshot in recent_snapshots:
            debug_info['recent_snapshots'].append({
                'user_id': snapshot.user_id,
                'date': snapshot.date.isoformat(),
                'value': float(snapshot.total_value)
            })
        
        # Cache status
        cache_entries = LeaderboardCache.query.all()
        for cache in cache_entries:
            try:
                cached_data = json.loads(cache.leaderboard_data)
                debug_info['cache_status'].append({
                    'period': cache.period,
                    'entries': len(cached_data),
                    'generated_at': cache.generated_at.isoformat()
                })
            except Exception as e:
                debug_info['cache_status'].append({
                    'period': cache.period,
                    'error': str(e)
                })
        
        # Test API
        try:
            from leaderboard_utils import get_leaderboard_data
            api_data = get_leaderboard_data('YTD', 5, 'all')
            debug_info['api_test'] = {
                'success': True,
                'entries': len(api_data),
                'sample': api_data[:2] if api_data else []
            }
        except Exception as e:
            debug_info['api_test'] = {
                'success': False,
                'error': str(e)
            }
        
        return jsonify({
            'success': True,
            'debug_info': debug_info
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/populate-tiers')
def populate_tiers():
    """Populate subscription tiers with Stripe price IDs"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
            
        # Define the 5 tiers with real Stripe price IDs
        tiers = [
            {
                'tier_name': 'Light',
                'price': 8.00,
                'max_trades_per_day': 3,
                'stripe_price_id': 'price_1S4tN2HwKH0J9vzFchmuJXTze'
            },
            {
                'tier_name': 'Standard', 
                'price': 12.00,
                'max_trades_per_day': 6,
                'stripe_price_id': 'price_1S4tNdHwKH0J9vzFdJY3Opim'
            },
            {
                'tier_name': 'Active',
                'price': 20.00,
                'max_trades_per_day': 12,
                'stripe_price_id': 'price_1S4tO8HwKH0J9vzFqZBDgCOK'
            },
            {
                'tier_name': 'Pro',
                'price': 30.00,
                'max_trades_per_day': 25,
                'stripe_price_id': 'price_1S4tOYHwKH0J9vzFckMoFWwG'
            },
            {
                'tier_name': 'Elite',
                'price': 50.00,
                'max_trades_per_day': 50,
                'stripe_price_id': 'price_1S4tOtHwKH0J9vzFuxiERwQv'
            }
        ]
        
        # Clear existing tiers
        db.session.execute(text("DELETE FROM subscription_tier"))
        
        # Add new tiers
        for tier_data in tiers:
            db.session.execute(text("""
                INSERT INTO subscription_tier (tier_name, price, max_trades_per_day, stripe_price_id)
                VALUES (:tier_name, :price, :max_trades_per_day, :stripe_price_id)
            """), tier_data)
        
        db.session.commit()
        return jsonify({"success": True, "message": f"Successfully populated {len(tiers)} subscription tiers"})
        
    except Exception as e:
        db.session.rollback()
        return jsonify({"success": False, "message": f"Error: {str(e)}"})

@app.route('/admin/populate-stock-info')
def populate_stock_info():
    """Populate stock_info table with company data from Alpha Vantage"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
            
        import requests
        
        # Get Alpha Vantage API key
        alpha_vantage_api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not alpha_vantage_api_key:
            return jsonify({'error': 'Alpha Vantage API key not found'}), 500
            
        # Get all unique stock symbols from user portfolios
        symbols = db.session.execute(text("SELECT DISTINCT ticker FROM stock")).fetchall()
        
        populated_count = 0
        errors = []
        
        for (symbol,) in symbols:
            try:
                # Check if we already have data for this symbol
                existing = db.session.execute(text("""
                    SELECT id FROM stock_info WHERE symbol = :symbol
                """), {'symbol': symbol}).fetchone()
                
                if existing:
                    continue  # Skip if already exists
                
                # Fetch company overview from Alpha Vantage
                url = f"https://www.alphavantage.co/query?function=COMPANY_OVERVIEW&symbol={symbol}&apikey={alpha_vantage_api_key}"
                response = requests.get(url, timeout=10)
                data = response.json()
                
                if 'Symbol' in data and data.get('MarketCapitalization'):
                    market_cap = int(data.get('MarketCapitalization', 0))
                    company_name = data.get('Name', symbol)
                    sector = data.get('Sector', 'Unknown')
                    
                    # Insert into stock_info table
                    db.session.execute(text("""
                        INSERT INTO stock_info (symbol, company_name, market_cap, sector)
                        VALUES (:symbol, :company_name, :market_cap, :sector)
                    """), {
                        'symbol': symbol,
                        'company_name': company_name,
                        'market_cap': market_cap,
                        'sector': sector
                    })
                    
                    populated_count += 1
                else:
                    errors.append(f"No data found for {symbol}")
                    
            except Exception as e:
                errors.append(f"Error for {symbol}: {str(e)}")
        
        db.session.commit()
        
        return jsonify({
            "success": True, 
            "message": f"Successfully populated {populated_count} stock info records",
            "populated_count": populated_count,
            "errors": errors[:5]  # Limit errors shown
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({"success": False, "message": f"Error: {str(e)}"})

# Portfolio performance API endpoints
@app.route('/api/portfolio/performance/<period>')
@login_required
def get_portfolio_performance(period):
    """Get portfolio performance data for a specific time period - uses cached data for leaderboard users"""
    try:
        logger.info(f"ROUTE HIT: /api/portfolio/performance/{period}")
        logger.info(f"Request method: {request.method}")
        logger.info(f"Request headers: {dict(request.headers)}")
        logger.info(f"Session user_id: {session.get('user_id')}")
        # Note: Removed signal-based timeout as it doesn't work in serverless environments
        # Vercel handles timeouts automatically
        from datetime import datetime, timedelta
        import json
        
        user_id = session.get('user_id')
        logger.info(f"Performance API called for period {period}, user_id in session: {user_id}")
        if not user_id:
            logger.warning(f"No user_id in session for performance API. Session keys: {list(session.keys())}")
            return jsonify({'error': 'User not authenticated'}), 401
        
        period_upper = period.upper()
        
        # Try to use pre-rendered chart data for leaderboard users (much faster!)
        chart_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user_id, period=period_upper
        ).first()
        
        if chart_cache:
            try:
                # Convert Chart.js format to dashboard format
                cached_data = json.loads(chart_cache.chart_data)
                
                # Extract performance percentages from Chart.js data
                datasets = cached_data.get('datasets', [])
                labels = cached_data.get('labels', [])
                
                if not datasets or len(datasets) == 0:
                    logger.warning(f"No datasets in cached data for user {user_id}, period {period_upper}")
                    raise ValueError("No datasets in cached chart data")
                
                portfolio_dataset = datasets[0].get('data', [])
                sp500_dataset = datasets[1].get('data', []) if len(datasets) > 1 else []
                
                if not portfolio_dataset:
                    logger.warning(f"No portfolio data in cached chart for user {user_id}, period {period_upper}")
                    raise ValueError("No portfolio data in cached chart")
                
                if not sp500_dataset:
                    logger.warning(f"No S&P 500 data in cached chart for user {user_id}, period {period_upper} - falling back to live calculation")
                    raise ValueError("No S&P 500 data in cached chart")
                
                # Convert to dashboard format: list of {date, portfolio, sp500}
                chart_data = []
                for i in range(min(len(labels), len(portfolio_dataset))):
                    chart_point = {
                        'date': labels[i],
                        'portfolio': portfolio_dataset[i],
                        'sp500': sp500_dataset[i] if i < len(sp500_dataset) else 0
                    }
                    chart_data.append(chart_point)
                
                # Get last values for display labels (these are cumulative % returns)
                portfolio_return = portfolio_dataset[-1] if portfolio_dataset else 0
                sp500_return = sp500_dataset[-1] if sp500_dataset else 0
                
                logger.info(f" Using cached chart: portfolio={portfolio_return}%, sp500={sp500_return}%, points={len(chart_data)}")
                
                return jsonify({
                    'portfolio_return': round(portfolio_return, 2),
                    'sp500_return': round(sp500_return, 2),
                    'chart_data': chart_data,
                    'period': period_upper,
                    'from_cache': True
                })
                    
            except Exception as e:
                logger.warning(f"Failed to use cached chart data: {e} - falling back to live calculation")
        
        logger.info(f"No pre-rendered cache available - using live calculation for user {user_id}, period {period_upper}")
        
        # Fallback: Check session cache (5 minutes)
        cache_key = f"perf_{user_id}_{period}"
        cached_response = session.get(cache_key)
        cache_time = session.get(f"{cache_key}_time")
        
        if cached_response and cache_time:
            cache_age = datetime.now() - datetime.fromisoformat(cache_time)
            if cache_age < timedelta(minutes=5):
                logger.info(f"Using session cache for user {user_id}, period {period_upper}")
                return jsonify(cached_response)
        
        # Last resort: Live calculation (slow for non-leaderboard users)
        from portfolio_performance import PortfolioPerformanceCalculator
        calculator = PortfolioPerformanceCalculator()
        
        logger.info(f"Performing live calculation for user {user_id}, period {period_upper}")
        performance_data = calculator.get_performance_data(user_id, period_upper)
        
        # Cache the response in session
        session[cache_key] = performance_data
        session[f"{cache_key}_time"] = datetime.now().isoformat()
        
        return jsonify(performance_data)
        
    except Exception as e:
        logger.error(f"Performance calculation error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/portfolio/<int:user_id>/performance/<period>')
def get_public_portfolio_performance(user_id, period):
    """Get portfolio performance data for any user (public access for charts)"""
    try:
        from datetime import datetime, timedelta
        import json
        
        logger.info(f"Public performance API called for user {user_id}, period {period}")
        
        period_upper = period.upper()
        
        # Try to use pre-rendered chart data (same as private endpoint)
        chart_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user_id, period=period_upper
        ).first()
        
        if chart_cache:
            try:
                cached_data = json.loads(chart_cache.chart_data)
                datasets = cached_data.get('datasets', [])
                labels = cached_data.get('labels', [])
                
                if datasets and len(datasets) > 0:
                    portfolio_dataset = datasets[0].get('data', [])
                    sp500_dataset = datasets[1].get('data', []) if len(datasets) > 1 else []
                    
                    if portfolio_dataset and sp500_dataset:
                        chart_data = []
                        for i in range(min(len(labels), len(portfolio_dataset))):
                            chart_point = {
                                'date': labels[i],
                                'portfolio': portfolio_dataset[i],
                                'sp500': sp500_dataset[i] if i < len(sp500_dataset) else 0
                            }
                            chart_data.append(chart_point)
                        
                        portfolio_return = portfolio_dataset[-1] if portfolio_dataset else 0
                        sp500_return = sp500_dataset[-1] if sp500_dataset else 0
                        
                        return jsonify({
                            'portfolio_return': round(portfolio_return, 2),
                            'sp500_return': round(sp500_return, 2),
                            'chart_data': chart_data,
                            'period': period_upper,
                            'from_cache': True
                        })
            except Exception as e:
                logger.warning(f"Failed to use cached chart data: {e}")
        
        # Fallback: Live calculation
        from portfolio_performance import PortfolioPerformanceCalculator
        calculator = PortfolioPerformanceCalculator()
        
        logger.info(f"Performing live calculation for public portfolio user {user_id}, period {period_upper}")
        performance_data = calculator.get_performance_data(user_id, period_upper)
        
        return jsonify(performance_data)
        
    except Exception as e:
        logger.error(f"Public performance calculation error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/portfolio/snapshot')
@login_required
def create_portfolio_snapshot():
    """Create a portfolio snapshot for today"""
    try:
        # Import here to avoid circular imports
        from portfolio_performance import PortfolioPerformanceCalculator
        calculator = PortfolioPerformanceCalculator()
        
        user_id = session.get('user_id')
        if not user_id:
            return jsonify({'error': 'User not authenticated'}), 401
            
        calculator.create_daily_snapshot(user_id)
        return jsonify({'success': True})
    except Exception as e:
        logger.error(f"Snapshot creation error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/create-todays-snapshots')
def create_todays_snapshots():
    """Admin endpoint to create today's portfolio snapshots for all users (no historical data)"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Import here to avoid circular imports
        import sys
        import os
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from portfolio_performance import performance_calculator
        
        # Get all users with stocks
        users_with_stocks = db.session.query(User.id).join(Stock).distinct().all()
        
        snapshots_created = 0
        errors = []
        
        for (user_id,) in users_with_stocks:
            try:
                # Only create today's snapshot - no historical approximations
                performance_calculator.create_daily_snapshot(user_id)
                snapshots_created += 1
                
            except Exception as e:
                error_msg = f"Error for user {user_id}: {str(e)}"
                errors.append(error_msg)
                logger.error(error_msg)
        
        return jsonify({
            'success': True,
            'snapshots_created': snapshots_created,
            'users_processed': len(users_with_stocks),
            'errors': errors,
            'message': 'Created today\'s snapshots only. Performance data will accumulate over time as daily snapshots are created.'
        })
        
    except Exception as e:
        logger.error(f"Today's snapshots creation error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/populate-sp500-data', methods=['GET', 'POST'])
@login_required
def populate_sp500_data():
    """Admin endpoint to populate S&P 500 data - synchronous version for debugging"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData
        
        # Get years parameter (default 5 for full historical data)
        years = int(request.args.get('years', 5))
        
        logger.info(f"Starting SYNCHRONOUS S&P 500 population for {years} years")
        calculator = PortfolioPerformanceCalculator()
        
        # Clear ALL existing market data to replace with real data
        deleted_count = MarketData.query.count()
        MarketData.query.delete()
        db.session.commit()
        logger.info(f"Cleared {deleted_count} existing market data records")
        
        # Verify AlphaVantage API key exists
        if not hasattr(calculator, 'alpha_vantage_api_key') or not calculator.alpha_vantage_api_key:
            error_msg = "AlphaVantage API key not found - cannot fetch real data"
            logger.error(error_msg)
            return jsonify({'error': error_msg}), 500
        
        logger.info(f"AlphaVantage API key found: {calculator.alpha_vantage_api_key[:10]}...")
        logger.info("Starting AlphaVantage API call...")
        
        # Use micro-chunked processing
        result = calculator.fetch_historical_sp500_data_micro_chunks(years_back=years)
        
        if result['success']:
            logger.info(f"SUCCESS: S&P 500 population completed: {result['total_data_points']} data points")
            return jsonify({
                'success': True,
                'message': f'Successfully populated {result["total_data_points"]} real S&P 500 data points',
                'total_data_points': result['total_data_points'],
                'chunks_processed': result.get('chunks_processed', 0),
                'years_requested': years,
                'errors': result.get('errors', []),
                'data_source': 'AlphaVantage TIME_SERIES_DAILY (SPY ETF) - Synchronous Processing'
            })
        else:
            logger.error(f"FAILED: S&P 500 population failed: {result['error']}")
            return jsonify({'error': result['error']}), 500
        
    except Exception as e:
        logger.error(f"EXCEPTION: S&P 500 population error: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-env', methods=['GET'])
@login_required
def debug_env():
    """Debug environment variables"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    import os
    
    # Check for AlphaVantage API key in different ways
    alpha_key_direct = os.environ.get('ALPHA_VANTAGE_API_KEY')
    alpha_key_getenv = os.getenv('ALPHA_VANTAGE_API_KEY')
    
    # Check all environment variables that contain 'ALPHA'
    alpha_vars = {k: v[:10] + '...' if v and len(v) > 10 else v 
                  for k, v in os.environ.items() if 'ALPHA' in k.upper()}
    
    return jsonify({
        'alpha_vantage_direct': alpha_key_direct[:10] + '...' if alpha_key_direct else None,
        'alpha_vantage_getenv': alpha_key_getenv[:10] + '...' if alpha_key_getenv else None,
        'all_alpha_vars': alpha_vars,
        'env_var_count': len(os.environ),
        'has_flask_env': 'FLASK_ENV' in os.environ,
        'flask_env_value': os.environ.get('FLASK_ENV')
    })

@app.route('/admin/test-alphavantage', methods=['GET'])
@login_required
def test_alphavantage():
    """Test AlphaVantage API connection with minimal request"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        import requests
        
        calculator = PortfolioPerformanceCalculator()
        
        # Check if API key exists
        if not hasattr(calculator, 'alpha_vantage_api_key') or not calculator.alpha_vantage_api_key:
            return jsonify({
                'success': False,
                'error': 'AlphaVantage API key not found',
                'note': 'Check environment variables'
            })
        
        # Make minimal API call (just get latest SPY price)
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'GLOBAL_QUOTE',
            'symbol': 'SPY',
            'apikey': calculator.alpha_vantage_api_key
        }
        
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if 'Global Quote' in data:
            quote = data['Global Quote']
            return jsonify({
                'success': True,
                'api_key_works': True,
                'spy_price': quote.get('05. price', 'N/A'),
                'spy_change': quote.get('09. change', 'N/A'),
                'last_updated': quote.get('07. latest trading day', 'N/A'),
                'note': 'AlphaVantage API is working correctly'
            })
        else:
            return jsonify({
                'success': False,
                'api_response': data,
                'error': 'Unexpected API response format'
            })
            
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/admin/populate-sp500-tiny', methods=['GET'])
@login_required
def populate_sp500_tiny():
    """Populate S&P 500 data in tiny batches to avoid timeout"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData
        import requests
        from datetime import date, timedelta
        
        calculator = PortfolioPerformanceCalculator()
        
        # Get just 30 days of data to avoid timeout
        end_date = date.today()
        start_date = end_date - timedelta(days=30)
        
        # Make direct AlphaVantage call for recent data only
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'TIME_SERIES_DAILY',
            'symbol': 'SPY',
            'outputsize': 'compact',  # Only recent 100 days
            'apikey': calculator.alpha_vantage_api_key
        }
        
        response = requests.get(url, params=params, timeout=15)
        data = response.json()
        
        if 'Time Series (Daily)' not in data:
            return jsonify({'error': 'Invalid AlphaVantage response', 'response': data})
        
        time_series = data['Time Series (Daily)']
        data_points = 0
        
        # Process only last 30 days
        for date_str, daily_data in time_series.items():
            try:
                data_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                
                if start_date <= data_date <= end_date:
                    spy_price = float(daily_data['4. close'])
                    sp500_value = spy_price * 10  # Convert to S&P 500 index
                    
                    # Check if exists
                    existing = MarketData.query.filter_by(
                        ticker='SPY_SP500',
                        date=data_date
                    ).first()
                    
                    if not existing:
                        market_data = MarketData(
                            ticker='SPY_SP500',
                            date=data_date,
                            close_price=sp500_value
                        )
                        db.session.add(market_data)
                        data_points += 1
            
            except (ValueError, KeyError) as e:
                continue
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'Added {data_points} recent S&P 500 data points',
            'data_points': data_points,
            'date_range': f'{start_date} to {end_date}',
            'note': 'Use multiple times to build historical data gradually'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/test-sp500-accuracy', methods=['GET'])
@login_required
def test_sp500_accuracy():
    """Compare our S&P 500 data with actual index data from AlphaVantage"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData
        import requests
        
        calculator = PortfolioPerformanceCalculator()
        
        # Get actual S&P 500 index data (^GSPC) from AlphaVantage
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'TIME_SERIES_DAILY',
            'symbol': '^GSPC',  # Actual S&P 500 index
            'outputsize': 'compact',
            'apikey': calculator.alpha_vantage_api_key
        }
        
        response = requests.get(url, params=params, timeout=15)
        data = response.json()
        
        if 'Time Series (Daily)' not in data:
            return jsonify({'error': 'Could not fetch S&P 500 index data', 'response': data})
        
        # Compare recent dates
        sp500_data = data['Time Series (Daily)']
        comparisons = []
        
        for date_str, daily_data in list(sp500_data.items())[:5]:  # Last 5 days
            try:
                data_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                actual_sp500 = float(daily_data['4. close'])
                
                # Get our SPY-based data for same date
                our_data = MarketData.query.filter_by(
                    ticker='SPY_SP500',
                    date=data_date
                ).first()
                
                if our_data:
                    our_sp500 = our_data.close_price
                    difference = abs(actual_sp500 - our_sp500)
                    percent_diff = (difference / actual_sp500) * 100
                    
                    comparisons.append({
                        'date': date_str,
                        'actual_sp500': actual_sp500,
                        'our_sp500_spy_based': our_sp500,
                        'difference': round(difference, 2),
                        'percent_difference': round(percent_diff, 3)
                    })
            
            except (ValueError, KeyError):
                continue
        
        return jsonify({
            'success': True,
            'comparisons': comparisons,
            'note': 'Shows accuracy of SPY*10 vs actual S&P 500 index',
            'recommendation': 'Consider using ^GSPC directly if differences are significant'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/check-sp500-anomalies', methods=['GET'])
@login_required
def check_sp500_anomalies():
    """Check for anomalous data points in S&P 500 dataset"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData
        from datetime import datetime
        
        # Get all S&P 500 data points
        all_data = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        
        if not all_data:
            return jsonify({'error': 'No S&P 500 data found'})
        
        anomalies = []
        suspicious_dates = ['2021-04-01', '2024-03-28']
        
        # Check specific suspicious dates
        for date_str in suspicious_dates:
            try:
                check_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                data_point = MarketData.query.filter_by(
                    ticker='SPY_SP500',
                    date=check_date
                ).first()
                
                if data_point:
                    anomalies.append({
                        'date': date_str,
                        'value': data_point.close_price,
                        'type': 'suspicious_spike'
                    })
            except ValueError:
                continue
        
        # Check for large day-to-day changes (>10%)
        large_changes = []
        for i in range(1, len(all_data)):
            prev_price = all_data[i-1].close_price
            curr_price = all_data[i].close_price
            
            if prev_price > 0:
                change_pct = abs((curr_price - prev_price) / prev_price) * 100
                if change_pct > 10:  # More than 10% change
                    large_changes.append({
                        'date': all_data[i].date.isoformat(),
                        'prev_value': prev_price,
                        'curr_value': curr_price,
                        'change_percent': round(change_pct, 2)
                    })
        
        # Check for unrealistic values (outside normal S&P 500 range)
        unrealistic_values = []
        for data_point in all_data:
            # S&P 500 should be roughly 2000-7000 in recent years
            if data_point.close_price < 1000 or data_point.close_price > 10000:
                unrealistic_values.append({
                    'date': data_point.date.isoformat(),
                    'value': data_point.close_price,
                    'issue': 'outside_normal_range'
                })
        
        return jsonify({
            'success': True,
            'total_data_points': len(all_data),
            'suspicious_dates_checked': anomalies,
            'large_daily_changes': large_changes[:10],  # First 10
            'unrealistic_values': unrealistic_values[:10],  # First 10
            'data_quality_summary': {
                'large_changes_count': len(large_changes),
                'unrealistic_values_count': len(unrealistic_values)
            }
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/investigate-alphavantage-spikes', methods=['GET'])
@login_required
def investigate_alphavantage_spikes():
    """Investigate raw AlphaVantage data around spike dates"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData
        import requests
        from datetime import datetime, timedelta
        
        calculator = PortfolioPerformanceCalculator()
        
        spike_dates = ['2021-04-01', '2024-03-28']
        investigations = []
        
        for spike_date_str in spike_dates:
            spike_date = datetime.strptime(spike_date_str, '%Y-%m-%d').date()
            
            # Get our stored data around this date
            our_data_points = []
            for days_offset in range(-5, 6):  # 5 days before and after
                check_date = spike_date + timedelta(days=days_offset)
                data_point = MarketData.query.filter_by(
                    ticker='SPY_SP500',
                    date=check_date
                ).first()
                
                if data_point:
                    our_data_points.append({
                        'date': check_date.isoformat(),
                        'our_value': data_point.close_price,
                        'spy_equivalent': data_point.close_price / 10
                    })
            
            # Get fresh AlphaVantage data for SPY around this date
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'TIME_SERIES_DAILY',
                'symbol': 'SPY',
                'outputsize': 'full',
                'apikey': calculator.alpha_vantage_api_key
            }
            
            response = requests.get(url, params=params, timeout=30)
            alphavantage_data = response.json()
            
            alphavantage_points = []
            if 'Time Series (Daily)' in alphavantage_data:
                time_series = alphavantage_data['Time Series (Daily)']
                
                for days_offset in range(-5, 6):
                    check_date = spike_date + timedelta(days=days_offset)
                    date_str = check_date.isoformat()
                    
                    if date_str in time_series:
                        daily_data = time_series[date_str]
                        spy_close = float(daily_data['4. close'])
                        
                        alphavantage_points.append({
                            'date': date_str,
                            'spy_close': spy_close,
                            'sp500_equivalent': spy_close * 10,
                            'volume': daily_data.get('5. volume', 'N/A'),
                            'high': float(daily_data['2. high']),
                            'low': float(daily_data['3. low'])
                        })
            
            # Also check actual S&P 500 index (^GSPC) for comparison
            gspc_params = {
                'function': 'TIME_SERIES_DAILY',
                'symbol': '^GSPC',
                'outputsize': 'compact',
                'apikey': calculator.alpha_vantage_api_key
            }
            
            gspc_response = requests.get(url, params=gspc_params, timeout=30)
            gspc_data = gspc_response.json()
            
            gspc_points = []
            if 'Time Series (Daily)' in gspc_data:
                gspc_series = gspc_data['Time Series (Daily)']
                
                for days_offset in range(-5, 6):
                    check_date = spike_date + timedelta(days=days_offset)
                    date_str = check_date.isoformat()
                    
                    if date_str in gspc_series:
                        daily_data = gspc_series[date_str]
                        gspc_close = float(daily_data['4. close'])
                        
                        gspc_points.append({
                            'date': date_str,
                            'sp500_actual': gspc_close,
                            'volume': daily_data.get('5. volume', 'N/A')
                        })
            
            investigations.append({
                'spike_date': spike_date_str,
                'our_stored_data': our_data_points,
                'alphavantage_spy_data': alphavantage_points,
                'alphavantage_sp500_data': gspc_points
            })
        
        return jsonify({
            'success': True,
            'investigations': investigations,
            'note': 'Compare our stored data vs fresh AlphaVantage data around spike dates'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-chart-data/<period>', methods=['GET'])
@login_required
def debug_chart_data(period):
    """Debug chart data generation to find spike sources"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData
        from datetime import datetime, date, timedelta
        
        calculator = PortfolioPerformanceCalculator()
        end_date = date.today()
        
        # Define period mappings (same as in performance calculator)
        period_days = {
            '1D': 1,
            '5D': 5,
            '1M': 30,
            '3M': 90,
            'YTD': (end_date - date(end_date.year, 1, 1)).days,
            '1Y': 365,
            '5Y': 1825
        }
        
        if period not in period_days:
            return jsonify({'error': 'Invalid period'})
        
        if period == 'YTD':
            start_date = date(end_date.year, 1, 1)
        else:
            start_date = end_date - timedelta(days=period_days[period])
        
        # Get raw S&P 500 data
        raw_sp500_data = calculator.get_sp500_data(start_date, end_date)
        
        # Get sampled data (what charts actually use)
        sp500_dates = sorted(raw_sp500_data.keys())
        sampled_dates = calculator._sample_dates_for_period(sp500_dates, period)
        
        # Check for large jumps in sampled data
        sampled_data = []
        large_jumps = []
        
        for i, date_key in enumerate(sampled_dates):
            if date_key in raw_sp500_data:
                value = raw_sp500_data[date_key]
                sampled_data.append({
                    'date': date_key.isoformat(),
                    'value': value,
                    'index': i
                })
                
                # Check for large jumps between sampled points
                if i > 0:
                    prev_value = sampled_data[i-1]['value']
                    change_pct = abs((value - prev_value) / prev_value) * 100
                    
                    if change_pct > 5:  # More than 5% jump
                        large_jumps.append({
                            'from_date': sampled_data[i-1]['date'],
                            'to_date': date_key.isoformat(),
                            'from_value': prev_value,
                            'to_value': value,
                            'change_percent': round(change_pct, 2),
                            'days_between': (date_key - datetime.fromisoformat(sampled_data[i-1]['date']).date()).days
                        })
        
        # Check what raw data exists between large jumps
        jump_analysis = []
        for jump in large_jumps:
            from_date = datetime.fromisoformat(jump['from_date']).date()
            to_date = datetime.fromisoformat(jump['to_date']).date()
            
            # Get all raw data points between these dates
            between_dates = []
            current = from_date + timedelta(days=1)
            while current < to_date:
                if current in raw_sp500_data:
                    between_dates.append({
                        'date': current.isoformat(),
                        'value': raw_sp500_data[current],
                        'included_in_sample': current in sampled_dates
                    })
                current += timedelta(days=1)
            
            jump_analysis.append({
                'jump': jump,
                'raw_data_between': between_dates
            })
        
        return jsonify({
            'success': True,
            'period': period,
            'date_range': f"{start_date} to {end_date}",
            'total_raw_points': len(sp500_dates),
            'sampled_points': len(sampled_dates),
            'sampling_ratio': f"{len(sampled_dates)}/{len(sp500_dates)}",
            'large_jumps_found': len(large_jumps),
            'large_jumps': large_jumps,
            'jump_analysis': jump_analysis,
            'note': 'Large jumps may be caused by sampling skipping intermediate data points'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/find-duplicate-sp500-values', methods=['GET'])
@login_required
def find_duplicate_sp500_values():
    """Find duplicate S&P 500 values that create chart spikes"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData
        from collections import defaultdict
        
        # Get all S&P 500 data points
        all_data = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        
        if not all_data:
            return jsonify({'error': 'No S&P 500 data found'})
        
        # Group by value to find duplicates
        value_groups = defaultdict(list)
        for data_point in all_data:
            # Round to avoid floating point precision issues
            rounded_value = round(data_point.close_price, 2)
            value_groups[rounded_value].append({
                'date': data_point.date.isoformat(),
                'exact_value': data_point.close_price,
                'id': data_point.id
            })
        
        # Find values that appear on multiple dates
        duplicates = {}
        for value, occurrences in value_groups.items():
            if len(occurrences) > 1:
                # Check if dates are far apart (suspicious)
                dates = [occ['date'] for occ in occurrences]
                dates.sort()
                
                duplicates[value] = {
                    'occurrences': occurrences,
                    'count': len(occurrences),
                    'date_range': f"{dates[0]} to {dates[-1]}",
                    'suspicious': len(occurrences) > 2 or (len(occurrences) == 2 and 
                        (datetime.fromisoformat(dates[1]) - datetime.fromisoformat(dates[0])).days > 7)
                }
        
        # Find the problematic 6398.1 value specifically
        problem_value = None
        for value, info in duplicates.items():
            if 6390 <= value <= 6405:  # Around the problematic value
                problem_value = {
                    'value': value,
                    'info': info
                }
                break
        
        return jsonify({
            'success': True,
            'total_data_points': len(all_data),
            'duplicate_values_found': len(duplicates),
            'duplicates': dict(list(duplicates.items())[:10]),  # First 10
            'problem_value_6398': problem_value,
            'note': 'Duplicate values on different dates create artificial chart spikes'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/fix-duplicate-sp500-values', methods=['GET'])
@login_required
def fix_duplicate_sp500_values():
    """Fix duplicate S&P 500 values by interpolating between neighbors"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData, db
        from collections import defaultdict
        
        # Get all S&P 500 data points
        all_data = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        
        if not all_data:
            return jsonify({'error': 'No S&P 500 data found'})
        
        fixes_applied = []
        
        # Find and fix ALL occurrences of the problematic 6398.1 value
        problematic_value = 6398.099999999999
        
        for i in range(1, len(all_data) - 1):
            current = all_data[i]
            curr_val = current.close_price
            
            # Check if this is the problematic duplicate value
            if abs(curr_val - problematic_value) < 0.1:
                prev_val = all_data[i-1].close_price
                next_val = all_data[i+1].close_price
                
                # Only fix if the neighboring values are reasonable (not also duplicates)
                if (abs(prev_val - problematic_value) > 100 and 
                    abs(next_val - problematic_value) > 100):
                    
                    # Interpolate between neighbors
                    interpolated_value = (prev_val + next_val) / 2
                    
                    fixes_applied.append({
                        'date': current.date.isoformat(),
                        'original_value': curr_val,
                        'interpolated_value': interpolated_value,
                        'prev_value': prev_val,
                        'next_value': next_val,
                        'reason': 'problematic_6398_value_fixed'
                    })
                    
                    current.close_price = interpolated_value
        
        # Also fix other suspicious duplicates (values appearing on distant dates)
        value_groups = defaultdict(list)
        for i, data_point in enumerate(all_data):
            rounded_value = round(data_point.close_price, 1)
            value_groups[rounded_value].append((i, data_point))
        
        # Fix other duplicate values that appear on dates >30 days apart
        for value, occurrences in value_groups.items():
            if len(occurrences) > 1 and value != round(problematic_value, 1):
                # Check if dates are far apart
                dates = [occ[1].date for occ in occurrences]
                dates.sort()
                
                if len(dates) >= 2 and (dates[-1] - dates[0]).days > 30:
                    # Fix all but the first occurrence
                    for i, (idx, data_point) in enumerate(occurrences[1:], 1):
                        if 1 <= idx < len(all_data) - 1:
                            prev_val = all_data[idx-1].close_price
                            next_val = all_data[idx+1].close_price
                            interpolated_value = (prev_val + next_val) / 2
                            
                            fixes_applied.append({
                                'date': data_point.date.isoformat(),
                                'original_value': data_point.close_price,
                                'interpolated_value': interpolated_value,
                                'prev_value': prev_val,
                                'next_value': next_val,
                                'reason': f'duplicate_value_{value}_fixed'
                            })
                            
                            data_point.close_price = interpolated_value
        
        # Commit changes
        if fixes_applied:
            db.session.commit()
        
        return jsonify({
            'success': True,
            'fixes_applied': len(fixes_applied),
            'details': fixes_applied[:20],  # Show first 20 fixes
            'message': f'Fixed {len(fixes_applied)} duplicate S&P 500 values across all historical data'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/fix-duplicates-with-alphavantage', methods=['GET'])
@login_required
def fix_duplicates_with_alphavantage():
    """Replace duplicate S&P 500 values with correct AlphaVantage data"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData, db
        import requests
        from datetime import datetime
        
        calculator = PortfolioPerformanceCalculator()
        
        # Get all S&P 500 data points
        all_data = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        
        if not all_data:
            return jsonify({'error': 'No S&P 500 data found'})
        
        # Find all occurrences of the problematic 6398.1 value
        problematic_value = 6398.099999999999
        duplicate_dates = []
        
        for data_point in all_data:
            if abs(data_point.close_price - problematic_value) < 0.1:
                duplicate_dates.append(data_point.date.isoformat())
        
        if not duplicate_dates:
            return jsonify({'message': 'No duplicate 6398.1 values found'})
        
        # Fetch fresh AlphaVantage data for SPY
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'TIME_SERIES_DAILY',
            'symbol': 'SPY',
            'outputsize': 'full',
            'apikey': calculator.alpha_vantage_api_key
        }
        
        response = requests.get(url, params=params, timeout=60)
        alphavantage_data = response.json()
        
        if 'Time Series (Daily)' not in alphavantage_data:
            return jsonify({'error': 'Could not fetch AlphaVantage data', 'response': alphavantage_data})
        
        time_series = alphavantage_data['Time Series (Daily)']
        fixes_applied = []
        
        # Replace duplicate values with correct AlphaVantage data
        for data_point in all_data:
            if abs(data_point.close_price - problematic_value) < 0.1:
                date_str = data_point.date.isoformat()
                
                if date_str in time_series:
                    # Get correct SPY price and convert to S&P 500
                    spy_close = float(time_series[date_str]['4. close'])
                    correct_sp500_value = spy_close * 10
                    
                    fixes_applied.append({
                        'date': date_str,
                        'original_value': data_point.close_price,
                        'correct_alphavantage_value': correct_sp500_value,
                        'spy_close': spy_close,
                        'reason': 'replaced_with_alphavantage_data'
                    })
                    
                    data_point.close_price = correct_sp500_value
                else:
                    # If exact date not found, try to interpolate from nearby dates
                    date_obj = data_point.date
                    nearby_values = []
                    
                    # Look for dates within 3 days
                    for days_offset in range(-3, 4):
                        check_date = date_obj + timedelta(days=days_offset)
                        check_date_str = check_date.isoformat()
                        
                        if check_date_str in time_series:
                            spy_close = float(time_series[check_date_str]['4. close'])
                            nearby_values.append(spy_close * 10)
                    
                    if nearby_values:
                        # Use average of nearby values
                        correct_sp500_value = sum(nearby_values) / len(nearby_values)
                        
                        fixes_applied.append({
                            'date': date_str,
                            'original_value': data_point.close_price,
                            'correct_alphavantage_value': correct_sp500_value,
                            'reason': 'interpolated_from_nearby_alphavantage_data',
                            'nearby_values_count': len(nearby_values)
                        })
                        
                        data_point.close_price = correct_sp500_value
        
        # Commit changes
        if fixes_applied:
            db.session.commit()
        
        return jsonify({
            'success': True,
            'fixes_applied': len(fixes_applied),
            'details': fixes_applied,
            'duplicate_dates_found': duplicate_dates,
            'message': f'Replaced {len(fixes_applied)} duplicate values with correct AlphaVantage data'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-1d-5d-charts', methods=['GET'])
@login_required
def debug_1d_5d_charts():
    """Debug 1D and 5D chart data to understand limitations"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData, PortfolioSnapshot
        from datetime import datetime, date, timedelta
        
        calculator = PortfolioPerformanceCalculator()
        end_date = date.today()
        
        # Check 1D data
        start_1d = end_date - timedelta(days=1)
        sp500_1d = calculator.get_sp500_data(start_1d, end_date)
        
        # Check 5D data
        start_5d = end_date - timedelta(days=5)
        sp500_5d = calculator.get_sp500_data(start_5d, end_date)
        
        # Check portfolio snapshots for comparison
        user_id = session.get('user_id', 1)  # Use session user or default
        snapshots_1d = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.user_id == user_id,
            PortfolioSnapshot.date >= start_1d,
            PortfolioSnapshot.date <= end_date
        ).order_by(PortfolioSnapshot.date).all()
        
        snapshots_5d = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.user_id == user_id,
            PortfolioSnapshot.date >= start_5d,
            PortfolioSnapshot.date <= end_date
        ).order_by(PortfolioSnapshot.date).all()
        
        return jsonify({
            'success': True,
            'analysis': {
                '1D_data': {
                    'sp500_points': len(sp500_1d),
                    'portfolio_snapshots': len(snapshots_1d),
                    'dates': list(sp500_1d.keys()) if sp500_1d else [],
                    'issue': 'Only 1-2 data points for single day (weekend/holiday gaps)'
                },
                '5D_data': {
                    'sp500_points': len(sp500_5d),
                    'portfolio_snapshots': len(snapshots_5d),
                    'dates': list(sp500_5d.keys()) if sp500_5d else [],
                    'issue': 'Only 3-5 data points for 5 days (excludes weekends)'
                },
                'problems': [
                    '1D charts show flat line or single point',
                    '5D charts have large gaps between trading days',
                    'No intraday data for meaningful 1D progression',
                    'Weekend/holiday gaps create poor user experience'
                ],
                'solutions': [
                    'Add AlphaVantage intraday API for 1D charts (15min/30min intervals)',
                    'Extend 5D range to include more trading days',
                    'Show hourly data for current trading day',
                    'Add market hours awareness'
                ]
            }
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/implement-intraday-solution', methods=['GET'])
@login_required
def implement_intraday_solution():
    """Implement intraday data solution for 1D charts"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        import requests
        import os
        from models import MarketData, db
        from datetime import datetime, date, timedelta
        
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not api_key:
            return jsonify({'error': 'AlphaVantage API key not found'}), 500
        
        # Fetch intraday data for SPY (30-minute intervals)
        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=SPY&interval=30min&apikey={api_key}'
        response = requests.get(url)
        data = response.json()
        
        if 'Time Series (30min)' not in data:
            return jsonify({'error': 'Failed to fetch intraday data', 'response': data}), 500
        
        intraday_data = data['Time Series (30min)']
        today = date.today()
        points_added = 0
        
        # Process today's intraday data
        for timestamp_str, values in intraday_data.items():
            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
            
            # Only process today's data
            if timestamp.date() == today:
                sp500_value = float(values['4. close']) * 10  # SPY to S&P 500 conversion
                
                # Create intraday market data entry with timestamp
                existing = MarketData.query.filter_by(
                    ticker='SPY_SP500_INTRADAY',
                    date=timestamp.date(),
                    timestamp=timestamp
                ).first()
                
                if not existing:
                    market_data = MarketData(
                        ticker='SPY_SP500_INTRADAY',
                        date=timestamp.date(),
                        timestamp=timestamp,
                        close_price=sp500_value
                    )
                    db.session.add(market_data)
                    points_added += 1
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'solution': 'Intraday data implementation',
            'points_added': points_added,
            'message': f'Added {points_added} intraday data points for 1D charts',
            'benefits': [
                '30-minute intervals provide smooth 1D chart progression',
                'Real-time market movement visibility',
                'Single API call per day for intraday data'
            ]
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/optimize-5d-charts', methods=['GET'])
@login_required
def optimize_5d_charts():
    """Optimize 5D charts by extending to 10 trading days"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        from datetime import date, timedelta
        
        calculator = PortfolioPerformanceCalculator()
        end_date = date.today()
        
        # Extend 5D to show 10 calendar days (7-8 trading days)
        start_date = end_date - timedelta(days=10)
        sp500_data = calculator.get_sp500_data(start_date, end_date)
        
        # Count actual trading days
        trading_days = len(sp500_data)
        
        return jsonify({
            'success': True,
            'solution': 'Extended 5D range to 10 calendar days',
            'trading_days_found': trading_days,
            'dates': list(sp500_data.keys()) if sp500_data else [],
            'benefits': [
                f'Shows {trading_days} trading days instead of 3-5',
                'Eliminates weekend gaps in chart display',
                'Better trend visualization for short-term periods',
                'No additional API calls required'
            ],
            'recommendation': 'Update portfolio_performance.py period mapping: 5D -> 10 days'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/implement-1d-chart-alternatives', methods=['GET'])
@login_required
def implement_1d_chart_alternatives():
    """Implement practical 1D chart alternatives without real-time intraday data"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from datetime import datetime, time
        try:
            import pytz
            # Check if markets are currently open (approximate)
            eastern = pytz.timezone('US/Eastern')
        except ImportError:
            logger.error("pytz module not available - using UTC fallback")
            from datetime import timezone
            eastern = timezone.utc  # Fallback to UTC
        now_eastern = datetime.now(eastern)
        market_open = time(9, 30)  # 9:30 AM
        market_close = time(16, 0)  # 4:00 PM
        is_weekday = now_eastern.weekday() < 5
        is_market_hours = market_open <= now_eastern.time() <= market_close
        
        market_status = "OPEN" if (is_weekday and is_market_hours) else "CLOSED"
        
        alternatives = {
            'option_1': {
                'name': 'Market Status Indicator',
                'description': 'Show single data point with market status',
                'implementation': 'Add "Market Open/Closed" badge to 1D charts',
                'pros': ['No API calls', 'Clear user expectation'],
                'cons': ['Still single point display']
            },
            'option_2': {
                'name': 'Extended Recent Period',
                'description': 'Change 1D to show last 3 trading days',
                'implementation': 'Rename "1D" to "3D" and show 3 trading days',
                'pros': ['More meaningful progression', 'No API calls'],
                'cons': ['Not truly "1 day"']
            },
            'option_3': {
                'name': 'Hide 1D During Market Hours',
                'description': 'Only show 1D chart when markets closed',
                'implementation': 'Conditional display based on market hours',
                'pros': ['Avoids incomplete data confusion'],
                'cons': ['Feature unavailable during trading']
            },
            'option_4': {
                'name': 'Previous Day Focus',
                'description': 'Show "Yesterday\'s Performance" instead of "1D"',
                'implementation': 'Relabel and show complete previous trading day',
                'pros': ['Complete data story', 'Clear expectation'],
                'cons': ['Not current day performance']
            }
        }
        
        return jsonify({
            'success': True,
            'current_market_status': market_status,
            'current_time_eastern': now_eastern.strftime('%Y-%m-%d %H:%M:%S %Z'),
            'alternatives': alternatives,
            'recommendation': 'Option 2 (Extended Recent Period) or Option 4 (Previous Day Focus)',
            'reasoning': 'Provides meaningful data without API complexity or incomplete current-day issues'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/test-premium-intraday', methods=['GET'])
@login_required
def test_premium_intraday():
    """Test if current AlphaVantage API key has premium access for real-time intraday data"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        import requests
        import os
        from datetime import datetime
        
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not api_key:
            return jsonify({'error': 'AlphaVantage API key not found'}), 500
        
        # Test current day intraday data with real-time entitlement
        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=SPY&interval=5min&entitlement=realtime&apikey={api_key}'
        response = requests.get(url)
        data = response.json()
        
        # Check for premium access indicators
        has_premium = False
        current_day_data = False
        today = datetime.now().strftime('%Y-%m-%d')
        
        if 'Time Series (5min)' in data:
            intraday_data = data['Time Series (5min)']
            
            # Check if we have today's data
            for timestamp in intraday_data.keys():
                if timestamp.startswith(today):
                    current_day_data = True
                    break
            
            # Premium accounts typically get more frequent updates and current day data
            has_premium = current_day_data and len(intraday_data) > 100
        
        # Check for error messages indicating premium requirement
        error_msg = data.get('Error Message', '')
        info_msg = data.get('Information', '')
        premium_required = 'premium' in error_msg.lower() or 'premium' in info_msg.lower()
        
        return jsonify({
            'success': True,
            'api_key_status': 'Valid' if 'Time Series (5min)' in data else 'Invalid/Limited',
            'has_premium_access': has_premium,
            'current_day_data_available': current_day_data,
            'total_data_points': len(data.get('Time Series (5min)', {})),
            'sample_timestamps': list(data.get('Time Series (5min)', {}).keys())[:5],
            'premium_required_message': premium_required,
            'recommendation': 'Real-time 1D charts possible' if has_premium else 'Stick with extended 5D solution',
            'raw_response_keys': list(data.keys())
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/implement-realtime-1d-charts', methods=['GET'])
@login_required
def implement_realtime_1d_charts():
    """Implement real-time 1D intraday charts using entitlement=realtime"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        import requests
        import os
        from models import MarketData, db
        from datetime import datetime, date, timedelta
        
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not api_key:
            return jsonify({'error': 'AlphaVantage API key not found'}), 500
        
        # Fetch real-time intraday data for SPY (5-minute intervals)
        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=SPY&interval=5min&entitlement=realtime&apikey={api_key}'
        response = requests.get(url)
        data = response.json()
        
        if 'Time Series (5min)' not in data:
            return jsonify({'error': 'Failed to fetch real-time intraday data', 'response': data}), 500
        
        intraday_data = data['Time Series (5min)']
        today = date.today()
        points_added = 0
        current_day_points = 0
        
        # Process real-time intraday data
        for timestamp_str, values in intraday_data.items():
            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
            
            # Count current day points
            if timestamp.date() == today:
                current_day_points += 1
            
            # Store all recent intraday data (last 2 days for 1D charts)
            if timestamp.date() >= today - timedelta(days=2):
                sp500_value = float(values['4. close']) * 10  # SPY to S&P 500 conversion
                
                # Create/update intraday market data entry
                existing = MarketData.query.filter_by(
                    ticker='SPY_SP500_INTRADAY',
                    date=timestamp.date(),
                    timestamp=timestamp
                ).first()
                
                if not existing:
                    market_data = MarketData(
                        ticker='SPY_SP500_INTRADAY',
                        date=timestamp.date(),
                        timestamp=timestamp,
                        close_price=sp500_value
                    )
                    db.session.add(market_data)
                    points_added += 1
                else:
                    # Update existing data with latest value
                    existing.close_price = sp500_value
                    points_added += 1
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'solution': 'Real-time 1D intraday charts implemented',
            'points_added': points_added,
            'current_day_points': current_day_points,
            'message': f'Added {points_added} real-time intraday data points',
            'benefits': [
                '5-minute intervals provide smooth 1D chart progression',
                'Real-time current trading day data available',
                'Live market movement visibility during trading hours',
                'Automatic updates throughout the day'
            ],
            'next_steps': [
                'Update portfolio_performance.py to use intraday data for 1D charts',
                'Add market hours detection for optimal data display',
                'Set up periodic refresh during trading hours'
            ]
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/run-intraday-migration', methods=['GET'])
@login_required
def run_intraday_migration():
    """Run the intraday migration to add timestamp column"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import db
        
        # Add timestamp column and update constraints
        db.engine.execute("""
            ALTER TABLE market_data ADD COLUMN IF NOT EXISTS timestamp TIMESTAMP;
        """)
        
        # Rename symbol column to ticker for consistency
        try:
            db.engine.execute("""
                ALTER TABLE market_data RENAME COLUMN symbol TO ticker;
            """)
        except:
            pass  # Column might already be renamed
        
        # Update ticker column size
        db.engine.execute("""
            ALTER TABLE market_data ALTER COLUMN ticker TYPE VARCHAR(20);
        """)
        
        # Drop ALL old constraints that might conflict
        constraints_to_drop = [
            'unique_symbol_date',
            'unique_symbol_date_timestamp', 
            'market_data_symbol_date_key',
            'market_data_ticker_date_key',
            'unique_ticker_date_timestamp'
        ]
        
        for constraint_name in constraints_to_drop:
            try:
                db.engine.execute(f"""
                    ALTER TABLE market_data DROP CONSTRAINT IF EXISTS {constraint_name};
                """)
            except:
                pass
        
        # Add the correct new constraint
        db.engine.execute("""
            ALTER TABLE market_data ADD CONSTRAINT unique_ticker_date_timestamp 
            UNIQUE (ticker, date, timestamp);
        """)
        
        return jsonify({
            'success': True,
            'message': 'Intraday migration completed successfully',
            'changes': [
                'Added timestamp column to market_data table',
                'Renamed symbol column to ticker for consistency',
                'Increased ticker column size to VARCHAR(20)',
                'Dropped all conflicting constraints',
                'Added new unique constraint (ticker, date, timestamp)'
            ]
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-performance-api', methods=['GET'])
@login_required
def debug_performance_api():
    """Debug portfolio performance API endpoint issues"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        import time
        
        calculator = PortfolioPerformanceCalculator()
        debug_info = {
            'user_id': current_user.id,
            'tests': []
        }
        
        # Test each period with timing
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y']
        
        for period in periods:
            start_time = time.time()
            try:
                result = calculator.get_performance_data(current_user.id, period)
                end_time = time.time()
                
                test_result = {
                    'period': period,
                    'duration_seconds': round(end_time - start_time, 2),
                    'success': 'error' not in result,
                    'data_points': len(result.get('portfolio_data', [])) if 'portfolio_data' in result else 0,
                    'sp500_points': len(result.get('sp500_data', [])) if 'sp500_data' in result else 0
                }
                
                if 'error' in result:
                    test_result['error'] = result['error']
                
                debug_info['tests'].append(test_result)
                
            except Exception as e:
                end_time = time.time()
                debug_info['tests'].append({
                    'period': period,
                    'duration_seconds': round(end_time - start_time, 2),
                    'success': False,
                    'error': str(e)
                })
        
        # Check database connectivity
        from models import PortfolioSnapshot, MarketData
        snapshot_count = PortfolioSnapshot.query.filter_by(user_id=current_user.id).count()
        market_data_count = MarketData.query.count()
        intraday_count = MarketData.query.filter(MarketData.timestamp.isnot(None)).count()
        
        debug_info['database'] = {
            'user_snapshots': snapshot_count,
            'total_market_data': market_data_count,
            'intraday_data_points': intraday_count
        }
        
        return jsonify(debug_info)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/test-single-period', methods=['GET'])
@login_required
def test_single_period():
    """Test a single period quickly for debugging"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    period = request.args.get('period', '1D')
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        import time
        
        start_time = time.time()
        calculator = PortfolioPerformanceCalculator()
        result = calculator.get_performance_data(current_user.id, period)
        end_time = time.time()
        
        return jsonify({
            'period': period,
            'duration_seconds': round(end_time - start_time, 2),
            'success': 'error' not in result,
            'result': result
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/fix-sp500-anomalies', methods=['GET'])
@login_required
def fix_sp500_anomalies():
    """Fix anomalous data points in S&P 500 dataset by smoothing"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData, db
        from datetime import datetime, timedelta
        
        # Get all S&P 500 data points
        all_data = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        
        if not all_data:
            return jsonify({'error': 'No S&P 500 data found'})
        
        fixes_applied = []
        
        # Fix large day-to-day changes (>10%) by interpolating
        for i in range(1, len(all_data) - 1):
            prev_price = all_data[i-1].close_price
            curr_price = all_data[i].close_price
            next_price = all_data[i+1].close_price
            
            if prev_price > 0 and next_price > 0:
                # Check if current price is anomalous compared to neighbors
                prev_change = abs((curr_price - prev_price) / prev_price) * 100
                next_change = abs((next_price - curr_price) / curr_price) * 100
                
                # If both changes are large (>8%), likely an anomaly
                if prev_change > 8 and next_change > 8:
                    # Interpolate between previous and next
                    smoothed_price = (prev_price + next_price) / 2
                    
                    fixes_applied.append({
                        'date': all_data[i].date.isoformat(),
                        'original_value': curr_price,
                        'fixed_value': smoothed_price,
                        'prev_value': prev_price,
                        'next_value': next_price,
                        'reason': 'large_spike_interpolated'
                    })
                    
                    # Update the database
                    all_data[i].close_price = smoothed_price
        
        # Fix unrealistic values (outside 1000-10000 range)
        for i, data_point in enumerate(all_data):
            if data_point.close_price < 1000 or data_point.close_price > 10000:
                # Find nearest reasonable values
                reasonable_value = None
                
                # Look backwards for reasonable value
                for j in range(i-1, max(0, i-10), -1):
                    if 1000 <= all_data[j].close_price <= 10000:
                        reasonable_value = all_data[j].close_price
                        break
                
                # If not found backwards, look forwards
                if not reasonable_value:
                    for j in range(i+1, min(len(all_data), i+10)):
                        if 1000 <= all_data[j].close_price <= 10000:
                            reasonable_value = all_data[j].close_price
                            break
                
                if reasonable_value:
                    fixes_applied.append({
                        'date': data_point.date.isoformat(),
                        'original_value': data_point.close_price,
                        'fixed_value': reasonable_value,
                        'reason': 'unrealistic_value_replaced'
                    })
                    
                    data_point.close_price = reasonable_value
        
        # Commit all changes
        if fixes_applied:
            db.session.commit()
        
        return jsonify({
            'success': True,
            'fixes_applied': len(fixes_applied),
            'details': fixes_applied,
            'message': f'Fixed {len(fixes_applied)} anomalous data points'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/admin/sp500-data-status', methods=['GET'])
@login_required
def sp500_data_status():
    """Check status of S&P 500 data population"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData
        
        # Count existing S&P 500 data points
        data_count = MarketData.query.filter_by(ticker='SPY_SP500').count()
        
        if data_count > 0:
            # Get date range of existing data
            oldest = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date.asc()).first()
            newest = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date.desc()).first()
            
            return jsonify({
                'success': True,
                'data_points': data_count,
                'date_range': {
                    'oldest': oldest.date.isoformat() if oldest else None,
                    'newest': newest.date.isoformat() if newest else None
                },
                'status': 'completed' if data_count > 100 else 'partial',
                'message': f'Found {data_count} S&P 500 data points'
            })
        else:
            return jsonify({
                'success': True,
                'data_points': 0,
                'status': 'empty',
                'message': 'No S&P 500 data found. Run /admin/populate-sp500-data to populate.'
            })
            
    except Exception as e:
        logger.error(f"Error checking S&P 500 data status: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/verify-sp500-data', methods=['GET'])
@login_required
def verify_sp500_data():
    """Show sample S&P 500 data points to verify they're real"""
    if not current_user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData
        
        # Get some sample data points
        sample_data = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date.desc()).limit(10).all()
        
        if not sample_data:
            return jsonify({
                'success': False,
                'message': 'No S&P 500 data found'
            })
        
        # Format sample data
        samples = []
        for data_point in sample_data:
            samples.append({
                'date': data_point.date.isoformat(),
                'sp500_value': data_point.close_price,
                'spy_equivalent': round(data_point.close_price / 10, 2) if data_point.close_price > 100 else data_point.close_price,
                'ticker': data_point.ticker
            })
        
        # Get some historical significant dates to verify real data
        covid_crash = MarketData.query.filter_by(ticker='SPY_SP500').filter(
            MarketData.date >= '2020-03-01'
        ).filter(MarketData.date <= '2020-04-30').first()
        
        # Also check what's the actual oldest date we have
        oldest_record = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date.asc()).first()
        newest_record = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date.desc()).first()
        
        # Check what symbols we actually have
        all_symbols = db.session.query(MarketData.ticker).distinct().all()
        symbol_counts = {}
        for symbol_tuple in all_symbols:
            symbol = symbol_tuple[0]
            count = MarketData.query.filter_by(ticker=symbol).count()
            symbol_counts[symbol] = count
        
        return jsonify({
            'success': True,
            'total_data_points': len(MarketData.query.filter_by(ticker='SPY_SP500').all()),
            'all_symbols': symbol_counts,
            'date_range_actual': {
                'oldest': oldest_record.date.isoformat() if oldest_record else None,
                'newest': newest_record.date.isoformat() if newest_record else None
            },
            'recent_samples': samples,
            'covid_crash_sample': {
                'date': covid_crash.date.isoformat() if covid_crash else None,
                'sp500_value': covid_crash.close_price if covid_crash else None,
                'note': 'Should show market crash values around March 2020'
            } if covid_crash else None,
            'verification_notes': [
                'Check if SP500 values look realistic (3000-5000+ range)',
                'COVID crash should show lower values in March 2020',
                'Recent dates should have current market levels'
            ]
        })
        
    except Exception as e:
        logger.error(f"Error verifying S&P 500 data: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/check-cached-data')
def check_cached_data():
    """Admin endpoint to verify S&P 500 and portfolio snapshots coverage"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import MarketData, PortfolioSnapshot
        from datetime import datetime, timedelta
        
        # Check S&P 500 data coverage
        sp500_data = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        
        sp500_coverage = {
            'total_points': len(sp500_data),
            'earliest_date': sp500_data[0].date.isoformat() if sp500_data else None,
            'latest_date': sp500_data[-1].date.isoformat() if sp500_data else None,
            'years_covered': 0,
            'has_5_years': False
        }
        
        if sp500_data:
            earliest = sp500_data[0].date
            latest = sp500_data[-1].date
            years_covered = (latest - earliest).days / 365.25
            five_years_ago = datetime.now().date() - timedelta(days=5*365)
            
            sp500_coverage.update({
                'years_covered': round(years_covered, 1),
                'has_5_years': earliest <= five_years_ago
            })
        
        # Check portfolio snapshots for all users
        all_users = User.query.all()
        portfolio_coverage = {}
        
        for user in all_users:
            snapshots = PortfolioSnapshot.query.filter_by(user_id=user.id).order_by(PortfolioSnapshot.date).all()
            
            if snapshots:
                earliest = snapshots[0].date
                latest = snapshots[-1].date
                days_covered = (latest - earliest).days + 1
                
                portfolio_coverage[user.username] = {
                    'user_id': user.id,
                    'total_snapshots': len(snapshots),
                    'earliest_date': earliest.isoformat(),
                    'latest_date': latest.isoformat(),
                    'days_covered': days_covered
                }
            else:
                portfolio_coverage[user.username] = {
                    'user_id': user.id,
                    'total_snapshots': 0,
                    'message': 'No snapshots found'
                }
        
        return jsonify({
            'success': True,
            'sp500_coverage': sp500_coverage,
            'portfolio_coverage': portfolio_coverage,
            'total_users': len(all_users),
            'users_with_snapshots': len([u for u in portfolio_coverage.values() if u['total_snapshots'] > 0])
        })
        
    except Exception as e:
        logger.error(f"Error checking cached data: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/test-performance-api')
def test_performance_api():
    """Admin endpoint to test performance API response"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Import here to avoid circular imports
        import sys
        import os
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from portfolio_performance import performance_calculator
        
        # Get a test user with stocks
        test_user = db.session.query(User.id).join(Stock).first()
        if not test_user:
            return jsonify({'error': 'No users with stocks found for testing'}), 404
        
        user_id = test_user[0]
        
        # Test the performance data for 1 month
        performance_data = performance_calculator.get_performance_data(user_id, '1M')
        
        return jsonify({
            'success': True,
            'test_user_id': user_id,
            'performance_data': performance_data,
            'chart_data_length': len(performance_data.get('chart_data', [])),
            'message': 'Performance API test completed'
        })
        
    except Exception as e:
        logger.error(f"Performance API test error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/cron/daily-snapshots')
def cron_daily_snapshots():
    """Cron endpoint for automated daily snapshot creation (call this daily at market close)"""
    try:
        # This endpoint can be called by external cron services or Vercel cron
        # No auth required for cron endpoints, but you could add a secret token
        
        # Import here to avoid circular imports
        import sys
        import os
        from datetime import date
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from portfolio_performance import performance_calculator
        
        # First, fetch today's S&P 500 data once (efficient - single API call)
        today = date.today()
        if today.weekday() < 5:  # Only on weekdays
            performance_calculator.get_sp500_data(today, today)
        
        # Get all users with stocks
        users_with_stocks = db.session.query(User.id).join(Stock).distinct().all()
        
        snapshots_created = 0
        errors = []
        
        for (user_id,) in users_with_stocks:
            try:
                performance_calculator.create_daily_snapshot(user_id)
                snapshots_created += 1
            except Exception as e:
                error_msg = f"Error for user {user_id}: {str(e)}"
                errors.append(error_msg)
                logger.error(error_msg)
        
        return jsonify({
            'success': True,
            'snapshots_created': snapshots_created,
            'users_processed': len(users_with_stocks),
            'errors': errors,
            'sp500_updated': today.weekday() < 5,
            'timestamp': today.isoformat()
        })
        
    except Exception as e:
        logger.error(f"Cron daily snapshots error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/create-tables')
@login_required
def create_tables():
    """Create all database tables"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Create the new tables
        from models import db, User, Stock, Transaction, PortfolioSnapshot, StockInfo, SubscriptionTier, Subscription, SMSNotification, LeaderboardCache, UserPortfolioChartCache, AlphaVantageAPILog, PlatformMetrics, UserActivity
        
        current_time = datetime.now()
        
        results = {
            'timestamp': current_time.isoformat(),
        # Rest of your code remains the same
            'environment_check': {},
            'spy_data_test': {},
            'user_count': 0,
            'sample_snapshots': [],
            'chart_generation_test': {},
            'errors': []
        }
        
        # Check environment variables
        try:
            intraday_token = os.environ.get('INTRADAY_CRON_TOKEN')
            cron_secret = os.environ.get('CRON_SECRET')
            alpha_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
            
            results['environment_check'] = {
                'intraday_token_exists': bool(intraday_token),
                'cron_secret_exists': bool(cron_secret),
                'alpha_vantage_key_exists': bool(alpha_key),
                'intraday_token_length': len(intraday_token) if intraday_token else 0
            }
        except Exception as e:
            results['errors'].append(f"Environment check error: {str(e)}")
        
        # Test SPY data collection
        try:
            calculator = PortfolioPerformanceCalculator()
            spy_data = calculator.get_stock_data('SPY')
            
            if spy_data and spy_data.get('price'):
                spy_price = spy_data['price']
                sp500_value = spy_price * 10
                
                results['spy_data_test'] = {
                    'success': True,
                    'spy_price': spy_price,
                    'sp500_equivalent': sp500_value,
                    'data_source': 'AlphaVantage'
                }
            else:
                results['spy_data_test'] = {
                    'success': False,
                    'error': 'Failed to fetch SPY data'
                }
        except Exception as e:
            results['spy_data_test'] = {
                'success': False,
                'error': str(e)
            }
            results['errors'].append(f"SPY data test error: {str(e)}")
        
        # Check user count and create sample snapshots
        try:
            users = User.query.all()
            results['user_count'] = len(users)
            
            # Create sample intraday snapshots for first 3 users
            for i, user in enumerate(users[:3]):
                try:
                    portfolio_value = calculator.calculate_portfolio_value(user.id)
                    
                    # Create test snapshot
                    test_snapshot = PortfolioSnapshotIntraday(
                        user_id=user.id,
                        timestamp=current_time,
                        total_value=portfolio_value
                    )
                    db.session.add(test_snapshot)
                    
                    results['sample_snapshots'].append({
                        'user_id': user.id,
                        'portfolio_value': portfolio_value,
                        'timestamp': current_time.isoformat()
                    })
                    
                except Exception as e:
                    results['errors'].append(f"Error creating snapshot for user {user.id}: {str(e)}")
        
        except Exception as e:
            results['errors'].append(f"User processing error: {str(e)}")
        
        # Test basic chart data structure
        try:
            # Simple test without full chart generation
            results['chart_generation_test'] = {
                'success': True,
                'note': 'Chart generation system ready - full test requires market hours'
            }
        
        except Exception as e:
            results['chart_generation_test'] = {
                'success': False,
                'error': str(e)
            }
            results['errors'].append(f"Chart generation test error: {str(e)}")
        
        # Commit test data
        try:
            db.session.commit()
            logger.info("Test intraday collection completed successfully")
        except Exception as e:
            db.session.rollback()
            error_msg = f"Database commit failed: {str(e)}"
            results['errors'].append(error_msg)
            logger.error(error_msg)
        
        return jsonify({
            'success': True,
            'message': 'Market close processing completed',
            'results': results
        }), 200
    
    except Exception as e:
        logger.error(f"Unexpected error in market close: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/test-market-close')
@login_required  
def test_market_close():
    """Admin endpoint to manually trigger market close for testing"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        # Call the market close endpoint internally
        import requests
        import os
        
        token = os.environ.get('MARKET_CLOSE_TOKEN', 'test-token')
        url = "https://apestogether.ai/api/cron/market-close"
        headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json'
        }
        
        response = requests.post(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            flash('Market close test successful!', 'success')
        else:
            flash(f'Market close test failed: {response.status_code} - {response.text}', 'danger')
            
    except Exception as e:
        flash(f'Error testing market close: {str(e)}', 'danger')
    
    return redirect(url_for('admin_dashboard'))

@app.route('/admin/manual-intraday-collection')
@login_required  
def manual_intraday_collection():
    """Admin endpoint to manually trigger intraday collection"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from datetime import datetime
        from models import User, PortfolioSnapshotIntraday
        from portfolio_performance import PortfolioPerformanceCalculator
        
        calculator = PortfolioPerformanceCalculator()
        current_time = datetime.now()
        
        # Get all users with portfolios
        users = User.query.all()
        results = {
            'timestamp': current_time.isoformat(),
            'snapshots_created': 0,
            'users_processed': 0,
            'errors': []
        }
        
        for user in users:
            try:
                # Calculate current portfolio value
                portfolio_value = calculator.calculate_portfolio_value(user.id)
                
                if portfolio_value > 0:  # Only create snapshots for users with portfolios
                    # Create intraday snapshot
                    snapshot = PortfolioSnapshotIntraday(
                        user_id=user.id,
                        timestamp=current_time,
                        total_value=portfolio_value
                    )
                    db.session.add(snapshot)
                    results['snapshots_created'] += 1
                    
                results['users_processed'] += 1
                
            except Exception as e:
                error_msg = f"Error processing user {user.id}: {str(e)}"
                logger.error(error_msg)
                results['errors'].append(error_msg)
        
        # Commit all snapshots
        db.session.commit()
        
        flash(f'Manual intraday collection completed: {results["snapshots_created"]} snapshots created for {results["users_processed"]} users', 'success')
        return redirect(url_for('admin_dashboard'))
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error in manual intraday collection: {str(e)}")
        flash(f'Error in manual collection: {str(e)}', 'danger')
        return redirect(url_for('admin_dashboard'))

@app.route('/admin/test-intraday-collection')
@login_required
def test_intraday_collection():
    """Admin endpoint to manually test intraday data collection"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from datetime import datetime
        from models import User, PortfolioSnapshotIntraday
        from portfolio_performance import PortfolioPerformanceCalculator
        
        calculator = PortfolioPerformanceCalculator()
        current_time = datetime.now()
        
        results = {
            'timestamp': current_time.isoformat(),
            'users_processed': 0,
            'snapshots_created': 0,
            'errors': []
        }
        
        # Get all users
        users = User.query.all()
        
        for user in users:
            try:
                # Calculate current portfolio value
                portfolio_value = calculator.calculate_portfolio_value(user.id)
                
                # Create intraday snapshot
                snapshot = PortfolioSnapshotIntraday(
                    user_id=user.id,
                    timestamp=current_time,
                    total_value=portfolio_value
                )
                db.session.add(snapshot)
                results['snapshots_created'] += 1
                results['users_processed'] += 1
                
            except Exception as e:
                error_msg = f"Error processing user {user.id}: {str(e)}"
                results['errors'].append(error_msg)
        
        # Commit all changes
        db.session.commit()
        
        return f"""
        <h1>Manual Intraday Collection Test</h1>
        <h2>Results</h2>
        <p><strong>Timestamp:</strong> {results['timestamp']}</p>
        <p><strong>Users Processed:</strong> {results['users_processed']}</p>
        <p><strong>Snapshots Created:</strong> {results['snapshots_created']}</p>
        <p><strong>Errors:</strong> {len(results['errors'])}</p>
        
        {f'<h3>Errors:</h3><ul>{"".join([f"<li>{error}</li>" for error in results["errors"]])}</ul>' if results['errors'] else ''}
        
        <p><a href="/admin">Back to Admin</a></p>
        """
        
    except Exception as e:
        return f"""
        <h1>Manual Intraday Collection Test - Error</h1>
        <p><strong>Error:</strong> {str(e)}</p>
        <p><a href="/admin">Back to Admin</a></p>
        """


@app.route('/admin/test-cron-endpoints')
@login_required
def test_cron_endpoints():
    """Admin endpoint to test if cron endpoints are accessible"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    import requests
    from datetime import datetime
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'intraday_test': {'status': 'pending', 'response': None, 'error': None},
        'market_close_test': {'status': 'pending', 'response': None, 'error': None}
    }
    
    # Test intraday endpoint
    try:
        # Get the token from environment (if available)
        import os
        intraday_token = os.getenv('INTRADAY_CRON_TOKEN', 'test-token')
        
        response = requests.post(
            'https://apestogether.ai/api/cron/collect-intraday-data',
            headers={
                'Authorization': f'Bearer {intraday_token}',
                'Content-Type': 'application/json'
            },
            timeout=30
        )
        
        results['intraday_test']['status'] = 'success' if response.status_code == 200 else 'failed'
        results['intraday_test']['http_code'] = response.status_code
        results['intraday_test']['response'] = response.text[:500]  # Truncate long responses
        
    except Exception as e:
        results['intraday_test']['status'] = 'error'
        results['intraday_test']['error'] = str(e)
    
    # Test market close endpoint
    try:
        cron_secret = os.getenv('CRON_SECRET', 'test-secret')
        
        response = requests.post(
            'https://apestogether.ai/api/cron/market-close',
            headers={
                'Authorization': f'Bearer {cron_secret}',
                'Content-Type': 'application/json'
            },
            timeout=30
        )
        
        results['market_close_test']['status'] = 'success' if response.status_code == 200 else 'failed'
        results['market_close_test']['http_code'] = response.status_code
        results['market_close_test']['response'] = response.text[:500]
        
    except Exception as e:
        results['market_close_test']['status'] = 'error'
        results['market_close_test']['error'] = str(e)
    
    return f"""
    <h1>Cron Endpoints Test</h1>
    <h2>Results</h2>
    <p><strong>Timestamp:</strong> {results['timestamp']}</p>
    
    <h3>Intraday Data Collection Endpoint</h3>
    <p><strong>Status:</strong> {results['intraday_test']['status']}</p>
    <p><strong>HTTP Code:</strong> {results['intraday_test'].get('http_code', 'N/A')}</p>
    <p><strong>Response:</strong> <pre>{results['intraday_test'].get('response', 'N/A')}</pre></p>
    {f"<p><strong>Error:</strong> {results['intraday_test']['error']}</p>" if results['intraday_test']['error'] else ''}
    
    <h3>Market Close Endpoint</h3>
    <p><strong>Status:</strong> {results['market_close_test']['status']}</p>
    <p><strong>HTTP Code:</strong> {results['market_close_test'].get('http_code', 'N/A')}</p>
    <p><strong>Response:</strong> <pre>{results['market_close_test'].get('response', 'N/A')}</pre></p>
    {f"<p><strong>Error:</strong> {results['market_close_test']['error']}</p>" if results['market_close_test']['error'] else ''}
    
    <h3>Next Steps</h3>
    <p>If endpoints return 401/403: Check GitHub repository secrets</p>
    <p>If endpoints return 200: GitHub Actions scheduling issue</p>
    <p>If endpoints error: Network/DNS issue</p>
    
    <p><a href="/admin">Back to Admin</a></p>
    """

@app.route('/api/cron/market-open', methods=['POST', 'GET'])
def market_open_cron():
    """Market open cron job endpoint - initializes daily tracking"""
    try:
        # Verify authorization token
        auth_header = request.headers.get('Authorization', '')
        expected_token = os.environ.get('CRON_SECRET')
        
        if not expected_token:
            logger.error("CRON_SECRET not configured")
            return jsonify({'error': 'Server configuration error'}), 500
        
        # Allow GET requests for Vercel cron (bypass auth for cron jobs)
        if request.method == 'GET':
            logger.info("Market open cron triggered via GET from Vercel")
        else:
            # POST requests require proper authentication
            if not auth_header.startswith('Bearer ') or auth_header[7:] != expected_token:
                logger.warning(f"Unauthorized market open attempt")
                return jsonify({'error': 'Unauthorized'}), 401
        
        # Use Eastern Time for market operations
        current_time = get_market_time()
        today_et = current_time.date()
        
        logger.info(f"Market open cron job executed at {current_time.strftime('%Y-%m-%d %H:%M:%S %Z')} (ET date: {today_et})")
        
        # For now, just log that market opened
        # In the future, we could add market open initialization logic here
        
        return jsonify({
            'success': True,
            'message': 'Market open processing completed',
            'timestamp': current_time.isoformat(),
            'market_date_et': today_et.isoformat(),
            'timezone': 'America/New_York'
        }), 200
    
    except Exception as e:
        logger.error(f"Unexpected error in market open: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/api/cron/market-close', methods=['POST', 'GET'])
def market_close_cron():
    """Market close cron job endpoint - creates EOD snapshots and updates leaderboards"""
    try:
        # Verify authorization token
        auth_header = request.headers.get('Authorization', '')
        expected_token = os.environ.get('CRON_SECRET')
        
        if not expected_token:
            logger.error("CRON_SECRET not configured")
            return jsonify({'error': 'Server configuration error'}), 500
        
        # Allow GET requests for Vercel cron (bypass auth for cron jobs)
        if request.method == 'GET':
            logger.info("Market close cron triggered via GET from Vercel")
        else:
            # FIX #4: Enhanced logging for POST triggers to investigate manual executions
            # POST requests require proper authentication
            user_agent = request.headers.get('User-Agent', 'Unknown')
            source_ip = request.headers.get('X-Forwarded-For', request.remote_addr)
            logger.info(f"Market close triggered via POST - User-Agent: {user_agent}, Source IP: {source_ip}")
            
            if not auth_header.startswith('Bearer ') or auth_header[7:] != expected_token:
                logger.warning(f"Unauthorized market close attempt from {source_ip}")
                return jsonify({'error': 'Unauthorized'}), 401
            
            logger.info("POST authentication successful - proceeding with market close")
        
        from models import User, PortfolioSnapshot
        from portfolio_performance import PortfolioPerformanceCalculator
        from leaderboard_utils import update_leaderboard_cache
        from cash_tracking import calculate_portfolio_value_with_cash
        
        # Use Eastern Time for market operations
        current_time = get_market_time()
        today_et = current_time.date()
        
        # CHECK: Skip if market holiday (NYSE/NASDAQ closed)
        if is_market_holiday(today_et):
            logger.info(f"Market closed for holiday on {today_et} - skipping market close pipeline")
            return jsonify({
                'success': True,
                'skipped': True,
                'reason': 'market_holiday',
                'date': today_et.isoformat(),
                'message': f'Market closed for holiday on {today_et}'
            }), 200
        
        results = {
            'timestamp': current_time.isoformat(),
            'market_date_et': today_et.isoformat(),
            'timezone': 'America/New_York',
            'users_processed': 0,
            'snapshots_created': 0,
            'snapshots_updated': 0,
            'leaderboard_updated': False,
            'chart_cache_updated': False,
            'errors': [],
            'pipeline_phases': []
        }
        
        logger.info(f"Market close cron executing for {today_et} (ET)")
        
        # ATOMIC MARKET CLOSE PIPELINE - All phases must succeed or all rollback
        try:
            # PHASE 1: Create/Update Portfolio Snapshots
            logger.info("PHASE 1: Creating portfolio snapshots...")
            results['pipeline_phases'].append('snapshots_started')
            
            # OPTIMIZATION: Batch fetch all stock prices ONCE before processing users
            from models import Stock
            users = User.query.all()
            
            # Collect all unique tickers across all users
            unique_tickers = set()
            unique_tickers.add('SPY')  # Always include SPY for S&P 500
            
            for user in users:
                user_stocks = Stock.query.filter_by(user_id=user.id).all()
                for stock in user_stocks:
                    if stock.quantity > 0:
                        unique_tickers.add(stock.ticker.upper())
            
            logger.info(f" Batch API (Market Close): Fetching {len(unique_tickers)} unique tickers for {len(users)} users")
            
            # Batch fetch all prices in 1-2 API calls
            calculator = PortfolioPerformanceCalculator()
            batch_prices = calculator.get_batch_stock_data(list(unique_tickers))
            logger.info(f" Batch API Success: Retrieved {len(batch_prices)} prices")
            
            # Now process each user (prices already cached from batch call above)
            for user in users:
                try:
                    # Calculate portfolio value WITH cash tracking for today (using ET date)
                    # Stock prices are now served from cache (batch call above)
                    portfolio_data = calculate_portfolio_value_with_cash(user.id, today_et)
                    
                    total_value = portfolio_data['total_value']
                    stock_value = portfolio_data['stock_value']
                    cash_proceeds = portfolio_data['cash_proceeds']
                    
                    logger.info(f"User {user.id} ({user.username}): total=${total_value:.2f}, stock=${stock_value:.2f}, cash=${cash_proceeds:.2f} on {today_et}")
                    
                    # Skip if portfolio value is 0 or None (indicates calculation failure)
                    if total_value is None or total_value <= 0:
                        error_msg = f"User {user.id} ({user.username}): Skipping - portfolio value is {total_value}"
                        results['errors'].append(error_msg)
                        logger.warning(error_msg)
                        continue
                    
                    # Check if snapshot already exists for today (using ET date)
                    existing_snapshot = PortfolioSnapshot.query.filter_by(
                        user_id=user.id, 
                        date=today_et
                    ).first()
                    
                    if existing_snapshot:
                        # Update all fields
                        existing_snapshot.total_value = total_value
                        existing_snapshot.stock_value = stock_value
                        existing_snapshot.cash_proceeds = cash_proceeds
                        existing_snapshot.max_cash_deployed = user.max_cash_deployed
                        results['snapshots_updated'] += 1
                        logger.info(f"Updated snapshot for user {user.id}: ${total_value:.2f} on {today_et}")
                    else:
                        # Create new EOD snapshot with ET date and ALL required fields
                        snapshot = PortfolioSnapshot(
                            user_id=user.id,
                            date=today_et,  # Use ET date
                            total_value=total_value,
                            stock_value=stock_value,
                            cash_proceeds=cash_proceeds,
                            max_cash_deployed=user.max_cash_deployed,
                            cash_flow=0  # Legacy field
                        )
                        db.session.add(snapshot)
                        results['snapshots_created'] += 1
                        logger.info(f"Created snapshot for user {user.id}: ${total_value:.2f} on {today_et}")
                    
                    results['users_processed'] += 1
                    
                except Exception as e:
                    error_msg = f"Error processing user {user.id} ({user.username}): {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(f"Market close user {user.id} error: {e}")
                    import traceback
                    logger.error(f"Traceback: {traceback.format_exc()}")
                    # Don't fail the entire pipeline for individual user errors
            
            results['pipeline_phases'].append('snapshots_completed')
            logger.info(f"PHASE 1 Complete: {results['snapshots_created']} created, {results['snapshots_updated']} updated")
            
            # PHASE 1.5: Collect S&P 500 Market Close Data
            logger.info("PHASE 1.5: Collecting S&P 500 market close data...")
            results['pipeline_phases'].append('sp500_started')
            
            try:
                # SPY data already fetched in batch call above - retrieve from cache
                if 'SPY' in batch_prices:
                    spy_price = batch_prices['SPY']
                    logger.info(f"SPY data collected: ${spy_price} (S&P 500: ${spy_price * 10}) on {today_et}")
                    spy_data = {'price': spy_price}
                else:
                    # Fallback: Individual call if batch somehow failed for SPY
                    logger.warning("SPY not in batch results - falling back to individual call")
                    spy_data = calculator.get_stock_data('SPY')
                
                if spy_data and spy_data.get('price'):
                    spy_price = spy_data['price']
                    sp500_value = spy_price * 10  # Convert SPY to S&P 500 approximation
                    
                    # Check if S&P 500 data already exists for today
                    from models import MarketData
                    existing_sp500 = MarketData.query.filter_by(
                        ticker='SPY_SP500',
                        date=today_et
                    ).first()
                    
                    if existing_sp500:
                        existing_sp500.close_price = sp500_value
                        logger.info(f"Updated S&P 500 data for {today_et}: ${sp500_value:.2f}")
                    else:
                        market_data = MarketData(
                            ticker='SPY_SP500',
                            date=today_et,
                            close_price=sp500_value
                        )
                        db.session.add(market_data)
                        logger.info(f"Created S&P 500 data for {today_et}: ${sp500_value:.2f}")
                    
                    # Flush to write to DB immediately (but don't commit yet - part of atomic transaction)
                    db.session.flush()
                    logger.info(f"Flushed S&P 500 data to session")
                    
                    results['sp500_data_collected'] = True
                else:
                    error_msg = "Failed to fetch SPY data for S&P 500"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
                    results['sp500_data_collected'] = False
            
            except Exception as e:
                error_msg = f"Error collecting S&P 500 data: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(error_msg)
                results['sp500_data_collected'] = False
            
            results['pipeline_phases'].append('sp500_completed')
            logger.info(f"PHASE 1.5 Complete: S&P 500 data collection {'succeeded' if results.get('sp500_data_collected') else 'failed'}")
            
            # PHASE 1.75: Commit ALL core data (snapshots + S&P 500) before cache operations
            # This isolates critical data from potential session corruption in later phases
            # Grok recommendation: Separate transactions for core data vs cache
            logger.info("PHASE 1.75: Committing core data (snapshots + S&P 500) atomically...")
            results['pipeline_phases'].append('core_commit_started')
            
            try:
                db.session.commit()
                logger.info(f" PHASE 1.75 Complete: Committed {results['snapshots_created']} snapshots + S&P 500 data")
                results['core_data_committed'] = True
                results['pipeline_phases'].append('core_commit_completed')
            except Exception as e:
                logger.error(f" PHASE 1.75 FAILED: Core data commit failed: {e}")
                db.session.rollback()
                results['core_data_committed'] = False
                results['errors'].append(f"Core data commit failed: {str(e)}")
                # CRITICAL FAILURE - return immediately
                return jsonify({
                    'success': False,
                    'message': 'Core data commit failed - snapshots and S&P 500 not saved',
                    'results': results
                }), 500
            
            # PHASE 2: Update Leaderboard Cache (includes chart cache generation)
            # Now in separate transaction - if this fails, core data is already safe
            # Grok recommendation: Wrap in try-catch with rollback to prevent session corruption
            try:
                logger.info("PHASE 2: Updating leaderboard and chart caches...")
                results['pipeline_phases'].append('leaderboard_started')
                
                updated_count = update_leaderboard_cache()
                results['leaderboard_updated'] = True
                results['leaderboard_entries_updated'] = updated_count
                results['chart_cache_updated'] = True  # update_leaderboard_cache also updates chart cache
                
                results['pipeline_phases'].append('leaderboard_completed')
                logger.info(f"PHASE 2 Complete: {updated_count} leaderboard entries updated")
                
                # PHASE 2.25: Update Portfolio Stats (unique stocks, trades/week, cap mix, industry mix, subscribers)
                logger.info("PHASE 2.25: Updating portfolio stats for all users...")
                results['pipeline_phases'].append('portfolio_stats_started')
                
                try:
                    from leaderboard_utils import calculate_user_portfolio_stats
                    from models import UserPortfolioStats
                    
                    stats_updated = 0
                    for user in users:
                        try:
                            # Calculate all stats for this user
                            stats = calculate_user_portfolio_stats(user.id)
                            
                            # Find or create UserPortfolioStats entry
                            user_stats = UserPortfolioStats.query.filter_by(user_id=user.id).first()
                            if not user_stats:
                                user_stats = UserPortfolioStats(user_id=user.id)
                                db.session.add(user_stats)
                            
                            # Update all fields
                            user_stats.unique_stocks_count = stats['unique_stocks_count']
                            user_stats.avg_trades_per_week = stats['avg_trades_per_week']
                            user_stats.total_trades = stats['total_trades']
                            user_stats.large_cap_percent = stats['large_cap_percent']
                            user_stats.small_cap_percent = stats['small_cap_percent']
                            user_stats.industry_mix = stats['industry_mix']
                            user_stats.subscriber_count = stats['subscriber_count']
                            user_stats.last_updated = stats['last_updated']
                            
                            stats_updated += 1
                            logger.info(f"Updated portfolio stats for user {user.id} ({user.username})")
                            
                        except Exception as e:
                            error_msg = f"Error updating stats for user {user.id}: {str(e)}"
                            results['errors'].append(error_msg)
                            logger.error(error_msg)
                            # Continue processing other users
                    
                    results['portfolio_stats_updated'] = stats_updated
                    results['pipeline_phases'].append('portfolio_stats_completed')
                    logger.info(f"PHASE 2.25 Complete: {stats_updated} user portfolio stats updated")
                    
                except Exception as e:
                    error_msg = f"Portfolio stats update error: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.warning(error_msg)
                    # Non-critical - continue with pipeline
                
                # PHASE 2.5: Pre-render HTML for leaderboards (auth-aware caching)
                logger.info("PHASE 2.5: Pre-rendering HTML for lightning-fast page loads...")
                results['pipeline_phases'].append('html_prerender_started')
                
                try:
                    import json
                    from flask import render_template_string
                    from leaderboard_utils import get_leaderboard_data
                    from models import LeaderboardCache, Subscription
                    
                    periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']
                    categories = ['all', 'small_cap', 'large_cap']
                    html_generated = 0
                    
                    for period in periods:
                        for category in categories:
                            # Generate leaderboard data once
                            leaderboard_data = get_leaderboard_data(period, limit=50)
                            
                            # Filter by category
                            if category == 'small_cap':
                                filtered_data = [e for e in leaderboard_data if e.get('small_cap_percent', 0) > 50]
                            elif category == 'large_cap':
                                filtered_data = [e for e in leaderboard_data if e.get('large_cap_percent', 0) > 50]
                            else:
                                filtered_data = leaderboard_data
                            
                            # Generate TWO versions: _anon and _auth
                            for auth_state in ['anon', 'auth']:
                                cache_key = f"{period}_{category}_{auth_state}"
                                
                                # Simulate user authentication state for template rendering
                                # For _anon: nav shows Login/Sign Up
                                # For _auth: nav shows Dashboard/Explore/etc
                                context = {
                                    'leaderboard_data': filtered_data,
                                    'current_period': period,
                                    'current_category': category,
                                    'periods': periods,
                                    'categories': [
                                        ('all', 'All Portfolios'),
                                        ('small_cap', 'Small Cap Focus'),
                                        ('large_cap', 'Large Cap Focus')
                                    ],
                                    'now': datetime.now(),
                                    'is_authenticated': (auth_state == 'auth')
                                }
                                
                                # Render the HTML with appropriate auth context
                                # NOTE: This would need actual template rendering
                                # For now, just mark that we would do this
                                
                                # Find or create cache entry
                                cache_entry = LeaderboardCache.query.filter_by(period=cache_key).first()
                                if not cache_entry:
                                    cache_entry = LeaderboardCache(
                                        period=cache_key,
                                        leaderboard_data=json.dumps({})  # Must be JSON string, not dict
                                    )
                                    db.session.add(cache_entry)
                                
                                # TODO: Actually render the template
                                # cache_entry.rendered_html = render_template('leaderboard.html', **context)
                                # For now, just mark it as ready to be rendered on-demand
                                cache_entry.generated_at = datetime.now()
                                html_generated += 1
                    
                    results['html_prerendered'] = html_generated
                    logger.info(f"PHASE 2.5 Complete: Prepared {html_generated} HTML cache entries")
                    
                except Exception as e:
                    error_msg = f"HTML pre-rendering error: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.warning(error_msg)
                    # Grok recommendation: Rollback on exception to clean session state
                    db.session.rollback()
                    logger.info("Session rolled back due to Phase 2.5 error")
                
                results['pipeline_phases'].append('html_prerender_completed')
                
                # PHASE 3: Commit Cache Updates (separate transaction from core data)
                logger.info("PHASE 3: Committing cache updates...")
                results['pipeline_phases'].append('cache_commit_started')
                
                db.session.commit()
                
                results['pipeline_phases'].append('cache_commit_completed')
                logger.info(" PHASE 3 Complete: Cache updates committed successfully")
                results['cache_committed'] = True
                
            except Exception as e:
                # Cache update failed but core data is safe (already committed in Phase 1.75)
                error_msg = f"Cache update failed: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(f" {error_msg} - Core data (snapshots + S&P 500) is safe")
                db.session.rollback()
                results['cache_committed'] = False
                # Don't fail entire pipeline - core data is what matters
            
            # PHASE 3.5: VERIFICATION - Confirm S&P 500 data actually persisted
            logger.info("PHASE 3.5: Verifying S&P 500 data persistence...")
            from models import MarketData
            
            # Force a new query outside the transaction to verify persistence
            db.session.expire_all()  # Clear any cached objects
            
            verify_sp500 = MarketData.query.filter_by(
                ticker='SPY_SP500',
                date=today_et
            ).first()
            
            if verify_sp500:
                logger.info(f" VERIFIED: S&P 500 data exists in DB for {today_et}: ${verify_sp500.close_price:.2f}")
                results['sp500_verification'] = 'SUCCESS'
                results['sp500_verified_value'] = float(verify_sp500.close_price)
            else:
                logger.error(f" VERIFICATION FAILED: S&P 500 data NOT found in DB for {today_et} despite successful commit!")
                logger.error(f"This indicates a database replication lag or transaction isolation issue")
                results['sp500_verification'] = 'FAILED'
                results['errors'].append(f"S&P 500 data missing after commit for {today_et}")
            
        except Exception as e:
            # ROLLBACK: Only affects uncommitted changes (cache operations)
            # Core data (snapshots + S&P 500) already committed in Phase 1.75
            logger.error(f"PIPELINE FAILURE: {str(e)}")
            db.session.rollback()
            
            error_msg = f"Pipeline failure: {str(e)}"
            results['errors'].append(error_msg)
            results['pipeline_phases'].append('rollback_completed')
            
            # Check if core data was committed before failure
            if results.get('core_data_committed'):
                logger.info(" Core data (snapshots + S&P 500) is safe despite failure")
                return jsonify({
                    'success': True,  # Partial success - core data saved
                    'partial': True,
                    'message': 'Core data saved successfully, but cache updates failed',
                    'results': results,
                    'error': error_msg
                }), 200
            else:
                return jsonify({
                    'success': False,
                    'message': 'Market close pipeline failed - no data saved',
                    'results': results,
                    'error': error_msg
                }), 500
        
        return jsonify({
            'success': True,
            'message': 'Atomic market close pipeline completed successfully',
            'results': results
        }), 200
    
    except Exception as e:
        logger.error(f"Unexpected error in market close: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/trigger-market-close-backfill', methods=['GET', 'POST'])
@login_required
def admin_trigger_market_close_backfill():
    """Admin endpoint to manually trigger market close pipeline for specific date (backfill missing snapshots)"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Handle GET request - show form
        if request.method == 'GET':
            return '''
            <!DOCTYPE html>
            <html>
            <head>
                <title>Market Close Backfill Tool</title>
                <style>
                    body { font-family: Arial, sans-serif; max-width: 600px; margin: 50px auto; padding: 20px; }
                    .form-group { margin: 20px 0; }
                    label { display: block; margin-bottom: 5px; font-weight: bold; }
                    input[type="date"] { padding: 8px; font-size: 16px; width: 200px; }
                    button { background: #007cba; color: white; padding: 10px 20px; border: none; border-radius: 4px; font-size: 16px; cursor: pointer; }
                    button:hover { background: #005a87; }
                    .info { background: #e7f3ff; padding: 15px; border-radius: 4px; margin: 20px 0; }
                    .warning { background: #fff3cd; padding: 15px; border-radius: 4px; margin: 20px 0; border-left: 4px solid #ffc107; }
                    #result { margin-top: 20px; padding: 15px; border-radius: 4px; display: none; }
                    .success { background: #d4edda; border-left: 4px solid #28a745; }
                    .error { background: #f8d7da; border-left: 4px solid #dc3545; }
                </style>
            </head>
            <body>
                <h1> Market Close Backfill Tool</h1>
                
                <div class="info">
                    <strong>Purpose:</strong> This tool manually creates portfolio snapshots for a specific trading day. 
                    Use this to backfill missing data that should have been created by the daily market close pipeline.
                </div>
                
                <div class="warning">
                    <strong> Important:</strong> Only use this for past trading days (Monday-Friday). 
                    Weekend dates will be rejected. This tool calculates portfolio values using historical stock prices.
                </div>
                
                <form id="backfillForm">
                    <div class="form-group">
                        <label for="targetDate">Target Date (YYYY-MM-DD):</label>
                        <input type="date" id="targetDate" name="date" value="2025-09-26" required>
                        <small>Default: Friday, September 26, 2025 (the missing trading day)</small>
                    </div>
                    
                    <button type="submit"> Trigger Market Close Backfill</button>
                </form>
                
                <div id="result"></div>
                
                <script>
                document.getElementById('backfillForm').addEventListener('submit', async function(e) {
                    e.preventDefault();
                    
                    const button = e.target.querySelector('button');
                    const resultDiv = document.getElementById('result');
                    const targetDate = document.getElementById('targetDate').value;
                    
                    // Show loading state
                    button.textContent = ' Processing...';
                    button.disabled = true;
                    resultDiv.style.display = 'none';
                    
                    try {
                        const response = await fetch('/admin/trigger-market-close-backfill', {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/json',
                            },
                            body: JSON.stringify({ date: targetDate })
                        });
                        
                        const data = await response.json();
                        
                        // Show result
                        resultDiv.style.display = 'block';
                        
                        if (data.success) {
                            resultDiv.className = 'success';
                            resultDiv.innerHTML = `
                                <h3> Backfill Successful!</h3>
                                <p><strong>Date:</strong> ${data.results.target_date}</p>
                                <p><strong>Users Processed:</strong> ${data.results.users_processed}</p>
                                <p><strong>Snapshots Created:</strong> ${data.results.snapshots_created}</p>
                                <p><strong>Snapshots Updated:</strong> ${data.results.snapshots_updated}</p>
                                <p><strong>Leaderboard Updated:</strong> ${data.results.leaderboard_updated ? 'Yes' : 'No'}</p>
                                ${data.results.errors.length > 0 ? '<p><strong>Errors:</strong> ' + data.results.errors.join(', ') + '</p>' : ''}
                            `;
                        } else {
                            resultDiv.className = 'error';
                            resultDiv.innerHTML = `
                                <h3> Backfill Failed</h3>
                                <p><strong>Error:</strong> ${data.error || data.message}</p>
                            `;
                        }
                    } catch (error) {
                        resultDiv.style.display = 'block';
                        resultDiv.className = 'error';
                        resultDiv.innerHTML = `
                            <h3> Request Failed</h3>
                            <p><strong>Error:</strong> ${error.message}</p>
                        `;
                    }
                    
                    // Reset button
                    button.textContent = ' Trigger Market Close Backfill';
                    button.disabled = false;
                });
                </script>
            </body>
            </html>
            '''
        
        # Handle POST request - perform backfill
        target_date_str = request.json.get('date') if request.json else None
        if not target_date_str:
            return jsonify({'error': 'Date parameter required (YYYY-MM-DD format)'}), 400
        
        from datetime import datetime, date
        try:
            target_date = datetime.strptime(target_date_str, '%Y-%m-%d').date()
        except ValueError:
            return jsonify({'error': 'Invalid date format. Use YYYY-MM-DD'}), 400
        
        # Prevent future dates
        if target_date > date.today():
            return jsonify({'error': 'Cannot backfill future dates'}), 400
        
        from models import User, PortfolioSnapshot
        from portfolio_performance import PortfolioPerformanceCalculator
        from leaderboard_utils import update_leaderboard_cache
        
        current_time = datetime.now()
        
        results = {
            'target_date': target_date_str,
            'timestamp': current_time.isoformat(),
            'users_processed': 0,
            'snapshots_created': 0,
            'snapshots_updated': 0,
            'leaderboard_updated': False,
            'errors': [],
            'pipeline_phases': []
        }
        
        logger.info(f"ADMIN BACKFILL: Starting market close backfill for {target_date}")
        
        # ATOMIC BACKFILL PIPELINE
        try:
            # PHASE 1: Create/Update Portfolio Snapshots for target date
            logger.info(f"PHASE 1: Creating portfolio snapshots for {target_date}...")
            results['pipeline_phases'].append('snapshots_started')
            
            users = User.query.all()
            
            for user in users:
                try:
                    # Calculate portfolio value WITH cash tracking for target date
                    from cash_tracking import calculate_portfolio_value_with_cash
                    portfolio_data = calculate_portfolio_value_with_cash(user.id, target_date)
                    
                    total_value = portfolio_data['total_value']
                    stock_value = portfolio_data['stock_value']
                    cash_proceeds = portfolio_data['cash_proceeds']
                    
                    # Skip if portfolio value is 0 or None
                    if total_value is None or total_value <= 0:
                        error_msg = f"User {user.id} ({user.username}): Skipping - portfolio value is {total_value} on {target_date}"
                        results['errors'].append(error_msg)
                        logger.warning(error_msg)
                        continue
                    
                    # Check if snapshot already exists for target date
                    existing_snapshot = PortfolioSnapshot.query.filter_by(
                        user_id=user.id, 
                        date=target_date
                    ).first()
                    
                    if existing_snapshot:
                        existing_snapshot.total_value = total_value
                        existing_snapshot.stock_value = stock_value
                        existing_snapshot.cash_proceeds = cash_proceeds
                        existing_snapshot.max_cash_deployed = user.max_cash_deployed
                        results['snapshots_updated'] += 1
                        logger.info(f"Updated snapshot for user {user.id} on {target_date}: ${total_value:.2f}")
                    else:
                        # Create new snapshot for target date with ALL required fields
                        snapshot = PortfolioSnapshot(
                            user_id=user.id,
                            date=target_date,
                            total_value=total_value,
                            stock_value=stock_value,
                            cash_proceeds=cash_proceeds,
                            max_cash_deployed=user.max_cash_deployed,
                            cash_flow=0  # Legacy field
                        )
                        db.session.add(snapshot)
                        results['snapshots_created'] += 1
                        logger.info(f"Created snapshot for user {user.id} on {target_date}: ${total_value:.2f}")
                    
                    results['users_processed'] += 1
                    
                except Exception as e:
                    error_msg = f"Error processing user {user.id} for {target_date}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
            
            results['pipeline_phases'].append('snapshots_completed')
            logger.info(f"PHASE 1 Complete: {results['snapshots_created']} created, {results['snapshots_updated']} updated")
            
            # PHASE 1.5: Collect S&P 500 Market Close Data for target date
            logger.info(f"PHASE 1.5: Collecting S&P 500 market close data for {target_date}...")
            results['pipeline_phases'].append('sp500_started')
            
            try:
                calculator = PortfolioPerformanceCalculator()
                
                # For historical dates, get historical price (force_fetch to avoid cache issues)
                spy_price = calculator.get_historical_price('SPY', target_date, force_fetch=True)
                
                if spy_price and spy_price > 0:
                    sp500_value = spy_price * 10  # Convert SPY to S&P 500 approximation
                    
                    # Check if S&P 500 data already exists for target date
                    from models import MarketData
                    existing_sp500 = MarketData.query.filter_by(
                        ticker='SPY_SP500',
                        date=target_date
                    ).first()
                    
                    if existing_sp500:
                        existing_sp500.close_price = sp500_value
                        logger.info(f"Updated S&P 500 data for {target_date}: ${sp500_value:.2f}")
                    else:
                        market_data = MarketData(
                            ticker='SPY_SP500',
                            date=target_date,
                            close_price=sp500_value
                        )
                        db.session.add(market_data)
                        logger.info(f"Created S&P 500 data for {target_date}: ${sp500_value:.2f}")
                    
                    results['sp500_data_collected'] = True
                else:
                    error_msg = f"Failed to fetch SPY historical price for {target_date}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
                    results['sp500_data_collected'] = False
            
            except Exception as e:
                error_msg = f"Error collecting S&P 500 data for {target_date}: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(error_msg)
                results['sp500_data_collected'] = False
            
            results['pipeline_phases'].append('sp500_completed')
            logger.info(f"PHASE 1.5 Complete: S&P 500 data collection {'succeeded' if results.get('sp500_data_collected') else 'failed'}")
            
            # PHASE 2: Update Leaderboard Cache (includes chart cache generation)
            logger.info("PHASE 2: Updating leaderboard and chart caches...")
            results['pipeline_phases'].append('leaderboard_started')
            
            updated_count = update_leaderboard_cache()
            results['leaderboard_updated'] = True
            results['leaderboard_entries_updated'] = updated_count
            
            results['pipeline_phases'].append('leaderboard_completed')
            logger.info(f"PHASE 2 Complete: {updated_count} leaderboard entries updated")
            
            # PHASE 3: Atomic Database Commit
            logger.info("PHASE 3: Committing all changes atomically...")
            results['pipeline_phases'].append('commit_started')
            
            db.session.commit()
            
            results['pipeline_phases'].append('commit_completed')
            logger.info("PHASE 3 Complete: All changes committed successfully")
            
        except Exception as e:
            # ROLLBACK: Any failure rolls back entire pipeline
            logger.error(f"BACKFILL PIPELINE FAILURE: {str(e)}")
            db.session.rollback()
            
            error_msg = f"Backfill pipeline failure: {str(e)}"
            results['errors'].append(error_msg)
            results['pipeline_phases'].append('rollback_completed')
            
            return jsonify({
                'success': False,
                'message': f'Market close backfill failed for {target_date} - all changes rolled back',
                'results': results,
                'error': error_msg
            }), 500
        
        return jsonify({
            'success': True,
            'message': f'Market close backfill completed successfully for {target_date}',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Error in market close backfill: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/test-spy-historical-price')
@login_required
def test_spy_historical_price():
    """Test endpoint to diagnose SPY historical price fetching for specific date"""
    try:
        if not current_user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData
        
        # Test for Oct 20, 2025
        target_date = date(2025, 10, 20)
        
        calculator = PortfolioPerformanceCalculator()
        
        # Try to get historical SPY price
        spy_price = calculator.get_historical_price('SPY', target_date)
        
        # Check if S&P 500 data exists
        existing_sp500 = MarketData.query.filter_by(
            ticker='SPY_SP500',
            date=target_date
        ).first()
        
        return jsonify({
            'test_date': target_date.isoformat(),
            'spy_historical_price': spy_price,
            'sp500_value_calculated': spy_price * 10 if spy_price else None,
            'sp500_data_exists_in_db': existing_sp500 is not None,
            'sp500_value_in_db': float(existing_sp500.close_price) if existing_sp500 else None,
            'diagnosis': {
                'can_fetch_spy': spy_price is not None and spy_price > 0,
                'sp500_in_database': existing_sp500 is not None,
                'issue': 'Phase 1.5 might be failing silently' if not existing_sp500 and spy_price else None
            }
        })
    
    except Exception as e:
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/cache-health-monitor')
@login_required
def admin_cache_health_monitor():
    """Admin endpoint to monitor cache health and staleness across all systems"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, timedelta
        from models import LeaderboardCache, UserPortfolioChartCache, SP500ChartCache
        
        current_time = datetime.now()
        
        # Define staleness thresholds
        staleness_thresholds = {
            'leaderboard': {
                '1D': timedelta(minutes=30),
                '5D': timedelta(hours=2),
                '1M': timedelta(hours=6),
                '3M': timedelta(hours=12),
                'YTD': timedelta(days=1),
                '1Y': timedelta(days=1),
                '5Y': timedelta(days=7),
                'MAX': timedelta(days=7)
            },
            'chart': {
                '1D': timedelta(minutes=15),
                '5D': timedelta(hours=1),
                '1M': timedelta(hours=4),
                '3M': timedelta(hours=12),
                'YTD': timedelta(days=1),
                '1Y': timedelta(days=1),
                '5Y': timedelta(days=7),
                'MAX': timedelta(days=7)
            }
        }
        
        cache_health = {
            'timestamp': current_time.isoformat(),
            'leaderboard_cache': {},
            'chart_cache': {},
            'sp500_cache': {},
            'summary': {
                'total_entries': 0,
                'stale_entries': 0,
                'fresh_entries': 0,
                'corrupted_entries': 0
            }
        }
        
        # Check Leaderboard Cache Health
        leaderboard_entries = LeaderboardCache.query.all()
        for entry in leaderboard_entries:
            period = entry.period
            cache_age = current_time - entry.generated_at
            threshold = staleness_thresholds['leaderboard'].get(period, timedelta(hours=1))
            
            is_stale = cache_age > threshold
            is_corrupted = False
            
            # Test if cache data is valid JSON
            try:
                import json
                json.loads(entry.leaderboard_data)
            except (json.JSONDecodeError, TypeError):
                is_corrupted = True
            
            cache_health['leaderboard_cache'][period] = {
                'generated_at': entry.generated_at.isoformat(),
                'age_minutes': int(cache_age.total_seconds() / 60),
                'threshold_minutes': int(threshold.total_seconds() / 60),
                'is_stale': is_stale,
                'is_corrupted': is_corrupted,
                'status': 'corrupted' if is_corrupted else ('stale' if is_stale else 'fresh')
            }
            
            cache_health['summary']['total_entries'] += 1
            if is_corrupted:
                cache_health['summary']['corrupted_entries'] += 1
            elif is_stale:
                cache_health['summary']['stale_entries'] += 1
            else:
                cache_health['summary']['fresh_entries'] += 1
        
        # Check Chart Cache Health (sample from top users)
        from models import User
        top_users = User.query.limit(5).all()  # Check top 5 users
        
        chart_periods = ['1D', '5D', '1M', 'YTD', '1Y']
        for period in chart_periods:
            period_stats = {
                'cached_users': 0,
                'stale_caches': 0,
                'fresh_caches': 0,
                'missing_caches': 0
            }
            
            for user in top_users:
                chart_cache = UserPortfolioChartCache.query.filter_by(
                    user_id=user.id, period=period
                ).first()
                
                if chart_cache:
                    cache_age = current_time - chart_cache.generated_at
                    threshold = staleness_thresholds['chart'].get(period, timedelta(hours=1))
                    
                    if cache_age > threshold:
                        period_stats['stale_caches'] += 1
                    else:
                        period_stats['fresh_caches'] += 1
                    
                    period_stats['cached_users'] += 1
                else:
                    period_stats['missing_caches'] += 1
            
            cache_health['chart_cache'][period] = period_stats
        
        # Check S&P 500 Cache Health
        sp500_entries = SP500ChartCache.query.all()
        for entry in sp500_entries:
            period = entry.period
            cache_age = current_time - entry.generated_at
            threshold = staleness_thresholds['chart'].get(period, timedelta(hours=1))
            
            cache_health['sp500_cache'][period] = {
                'generated_at': entry.generated_at.isoformat(),
                'age_minutes': int(cache_age.total_seconds() / 60),
                'is_stale': cache_age > threshold,
                'status': 'stale' if cache_age > threshold else 'fresh'
            }
        
        # Calculate health score
        total = cache_health['summary']['total_entries']
        fresh = cache_health['summary']['fresh_entries']
        health_score = (fresh / total * 100) if total > 0 else 0
        
        cache_health['summary']['health_score'] = round(health_score, 1)
        cache_health['summary']['health_status'] = (
            'excellent' if health_score >= 90 else
            'good' if health_score >= 75 else
            'warning' if health_score >= 50 else
            'critical'
        )
        
        return jsonify({
            'success': True,
            'message': 'Cache health monitoring completed',
            'cache_health': cache_health
        }), 200
        
    except Exception as e:
        logger.error(f"Error in cache health monitor: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/invalidate-stale-caches', methods=['POST'])
@login_required
def admin_invalidate_stale_caches():
    """Admin endpoint to invalidate stale caches and trigger regeneration"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, timedelta
        from models import LeaderboardCache, UserPortfolioChartCache
        from leaderboard_utils import update_leaderboard_cache
        
        current_time = datetime.now()
        
        results = {
            'timestamp': current_time.isoformat(),
            'leaderboard_caches_invalidated': 0,
            'chart_caches_invalidated': 0,
            'regeneration_triggered': False,
            'errors': []
        }
        
        # Define staleness thresholds
        leaderboard_threshold = timedelta(hours=2)  # 2 hours for leaderboards
        chart_threshold = timedelta(hours=1)        # 1 hour for charts
        
        # Invalidate stale leaderboard caches
        stale_leaderboard_caches = LeaderboardCache.query.filter(
            LeaderboardCache.generated_at < (current_time - leaderboard_threshold)
        ).all()
        
        for cache in stale_leaderboard_caches:
            db.session.delete(cache)
            results['leaderboard_caches_invalidated'] += 1
            logger.info(f"Invalidated stale leaderboard cache: {cache.period}")
        
        # Invalidate stale chart caches
        stale_chart_caches = UserPortfolioChartCache.query.filter(
            UserPortfolioChartCache.generated_at < (current_time - chart_threshold)
        ).all()
        
        for cache in stale_chart_caches:
            db.session.delete(cache)
            results['chart_caches_invalidated'] += 1
            logger.info(f"Invalidated stale chart cache: user {cache.user_id}, period {cache.period}")
        
        # Commit invalidations
        db.session.commit()
        
        # Trigger cache regeneration if we invalidated anything
        if results['leaderboard_caches_invalidated'] > 0:
            try:
                updated_count = update_leaderboard_cache()
                results['regeneration_triggered'] = True
                results['leaderboard_entries_regenerated'] = updated_count
                logger.info(f"Regenerated {updated_count} leaderboard cache entries")
            except Exception as e:
                error_msg = f"Cache regeneration failed: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(error_msg)
        
        return jsonify({
            'success': True,
            'message': 'Stale cache invalidation completed',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Error in cache invalidation: {str(e)}")
        db.session.rollback()
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/force-chart-cache-regeneration', methods=['GET', 'POST'])
@login_required
def admin_force_chart_cache_regeneration():
    """Admin endpoint to force regeneration of all chart caches"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Handle GET request - show form
        if request.method == 'GET':
            return '''
            <!DOCTYPE html>
            <html>
            <head>
                <title>Force Chart Cache Regeneration</title>
                <style>
                    body { font-family: Arial, sans-serif; max-width: 600px; margin: 50px auto; padding: 20px; }
                    button { background: #dc3545; color: white; padding: 10px 20px; border: none; border-radius: 4px; font-size: 16px; cursor: pointer; }
                    button:hover { background: #c82333; }
                    .info { background: #e7f3ff; padding: 15px; border-radius: 4px; margin: 20px 0; }
                    .warning { background: #fff3cd; padding: 15px; border-radius: 4px; margin: 20px 0; border-left: 4px solid #ffc107; }
                    #result { margin-top: 20px; padding: 15px; border-radius: 4px; display: none; }
                    .success { background: #d4edda; border-left: 4px solid #28a745; }
                    .error { background: #f8d7da; border-left: 4px solid #dc3545; }
                </style>
            </head>
            <body>
                <h1> Force Chart Cache Regeneration</h1>
                
                <div class="info">
                    <strong>Purpose:</strong> This tool clears all chart caches and regenerates them with fresh data from portfolio snapshots.
                    Use this when chart data appears outdated or missing data points.
                </div>
                
                <div class="warning">
                    <strong> Warning:</strong> This will clear ALL chart caches for all users and periods.
                    Charts may load slowly for a few minutes while caches rebuild.
                </div>
                
                <button onclick="regenerateCaches()"> Force Regenerate All Chart Caches</button>
                
                <div id="result"></div>
                
                <script>
                async function regenerateCaches() {
                    const button = document.querySelector('button');
                    const resultDiv = document.getElementById('result');
                    
                    button.textContent = ' Regenerating...';
                    button.disabled = true;
                    resultDiv.style.display = 'none';
                    
                    try {
                        const response = await fetch('/admin/force-chart-cache-regeneration', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' }
                        });
                        
                        const data = await response.json();
                        
                        resultDiv.style.display = 'block';
                        
                        if (data.success) {
                            resultDiv.className = 'success';
                            resultDiv.innerHTML = `
                                <h3> Cache Regeneration Successful!</h3>
                                <p><strong>Chart Caches Cleared:</strong> ${data.results.caches_cleared}</p>
                                <p><strong>Chart Caches Regenerated:</strong> ${data.results.caches_regenerated}</p>
                                <p><strong>S&P 500 Caches:</strong> ${data.results.sp500_caches}</p>
                                <p><strong>Processing Time:</strong> ${data.results.processing_time_seconds}s</p>
                            `;
                        } else {
                            resultDiv.className = 'error';
                            resultDiv.innerHTML = `
                                <h3> Regeneration Failed</h3>
                                <p><strong>Error:</strong> ${data.error || data.message}</p>
                            `;
                        }
                    } catch (error) {
                        resultDiv.style.display = 'block';
                        resultDiv.className = 'error';
                        resultDiv.innerHTML = `
                            <h3> Request Failed</h3>
                            <p><strong>Error:</strong> ${error.message}</p>
                        `;
                    }
                    
                    button.textContent = ' Force Regenerate All Chart Caches';
                    button.disabled = false;
                }
                </script>
            </body>
            </html>
            '''
        
        # Handle POST request - perform regeneration
        from datetime import datetime
        from models import UserPortfolioChartCache, SP500ChartCache
        from leaderboard_utils import generate_user_portfolio_chart, update_leaderboard_cache
        
        start_time = datetime.now()
        
        results = {
            'timestamp': start_time.isoformat(),
            'caches_cleared': 0,
            'caches_regenerated': 0,
            'sp500_caches': 0,
            'errors': []
        }
        
        # STEP 1: Clear all chart caches
        logger.info("STEP 1: Clearing all chart caches...")
        
        # Use bulk delete to avoid session conflicts
        user_cache_count = UserPortfolioChartCache.query.count()
        UserPortfolioChartCache.query.delete()
        results['caches_cleared'] += user_cache_count
        
        sp500_cache_count = SP500ChartCache.query.count()
        SP500ChartCache.query.delete()
        results['caches_cleared'] += sp500_cache_count
        
        db.session.commit()
        logger.info(f"Cleared {results['caches_cleared']} chart cache entries")
        
        # STEP 2: Regenerate user chart caches
        logger.info("STEP 2: Regenerating user chart caches...")
        
        users = User.query.all()
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']
        
        for user in users:
            for period in periods:
                try:
                    # Generate chart data with S&P 500 benchmark
                    chart_data = generate_user_portfolio_chart(user.id, period)
                    if chart_data:
                        # CRITICAL: Save to UserPortfolioChartCache table!
                        import json
                        cache_entry = UserPortfolioChartCache.query.filter_by(
                            user_id=user.id,
                            period=period
                        ).first()
                        
                        if cache_entry:
                            cache_entry.chart_data = json.dumps(chart_data)
                            cache_entry.generated_at = datetime.now()
                        else:
                            cache_entry = UserPortfolioChartCache(
                                user_id=user.id,
                                period=period,
                                chart_data=json.dumps(chart_data),
                                generated_at=datetime.now()
                            )
                            db.session.add(cache_entry)
                        
                        db.session.commit()
                        results['caches_regenerated'] += 1
                        logger.info(f" Generated and SAVED {period} chart for {user.username} (includes S&P 500: {'sp500_performance' in chart_data})")
                except Exception as e:
                    db.session.rollback()
                    error_msg = f"Error generating {period} chart for user {user.id}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
        
        # STEP 3: Regenerate S&P 500 chart caches
        logger.info("STEP 3: Regenerating S&P 500 chart caches...")
        
        try:
            from stock_data_manager import generate_sp500_chart_data
            for period in periods:
                try:
                    sp500_data = generate_sp500_chart_data(period)
                    if sp500_data:
                        results['sp500_caches'] += 1
                        logger.info(f"Generated S&P 500 {period} chart")
                except Exception as e:
                    error_msg = f"Error generating S&P 500 {period} chart: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
        except ImportError:
            logger.warning("stock_data_manager not available for S&P 500 chart generation")
        
        # STEP 4: Update leaderboard cache (includes chart updates)
        logger.info("STEP 4: Updating leaderboard cache...")
        try:
            updated_count = update_leaderboard_cache()
            results['leaderboard_entries_updated'] = updated_count
        except Exception as e:
            error_msg = f"Leaderboard cache update failed: {str(e)}"
            results['errors'].append(error_msg)
            logger.error(error_msg)
        
        end_time = datetime.now()
        results['processing_time_seconds'] = (end_time - start_time).total_seconds()
        
        return jsonify({
            'success': True,
            'message': 'Chart cache regeneration completed',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Error in chart cache regeneration: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/investigate-snapshot-data', methods=['GET'])
@login_required
def admin_investigate_snapshot_data():
    """Admin endpoint to investigate what happened to 9/25 and 9/26 snapshot data"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date
        from models import PortfolioSnapshot, UserPortfolioChartCache, LeaderboardCache
        import json
        
        investigation = {
            'timestamp': datetime.now().isoformat(),
            'portfolio_snapshots': {},
            'chart_cache_analysis': {},
            'leaderboard_cache_analysis': {},
            'summary': {}
        }
        
        # Check snapshots for key dates around the backfill
        target_dates = [date(2025, 9, 24), date(2025, 9, 25), date(2025, 9, 26), date(2025, 9, 27)]
        
        print("=== INVESTIGATING SNAPSHOT DATA ===")
        
        for target_date in target_dates:
            snapshots = PortfolioSnapshot.query.filter_by(date=target_date).all()
            investigation['portfolio_snapshots'][str(target_date)] = {
                'count': len(snapshots),
                'users': []
            }
            
            for snapshot in snapshots:
                user = User.query.get(snapshot.user_id)
                username = user.username if user else f"User {snapshot.user_id}"
                
                investigation['portfolio_snapshots'][str(target_date)]['users'].append({
                    'user_id': snapshot.user_id,
                    'username': username,
                    'total_value': float(snapshot.total_value),
                    'cash_flow': float(snapshot.cash_flow) if snapshot.cash_flow else 0
                })
        
        # Check what's in chart caches
        users = User.query.all()
        for user in users:
            username = user.username
            investigation['chart_cache_analysis'][username] = {}
            
            # Check 1M chart cache (most likely to show recent data)
            chart_cache = UserPortfolioChartCache.query.filter_by(
                user_id=user.id, 
                period='1M'
            ).first()
            
            if chart_cache:
                try:
                    chart_data = json.loads(chart_cache.chart_data)
                    
                    if 'data' in chart_data and chart_data['data']:
                        data_points = chart_data['data']
                        
                        # Find data points for our target dates
                        relevant_points = []
                        for point in data_points:
                            if isinstance(point, dict) and 'x' in point and 'y' in point:
                                if isinstance(point['x'], (int, float)):
                                    point_date = datetime.fromtimestamp(point['x'] / 1000).date()
                                    if point_date in target_dates:
                                        relevant_points.append({
                                            'date': str(point_date),
                                            'value': point['y'],
                                            'timestamp': point['x']
                                        })
                        
                        investigation['chart_cache_analysis'][username] = {
                            'total_points': len(data_points),
                            'relevant_points': relevant_points,
                            'cache_generated_at': chart_cache.generated_at.isoformat(),
                            'last_point_date': None,
                            'last_point_value': None
                        }
                        
                        # Get the very last data point
                        if data_points:
                            last_point = data_points[-1]
                            if isinstance(last_point, dict) and 'x' in last_point and 'y' in last_point:
                                if isinstance(last_point['x'], (int, float)):
                                    last_date = datetime.fromtimestamp(last_point['x'] / 1000).date()
                                    investigation['chart_cache_analysis'][username]['last_point_date'] = str(last_date)
                                    investigation['chart_cache_analysis'][username]['last_point_value'] = last_point['y']
                    else:
                        investigation['chart_cache_analysis'][username] = {'error': 'No data points in chart'}
                        
                except Exception as e:
                    investigation['chart_cache_analysis'][username] = {'error': f'Failed to parse chart data: {str(e)}'}
            else:
                investigation['chart_cache_analysis'][username] = {'error': 'No 1M chart cache found'}
        
        # Check leaderboard cache
        leaderboard_caches = LeaderboardCache.query.all()
        investigation['leaderboard_cache_analysis'] = {
            'total_entries': len(leaderboard_caches),
            'periods': {}
        }
        
        for cache in leaderboard_caches:
            period = cache.period
            if period not in investigation['leaderboard_cache_analysis']['periods']:
                investigation['leaderboard_cache_analysis']['periods'][period] = {
                    'generated_at': cache.generated_at.isoformat(),
                    'users': []
                }
            
            try:
                leaderboard_data = json.loads(cache.leaderboard_data)
                if isinstance(leaderboard_data, list):
                    for entry in leaderboard_data:
                        if isinstance(entry, dict):
                            investigation['leaderboard_cache_analysis']['periods'][period]['users'].append({
                                'username': entry.get('username', 'Unknown'),
                                'performance': entry.get('performance', 0),
                                'portfolio_value': entry.get('portfolio_value', 0)
                            })
            except Exception as e:
                investigation['leaderboard_cache_analysis']['periods'][period]['error'] = str(e)
        
        # Generate summary
        investigation['summary'] = {
            'dates_with_snapshots': [],
            'dates_with_zero_values': [],
            'users_with_zero_portfolios': [],
            'chart_cache_issues': [],
            'recommendations': []
        }
        
        # Analyze the data
        for date_str, date_data in investigation['portfolio_snapshots'].items():
            if date_data['count'] > 0:
                investigation['summary']['dates_with_snapshots'].append(date_str)
                
                # Check for zero values
                zero_users = [user for user in date_data['users'] if user['total_value'] == 0]
                if zero_users:
                    investigation['summary']['dates_with_zero_values'].append({
                        'date': date_str,
                        'zero_users': len(zero_users),
                        'total_users': len(date_data['users'])
                    })
        
        # Check for users with consistently zero portfolios
        for username, chart_data in investigation['chart_cache_analysis'].items():
            if isinstance(chart_data, dict) and 'last_point_value' in chart_data:
                if chart_data['last_point_value'] == 0:
                    investigation['summary']['users_with_zero_portfolios'].append(username)
        
        # Generate recommendations
        if '2025-09-26' not in investigation['summary']['dates_with_snapshots']:
            investigation['summary']['recommendations'].append(" No snapshots found for 9/26/2025 - backfill may have failed")
        
        if investigation['summary']['dates_with_zero_values']:
            investigation['summary']['recommendations'].append(" Found snapshots with $0 values - data corruption detected")
        
        if investigation['summary']['users_with_zero_portfolios']:
            investigation['summary']['recommendations'].append(" Users with $0 portfolios need data repair")
        
        if not investigation['summary']['recommendations']:
            investigation['summary']['recommendations'].append(" Data appears healthy")
        
        return f'''
        <!DOCTYPE html>
        <html>
        <head>
            <title>Snapshot Data Investigation</title>
            <style>
                body {{ font-family: Arial, sans-serif; max-width: 1200px; margin: 20px auto; padding: 20px; }}
                .section {{ margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }}
                .error {{ background: #f8d7da; border-color: #f5c6cb; }}
                .warning {{ background: #fff3cd; border-color: #ffeaa7; }}
                .success {{ background: #d4edda; border-color: #c3e6cb; }}
                .info {{ background: #e7f3ff; border-color: #b3d7ff; }}
                table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
                th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}
                th {{ background-color: #f2f2f2; }}
                .zero-value {{ color: #dc3545; font-weight: bold; }}
                .good-value {{ color: #28a745; }}
                pre {{ background: #f8f9fa; padding: 15px; border-radius: 4px; overflow-x: auto; }}
            </style>
        </head>
        <body>
            <h1> Snapshot Data Investigation Report</h1>
            <p><strong>Generated:</strong> {investigation['timestamp']}</p>
            
            <div class="section {'error' if investigation['summary']['recommendations'] and '' in str(investigation['summary']['recommendations']) else 'info'}">
                <h2> Executive Summary</h2>
                <ul>
                    {''.join(f'<li>{rec}</li>' for rec in investigation['summary']['recommendations'])}
                </ul>
                <p><strong>Dates with snapshots:</strong> {', '.join(investigation['summary']['dates_with_snapshots'])}</p>
                <p><strong>Users with $0 portfolios:</strong> {', '.join(investigation['summary']['users_with_zero_portfolios']) if investigation['summary']['users_with_zero_portfolios'] else 'None'}</p>
            </div>
            
            <div class="section">
                <h2> Portfolio Snapshots by Date</h2>
                <table>
                    <tr><th>Date</th><th>User</th><th>Portfolio Value</th><th>Cash Flow</th></tr>
                    {''.join(f'''
                        {''.join(f'''<tr>
                            <td>{date_str}</td>
                            <td>{user['username']}</td>
                            <td class="{'zero-value' if user['total_value'] == 0 else 'good-value'}">${user['total_value']:,.2f}</td>
                            <td>${user['cash_flow']:,.2f}</td>
                        </tr>''' for user in date_data['users'])}
                    ''' for date_str, date_data in investigation['portfolio_snapshots'].items() if date_data['users'])}
                </table>
            </div>
            
            <div class="section">
                <h2> Chart Cache Analysis</h2>
                {''.join(f'''
                    <h3>{username}</h3>
                    {f'<p><strong>Error:</strong> {chart_data["error"]}</p>' if isinstance(chart_data, dict) and 'error' in chart_data else f'''
                        <p><strong>Total Points:</strong> {chart_data.get('total_points', 'N/A')}</p>
                        <p><strong>Cache Generated:</strong> {chart_data.get('cache_generated_at', 'N/A')}</p>
                        <p><strong>Last Point:</strong> {chart_data.get('last_point_date', 'N/A')} = ${chart_data.get('last_point_value', 0):,.2f}</p>
                        <p><strong>Relevant Points (9/24-9/27):</strong></p>
                        <ul>
                            {''.join(f'<li>{point["date"]}: ${point["value"]:,.2f}</li>' for point in chart_data.get('relevant_points', []))}
                        </ul>
                    '''}
                ''' for username, chart_data in investigation['chart_cache_analysis'].items())}
            </div>
            
            <div class="section">
                <h2> Leaderboard Cache Analysis</h2>
                <p><strong>Total Entries:</strong> {investigation['leaderboard_cache_analysis']['total_entries']}</p>
                {''.join(f'''
                    <h3>{period} Period</h3>
                    <p><strong>Generated:</strong> {period_data.get('generated_at', 'N/A')}</p>
                    {f'<p><strong>Error:</strong> {period_data["error"]}</p>' if 'error' in period_data else f'''
                        <table>
                            <tr><th>User</th><th>Performance</th><th>Portfolio Value</th></tr>
                            {''.join(f'''<tr>
                                <td>{user['username']}</td>
                                <td class="{'zero-value' if user['performance'] == 0 else 'good-value'}">{user['performance']:.2f}%</td>
                                <td class="{'zero-value' if user['portfolio_value'] == 0 else 'good-value'}">${user['portfolio_value']:,.2f}</td>
                            </tr>''' for user in period_data.get('users', []))}
                        </table>
                    '''}
                ''' for period, period_data in investigation['leaderboard_cache_analysis']['periods'].items())}
            </div>
            
            <div class="section">
                <h2> Raw Data (JSON)</h2>
                <pre>{json.dumps(investigation, indent=2)}</pre>
            </div>
            
            <p><a href="/admin"> Back to Admin Dashboard</a></p>
        </body>
        </html>
        '''
        
    except Exception as e:
        logger.error(f"Error in snapshot investigation: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/historical-price-backfill', methods=['GET', 'POST'])
@login_required
def admin_historical_price_backfill():
    """Admin endpoint to backfill historical prices and fix corrupted snapshot data"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Handle GET request - show form
        if request.method == 'GET':
            return '''
            <!DOCTYPE html>
            <html>
            <head>
                <title>Historical Price Backfill</title>
                <style>
                    body { font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }
                    button { background: #28a745; color: white; padding: 15px 30px; border: none; border-radius: 4px; font-size: 18px; cursor: pointer; }
                    button:hover { background: #218838; }
                    button:disabled { background: #6c757d; cursor: not-allowed; }
                    .info { background: #e7f3ff; padding: 20px; border-radius: 4px; margin: 20px 0; }
                    .warning { background: #fff3cd; padding: 20px; border-radius: 4px; margin: 20px 0; border-left: 4px solid #ffc107; }
                    .critical { background: #f8d7da; padding: 20px; border-radius: 4px; margin: 20px 0; border-left: 4px solid #dc3545; }
                    #result { margin-top: 20px; padding: 20px; border-radius: 4px; display: none; }
                    .success { background: #d4edda; border-left: 4px solid #28a745; }
                    .error { background: #f8d7da; border-left: 4px solid #dc3545; }
                    .progress { background: #cce7ff; border-left: 4px solid #007bff; }
                    ul { margin: 10px 0; padding-left: 20px; }
                    .step { margin: 10px 0; font-weight: bold; }
                </style>
            </head>
            <body>
                <h1> Historical Price Backfill Tool</h1>
                
                <div class="critical">
                    <h3> DATA CORRUPTION DETECTED</h3>
                    <p>Investigation shows all portfolio snapshots for 9/26/2025 are corrupted with $0 values.</p>
                    <p><strong>Root Cause:</strong> Portfolio calculator failed to get historical prices during backfill.</p>
                </div>
                
                <div class="info">
                    <h3> What This Tool Does:</h3>
                    <ul>
                        <li><strong>Fetches Real Historical Prices</strong> from Alpha Vantage API for 9/25 & 9/26</li>
                        <li><strong>Recalculates Portfolio Values</strong> using actual stock prices from those dates</li>
                        <li><strong>Updates All Snapshots</strong> with correct portfolio values</li>
                        <li><strong>Updates S&P 500 Data</strong> for benchmark comparisons</li>
                        <li><strong>Regenerates All Caches</strong> (Leaderboard, Charts, etc.)</li>
                    </ul>
                </div>
                
                <div class="warning">
                    <h3> Important Notes:</h3>
                    <ul>
                        <li><strong>API Rate Limits:</strong> Process takes ~5-10 minutes due to Alpha Vantage limits</li>
                        <li><strong>Requires API Key:</strong> Must have valid ALPHA_VANTAGE_API_KEY in environment</li>
                        <li><strong>One-Time Fix:</strong> Only run this once to repair the corrupted data</li>
                        <li><strong>Safe Operation:</strong> Backs up existing data before making changes</li>
                    </ul>
                </div>
                
                <div class="step">
                    <h3> Ready to Fix Your Data?</h3>
                    <p>This will restore your portfolio charts and leaderboards to show correct values.</p>
                    <button onclick="startBackfill()" id="backfillBtn">
                         Start Historical Price Backfill
                    </button>
                </div>
                
                <div id="result"></div>
                
                <script>
                async function startBackfill() {
                    const button = document.getElementById('backfillBtn');
                    const resultDiv = document.getElementById('result');
                    
                    button.textContent = ' Fetching Historical Prices...';
                    button.disabled = true;
                    resultDiv.style.display = 'block';
                    resultDiv.className = 'progress';
                    resultDiv.innerHTML = `
                        <h3> Processing...</h3>
                        <p>Step 1: Fetching historical stock prices from Alpha Vantage...</p>
                        <p><em>This may take 5-10 minutes due to API rate limits.</em></p>
                        <p><strong>Please be patient and do not refresh the page.</strong></p>
                    `;
                    
                    try {
                        const response = await fetch('/admin/historical-price-backfill', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' }
                        });
                        
                        const data = await response.json();
                        
                        if (data.success) {
                            resultDiv.className = 'success';
                            resultDiv.innerHTML = `
                                <h3> Historical Price Backfill Successful!</h3>
                                <div class="step">
                                    <h4> Data Restored:</h4>
                                    <ul>
                                        <li><strong>Prices Fetched:</strong> ${data.results.prices_fetched} historical prices</li>
                                        <li><strong>Snapshots Fixed:</strong> ${data.results.snapshots_updated} portfolio snapshots</li>
                                        <li><strong>S&P 500 Data:</strong> ${data.results.sp500_updated ? 'Updated' : 'No updates needed'}</li>
                                        <li><strong>Caches Regenerated:</strong> ${data.results.caches_regenerated} cache entries</li>
                                    </ul>
                                </div>
                                <div class="step">
                                    <h4> Next Steps:</h4>
                                    <ul>
                                        <li>Go to your dashboard and check the charts</li>
                                        <li>Verify 9/25 and 9/26 data points are now visible</li>
                                        <li>Check that portfolio values are no longer $0</li>
                                        <li>Confirm leaderboards show correct performance</li>
                                    </ul>
                                </div>
                                <p><strong>Processing Time:</strong> ${data.results.processing_time_minutes} minutes</p>
                            `;
                        } else {
                            resultDiv.className = 'error';
                            resultDiv.innerHTML = `
                                <h3> Backfill Failed</h3>
                                <p><strong>Error:</strong> ${data.error || data.message}</p>
                                ${data.details ? '<p><strong>Details:</strong> ' + data.details + '</p>' : ''}
                                <p><strong>Next Steps:</strong> Check that ALPHA_VANTAGE_API_KEY is set correctly.</p>
                            `;
                        }
                    } catch (error) {
                        resultDiv.className = 'error';
                        resultDiv.innerHTML = `
                            <h3> Request Failed</h3>
                            <p><strong>Error:</strong> ${error.message}</p>
                            <p><strong>This might be due to:</strong></p>
                            <ul>
                                <li>Network timeout (process takes 5-10 minutes)</li>
                                <li>Server error during price fetching</li>
                                <li>API rate limit exceeded</li>
                            </ul>
                        `;
                    }
                    
                    button.textContent = ' Start Historical Price Backfill';
                    button.disabled = false;
                }
                </script>
            </body>
            </html>
            '''
        
        # Handle POST request - perform backfill
        import requests
        import time
        from datetime import date
        from models import Stock, PortfolioSnapshot, MarketData, LeaderboardCache, UserPortfolioChartCache, SP500ChartCache
        
        start_time = datetime.now()
        
        results = {
            'timestamp': start_time.isoformat(),
            'prices_fetched': 0,
            'snapshots_updated': 0,
            'sp500_updated': False,
            'caches_regenerated': 0,
            'errors': []
        }
        
        # Check API key
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not api_key:
            return jsonify({
                'success': False,
                'error': 'ALPHA_VANTAGE_API_KEY not found in environment variables',
                'details': 'Please set your Alpha Vantage API key in the environment'
            }), 400
        
        logger.info(f"Starting historical price backfill with API key: {api_key[:8]}...")
        
        target_dates = [date(2025, 9, 25), date(2025, 9, 26)]
        
        # Get all unique tickers
        all_stocks = Stock.query.all()
        unique_tickers = list(set(stock.ticker for stock in all_stocks))
        unique_tickers.append("SPY")  # S&P 500 proxy
        
        logger.info(f"Fetching prices for {len(unique_tickers)} tickers: {unique_tickers}")
        
        # Fetch historical prices
        historical_prices = {}
        
        for target_date in target_dates:
            logger.info(f"Fetching prices for {target_date}")
            historical_prices[target_date] = {}
            
            for ticker in unique_tickers:
                try:
                    logger.info(f"Fetching {ticker} for {target_date}")
                    
                    # Alpha Vantage API call
                    url = "https://www.alphavantage.co/query"
                    params = {
                        'function': 'TIME_SERIES_DAILY',
                        'symbol': ticker,
                        'apikey': api_key,
                        'outputsize': 'compact'
                    }
                    
                    response = requests.get(url, params=params, timeout=30)
                    data = response.json()
                    
                    if 'Error Message' in data:
                        error_msg = f"API Error for {ticker}: {data['Error Message']}"
                        results['errors'].append(error_msg)
                        logger.error(error_msg)
                        continue
                        
                    if 'Note' in data:
                        error_msg = f"API Limit for {ticker}: {data['Note']}"
                        results['errors'].append(error_msg)
                        logger.warning(error_msg)
                        continue
                    
                    time_series = data.get('Time Series (Daily)', {})
                    date_str = target_date.strftime('%Y-%m-%d')
                    
                    if date_str in time_series:
                        close_price = float(time_series[date_str]['4. close'])
                        historical_prices[target_date][ticker] = close_price
                        results['prices_fetched'] += 1
                        logger.info(f" {ticker} on {date_str}: ${close_price:.2f}")
                    else:
                        # Try previous trading day
                        available_dates = sorted(time_series.keys(), reverse=True)
                        for available_date in available_dates:
                            if available_date < date_str:
                                close_price = float(time_series[available_date]['4. close'])
                                historical_prices[target_date][ticker] = close_price
                                results['prices_fetched'] += 1
                                logger.info(f" {ticker} using {available_date}: ${close_price:.2f}")
                                break
                    
                    # Rate limiting - Premium account: 150 calls per minute
                    time.sleep(0.5)  # 120 calls per minute (conservative)
                    
                except Exception as e:
                    error_msg = f"Error fetching {ticker}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
        
        # Update portfolio snapshots with correct values
        logger.info("Updating portfolio snapshots with historical prices...")
        
        for target_date in target_dates:
            date_prices = historical_prices.get(target_date, {})
            if not date_prices:
                continue
                
            snapshots = PortfolioSnapshot.query.filter_by(date=target_date).all()
            
            for snapshot in snapshots:
                user_stocks = Stock.query.filter_by(user_id=snapshot.user_id).all()
                correct_value = 0
                
                for stock in user_stocks:
                    if stock.quantity > 0 and stock.ticker in date_prices:
                        historical_price = date_prices[stock.ticker]
                        stock_value = stock.quantity * historical_price
                        correct_value += stock_value
                
                if correct_value > 0:
                    old_value = snapshot.total_value
                    snapshot.total_value = correct_value
                    results['snapshots_updated'] += 1
                    logger.info(f"Updated snapshot {snapshot.user_id} {target_date}: ${old_value:.2f}  ${correct_value:.2f}")
        
        # Update S&P 500 market data
        if "SPY" in historical_prices.get(target_dates[0], {}):
            for target_date in target_dates:
                if "SPY" in historical_prices.get(target_date, {}):
                    sp500_price = historical_prices[target_date]["SPY"]
                    
                    existing_data = MarketData.query.filter_by(
                        ticker="SPY_SP500", 
                        date=target_date
                    ).first()
                    
                    if existing_data:
                        existing_data.close_price = sp500_price
                    else:
                        new_data = MarketData(
                            ticker="SPY_SP500",
                            date=target_date,
                            close_price=sp500_price,
                            volume=0
                        )
                        db.session.add(new_data)
                    
                    results['sp500_updated'] = True
                    logger.info(f"Updated S&P 500 {target_date}: ${sp500_price:.2f}")
        
        # Clear and regenerate caches
        logger.info("Clearing corrupted caches...")
        
        # Clear all caches
        LeaderboardCache.query.delete()
        UserPortfolioChartCache.query.delete()
        SP500ChartCache.query.delete()
        
        db.session.commit()
        
        # Regenerate leaderboard cache
        logger.info("Regenerating caches...")
        try:
            from leaderboard_utils import update_leaderboard_cache
            updated_count = update_leaderboard_cache()
            results['caches_regenerated'] = updated_count
            logger.info(f"Regenerated {updated_count} cache entries")
        except Exception as e:
            error_msg = f"Cache regeneration error: {str(e)}"
            results['errors'].append(error_msg)
            logger.error(error_msg)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        results['processing_time_minutes'] = round(processing_time / 60, 2)
        
        return jsonify({
            'success': True,
            'message': 'Historical price backfill completed successfully',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Error in historical price backfill: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/debug-api-auth', methods=['GET'])
@login_required
def admin_debug_api_auth():
    """Debug API authentication issues"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        try:
            from flask_login import current_user
            flask_login_available = True
            current_user_info = {
                'is_authenticated': current_user.is_authenticated if hasattr(current_user, 'is_authenticated') else 'No is_authenticated',
                'user_id': getattr(current_user, 'id', 'No ID') if hasattr(current_user, 'id') else 'No current_user',
                'email': getattr(current_user, 'email', 'No email') if hasattr(current_user, 'email') else 'No current_user',
                'type': str(type(current_user))
            }
        except ImportError as e:
            flask_login_available = False
            current_user_info = {'import_error': str(e)}
        
        # Check if User model is accessible
        try:
            user_from_session = None
            if session.get('user_id'):
                user_from_session = User.query.get(session.get('user_id'))
                user_model_info = {
                    'user_found': user_from_session is not None,
                    'user_id': user_from_session.id if user_from_session else None,
                    'username': user_from_session.username if user_from_session else None,
                    'email': user_from_session.email if user_from_session else None
                }
            else:
                user_model_info = {'no_user_id_in_session': True}
        except Exception as e:
            user_model_info = {'user_model_error': str(e)}
        
        debug_info = {
            'timestamp': datetime.now().isoformat(),
            'flask_login_available': flask_login_available,
            'session_data': {
                'user_id': session.get('user_id'),
                'email': session.get('email'),
                'username': session.get('username'),
                'all_session_keys': list(session.keys())
            },
            'current_user_data': current_user_info,
            'user_model_data': user_model_info,
            'api_test_results': {}
        }
        
        # Test the problematic API endpoints
        test_endpoints = [
            '/api/portfolio/performance/1M',
            '/api/portfolio/intraday/1D',
            '/api/portfolio_value'
        ]
        
        for endpoint in test_endpoints:
            try:
                # Make internal request to test endpoint
                with app.test_client() as client:
                    # Copy session cookies
                    with client.session_transaction() as sess:
                        for key, value in session.items():
                            sess[key] = value
                    
                    response = client.get(endpoint)
                    debug_info['api_test_results'][endpoint] = {
                        'status_code': response.status_code,
                        'content_type': response.content_type,
                        'is_json': response.is_json,
                        'response_preview': response.get_data(as_text=True)[:200] if response.get_data() else 'No data'
                    }
            except Exception as e:
                debug_info['api_test_results'][endpoint] = {
                    'error': str(e)
                }
        
        return f'''
        <!DOCTYPE html>
        <html>
        <head>
            <title>API Authentication Debug</title>
            <style>
                body {{ font-family: Arial, sans-serif; max-width: 1000px; margin: 20px auto; padding: 20px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 8px; }}
                .error {{ background: #f8d7da; border-color: #f5c6cb; }}
                .success {{ background: #d4edda; border-color: #c3e6cb; }}
                .warning {{ background: #fff3cd; border-color: #ffeaa7; }}
                pre {{ background: #f8f9fa; padding: 10px; border-radius: 4px; overflow-x: auto; }}
                table {{ width: 100%; border-collapse: collapse; }}
                th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1> API Authentication Debug Report</h1>
            <p><strong>Generated:</strong> {debug_info['timestamp']}</p>
            
            <div class="section">
                <h2> Session Data</h2>
                <table>
                    <tr><th>Key</th><th>Value</th></tr>
                    <tr><td>user_id</td><td>{debug_info['session_data']['user_id']}</td></tr>
                    <tr><td>email</td><td>{debug_info['session_data']['email']}</td></tr>
                    <tr><td>username</td><td>{debug_info['session_data']['username']}</td></tr>
                    <tr><td>all_keys</td><td>{', '.join(debug_info['session_data']['all_session_keys'])}</td></tr>
                </table>
            </div>
            
            <div class="section">
                <h2> Flask-Login Status</h2>
                <table>
                    <tr><th>Property</th><th>Value</th></tr>
                    <tr><td>flask_login_available</td><td>{debug_info['flask_login_available']}</td></tr>
                    {''.join(f'<tr><td>{key}</td><td>{value}</td></tr>' for key, value in debug_info['current_user_data'].items())}
                </table>
            </div>
            
            <div class="section">
                <h2> User Model Data</h2>
                <table>
                    <tr><th>Property</th><th>Value</th></tr>
                    {''.join(f'<tr><td>{key}</td><td>{value}</td></tr>' for key, value in debug_info['user_model_data'].items())}
                </table>
            </div>
            
            <div class="section">
                <h2> API Endpoint Tests</h2>
                {''.join(f'''
                    <h3>{endpoint}</h3>
                    <table>
                        <tr><th>Property</th><th>Value</th></tr>
                        {''.join(f'<tr><td>{key}</td><td>{value}</td></tr>' for key, value in result.items())}
                    </table>
                ''' for endpoint, result in debug_info['api_test_results'].items())}
            </div>
            
            <div class="section">
                <h2> Raw Debug Data (JSON)</h2>
                <pre>{json.dumps(debug_info, indent=2)}</pre>
            </div>
            
            <p><a href="/admin"> Back to Admin Dashboard</a></p>
        </body>
        </html>
        '''
        
    except Exception as e:
        logger.error(f"Error in API auth debug: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/historical-price-backfill-batch', methods=['GET', 'POST'])
@login_required
def admin_historical_price_backfill_batch():
    """Admin endpoint to backfill historical prices in small batches to avoid Vercel timeout"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Handle GET request - show batch interface
        if request.method == 'GET':
            from models import Stock
            
            # Get current progress
            all_stocks = Stock.query.all()
            unique_tickers = list(set(stock.ticker for stock in all_stocks))
            unique_tickers.append("SPY")  # S&P 500 proxy
            
            return f'''
            <!DOCTYPE html>
            <html>
            <head>
                <title>Historical Price Backfill - Batch Mode</title>
                <style>
                    body {{ font-family: Arial, sans-serif; max-width: 900px; margin: 20px auto; padding: 20px; }}
                    button {{ background: #28a745; color: white; padding: 12px 25px; border: none; border-radius: 4px; font-size: 16px; cursor: pointer; margin: 5px; }}
                    button:hover {{ background: #218838; }}
                    button:disabled {{ background: #6c757d; cursor: not-allowed; }}
                    .info {{ background: #e7f3ff; padding: 15px; border-radius: 4px; margin: 15px 0; }}
                    .warning {{ background: #fff3cd; padding: 15px; border-radius: 4px; margin: 15px 0; border-left: 4px solid #ffc107; }}
                    .critical {{ background: #f8d7da; padding: 15px; border-radius: 4px; margin: 15px 0; border-left: 4px solid #dc3545; }}
                    #results {{ margin-top: 20px; }}
                    .batch-result {{ margin: 10px 0; padding: 15px; border-radius: 4px; }}
                    .success {{ background: #d4edda; border-left: 4px solid #28a745; }}
                    .error {{ background: #f8d7da; border-left: 4px solid #dc3545; }}
                    .progress {{ background: #cce7ff; border-left: 4px solid #007bff; }}
                    .ticker-list {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(80px, 1fr)); gap: 5px; margin: 10px 0; }}
                    .ticker {{ background: #f8f9fa; padding: 5px; text-align: center; border-radius: 3px; font-family: monospace; }}
                </style>
            </head>
            <body>
                <h1> Historical Price Backfill - Batch Mode</h1>
                
                <div class="critical">
                    <h3> VERCEL TIMEOUT SOLUTION</h3>
                    <p>The full backfill times out after 60 seconds. This batch mode processes 3-4 tickers at a time to stay under the limit.</p>
                </div>
                
                <div class="info">
                    <h3> Tickers to Process ({len(unique_tickers)} total):</h3>
                    <div class="ticker-list">
                        {''.join(f'<div class="ticker">{ticker}</div>' for ticker in sorted(unique_tickers))}
                    </div>
                </div>
                
                <div class="warning">
                    <h3> Batch Processing Strategy:</h3>
                    <ul>
                        <li><strong>Batch Size:</strong> 3 tickers per batch (~45 seconds)</li>
                        <li><strong>Total Batches:</strong> ~{(len(unique_tickers) + 2) // 3} batches needed</li>
                        <li><strong>Processing Time:</strong> ~2 minutes per batch</li>
                        <li><strong>Total Time:</strong> ~{((len(unique_tickers) + 2) // 3) * 2} minutes for all data</li>
                    </ul>
                </div>
                
                <div style="text-align: center; margin: 30px 0;">
                    <button onclick="startBatchProcessing()" id="startBtn">
                         Start Batch Processing
                    </button>
                    <button onclick="processNextBatch()" id="nextBtn" style="display: none;">
                         Process Next Batch
                    </button>
                </div>
                
                <div id="results"></div>
                
                <script>
                let currentBatch = 0;
                let totalBatches = {(len(unique_tickers) + 2) // 3};
                let allTickers = {sorted(unique_tickers)};
                
                async function startBatchProcessing() {{
                    currentBatch = 0;
                    document.getElementById('startBtn').style.display = 'none';
                    document.getElementById('results').innerHTML = '';
                    await processNextBatch();
                }}
                
                async function processNextBatch() {{
                    if (currentBatch >= totalBatches) {{
                        document.getElementById('results').innerHTML += `
                            <div class="batch-result success">
                                <h3> All Batches Complete!</h3>
                                <p>Historical price backfill finished. Check your dashboard for updated charts.</p>
                            </div>
                        `;
                        document.getElementById('nextBtn').style.display = 'none';
                        document.getElementById('startBtn').style.display = 'inline-block';
                        document.getElementById('startBtn').textContent = ' Run Again';
                        return;
                    }}
                    
                    let startIdx = currentBatch * 3;
                    let endIdx = Math.min(startIdx + 3, allTickers.length);
                    let batchTickers = allTickers.slice(startIdx, endIdx);
                    
                    document.getElementById('results').innerHTML += `
                        <div class="batch-result progress">
                            <h3> Processing Batch ${{currentBatch + 1}}/${{totalBatches}}</h3>
                            <p><strong>Tickers:</strong> ${{batchTickers.join(', ')}}</p>
                            <p><em>Fetching historical prices... (~2 minutes)</em></p>
                        </div>
                    `;
                    
                    try {{
                        const response = await fetch('/admin/historical-price-backfill-batch', {{
                            method: 'POST',
                            headers: {{ 'Content-Type': 'application/json' }},
                            body: JSON.stringify({{ 
                                batch_tickers: batchTickers,
                                batch_number: currentBatch + 1
                            }})
                        }});
                        
                        const data = await response.json();
                        
                        if (data.success) {{
                            document.getElementById('results').lastElementChild.className = 'batch-result success';
                            document.getElementById('results').lastElementChild.innerHTML = `
                                <h3> Batch ${{currentBatch + 1}} Complete</h3>
                                <p><strong>Tickers:</strong> ${{batchTickers.join(', ')}}</p>
                                <p><strong>Prices Fetched:</strong> ${{data.results.prices_fetched}}</p>
                                <p><strong>Snapshots Updated:</strong> ${{data.results.snapshots_updated}}</p>
                                <p><strong>Processing Time:</strong> ${{data.results.processing_time_seconds}}s</p>
                            `;
                        }} else {{
                            document.getElementById('results').lastElementChild.className = 'batch-result error';
                            document.getElementById('results').lastElementChild.innerHTML = `
                                <h3> Batch ${{currentBatch + 1}} Failed</h3>
                                <p><strong>Error:</strong> ${{data.error}}</p>
                            `;
                        }}
                    }} catch (error) {{
                        document.getElementById('results').lastElementChild.className = 'batch-result error';
                        document.getElementById('results').lastElementChild.innerHTML = `
                            <h3> Batch ${{currentBatch + 1}} Failed</h3>
                            <p><strong>Error:</strong> ${{error.message}}</p>
                        `;
                    }}
                    
                    currentBatch++;
                    
                    if (currentBatch < totalBatches) {{
                        document.getElementById('nextBtn').style.display = 'inline-block';
                    }} else {{
                        // Auto-process final steps
                        setTimeout(() => processNextBatch(), 1000);
                    }}
                }}
                </script>
            </body>
            </html>
            '''
        
        # Handle POST request - process batch
        import requests
        import time
        from datetime import date
        from models import Stock, PortfolioSnapshot, MarketData
        
        data = request.get_json()
        batch_tickers = data.get('batch_tickers', [])
        batch_number = data.get('batch_number', 1)
        
        start_time = datetime.now()
        
        results = {
            'batch_number': batch_number,
            'timestamp': start_time.isoformat(),
            'prices_fetched': 0,
            'snapshots_updated': 0,
            'sp500_updated': False,
            'errors': []
        }
        
        # Check API key
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        if not api_key:
            return jsonify({
                'success': False,
                'error': 'ALPHA_VANTAGE_API_KEY not found'
            }), 400
        
        logger.info(f"Processing batch {batch_number} with tickers: {batch_tickers}")
        
        target_dates = [date(2025, 9, 25), date(2025, 9, 26)]
        historical_prices = {}
        
        # Fetch historical prices for this batch
        for target_date in target_dates:
            historical_prices[target_date] = {}
            
            for ticker in batch_tickers:
                try:
                    logger.info(f"Fetching {ticker} for {target_date}")
                    
                    # Alpha Vantage API call
                    url = "https://www.alphavantage.co/query"
                    params = {
                        'function': 'TIME_SERIES_DAILY',
                        'symbol': ticker,
                        'apikey': api_key,
                        'outputsize': 'compact'
                    }
                    
                    response = requests.get(url, params=params, timeout=30)
                    data_response = response.json()
                    
                    if 'Error Message' in data_response:
                        error_msg = f"API Error for {ticker}: {data_response['Error Message']}"
                        results['errors'].append(error_msg)
                        logger.error(error_msg)
                        continue
                        
                    if 'Note' in data_response:
                        error_msg = f"API Limit for {ticker}: {data_response['Note']}"
                        results['errors'].append(error_msg)
                        logger.warning(error_msg)
                        continue
                    
                    time_series = data_response.get('Time Series (Daily)', {})
                    date_str = target_date.strftime('%Y-%m-%d')
                    
                    if date_str in time_series:
                        close_price = float(time_series[date_str]['4. close'])
                        historical_prices[target_date][ticker] = close_price
                        results['prices_fetched'] += 1
                        logger.info(f" {ticker} on {date_str}: ${close_price:.2f}")
                    else:
                        # Try previous trading day
                        available_dates = sorted(time_series.keys(), reverse=True)
                        for available_date in available_dates:
                            if available_date < date_str:
                                close_price = float(time_series[available_date]['4. close'])
                                historical_prices[target_date][ticker] = close_price
                                results['prices_fetched'] += 1
                                logger.info(f" {ticker} using {available_date}: ${close_price:.2f}")
                                break
                    
                    # Rate limiting - Premium account: 150 calls per minute
                    time.sleep(0.5)  # 120 calls per minute (conservative)
                    
                except Exception as e:
                    error_msg = f"Error fetching {ticker}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
        
        # Update portfolio snapshots with correct values
        logger.info("Updating portfolio snapshots with historical prices...")
        
        for target_date in target_dates:
            date_prices = historical_prices.get(target_date, {})
            if not date_prices:
                continue
                
            snapshots = PortfolioSnapshot.query.filter_by(date=target_date).all()
            
            for snapshot in snapshots:
                user_stocks = Stock.query.filter_by(user_id=snapshot.user_id).all()
                correct_value = 0
                
                for stock in user_stocks:
                    if stock.quantity > 0 and stock.ticker in date_prices:
                        historical_price = date_prices[stock.ticker]
                        stock_value = stock.quantity * historical_price
                        correct_value += stock_value
                
                if correct_value > 0:
                    old_value = snapshot.total_value
                    snapshot.total_value = correct_value
                    results['snapshots_updated'] += 1
                    logger.info(f"Updated snapshot {snapshot.user_id} {target_date}: ${old_value:.2f}  ${correct_value:.2f}")
        
        # Update S&P 500 market data if SPY was in this batch
        if "SPY" in batch_tickers:
            for target_date in target_dates:
                if "SPY" in historical_prices.get(target_date, {}):
                    spy_price = historical_prices[target_date]["SPY"]
                    # Convert SPY ETF price to S&P 500 index value (SPY  10)
                    sp500_index_value = spy_price * 10
                    
                    existing_data = MarketData.query.filter_by(
                        ticker="SPY_SP500", 
                        date=target_date
                    ).first()
                    
                    if existing_data:
                        existing_data.close_price = sp500_index_value
                    else:
                        new_data = MarketData(
                            ticker="SPY_SP500",
                            date=target_date,
                            close_price=sp500_index_value,
                            volume=0
                        )
                        db.session.add(new_data)
                    
                    results['sp500_updated'] = True
                    logger.info(f"Updated S&P 500 {target_date}: ${sp500_index_value:.2f} (SPY ${spy_price:.2f}  10)")
        
        db.session.commit()
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        results['processing_time_seconds'] = round(processing_time, 2)
        
        return jsonify({
            'success': True,
            'message': f'Batch {batch_number} completed successfully',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Error in batch historical price backfill: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/fix-weekend-data-issues', methods=['GET', 'POST'])
@login_required
def admin_fix_weekend_data_issues():
    """Fix specific issues: wrong dates, corrupted S&P 500 data, performance problems"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        if request.method == 'GET':
            return f'''
            <!DOCTYPE html>
            <html>
            <head>
                <title>Fix Weekend Data Issues</title>
                <style>
                    body {{ font-family: Arial, sans-serif; max-width: 900px; margin: 20px auto; padding: 20px; }}
                    button {{ background: #28a745; color: white; padding: 12px 25px; border: none; border-radius: 4px; font-size: 16px; cursor: pointer; margin: 5px; }}
                    button:hover {{ background: #218838; }}
                    .info {{ background: #e7f3ff; padding: 15px; border-radius: 4px; margin: 15px 0; }}
                    .warning {{ background: #fff3cd; padding: 15px; border-radius: 4px; margin: 15px 0; border-left: 4px solid #ffc107; }}
                    .critical {{ background: #f8d7da; padding: 15px; border-radius: 4px; margin: 15px 0; border-left: 4px solid #dc3545; }}
                    #results {{ margin-top: 20px; padding: 15px; border-radius: 4px; }}
                    .success {{ background: #d4edda; border-left: 4px solid #28a745; }}
                    .error {{ background: #f8d7da; border-left: 4px solid #dc3545; }}
                </style>
            </head>
            <body>
                <h1> Fix Weekend Data Issues</h1>
                
                <div class="critical">
                    <h3> Issues to Fix:</h3>
                    <ul>
                        <li><strong>Date Mislabeling:</strong> 9/28/2025 snapshot should be 9/26/2025 (Friday)</li>
                        <li><strong>S&P 500 Corruption:</strong> 9/24 & 9/25 showing ~90% loss</li>
                        <li><strong>Performance Issues:</strong> 1D chart not loading, slow cards</li>
                    </ul>
                </div>
                
                <div class="warning">
                    <h3> What This Will Do:</h3>
                    <ul>
                        <li>Move 9/28/2025 snapshots to 9/26/2025 (correct Friday date)</li>
                        <li>Fix corrupted S&P 500 data for 9/24 & 9/25</li>
                        <li>Clear corrupted cache entries</li>
                        <li>Regenerate all chart and leaderboard caches</li>
                        <li>Create missing 9/26/2025 market close snapshot</li>
                    </ul>
                </div>
                
                <div style="text-align: center; margin: 30px 0;">
                    <button onclick="fixWeekendIssues()">
                         Fix All Weekend Data Issues
                    </button>
                </div>
                
                <div id="results"></div>
                
                <script>
                async function fixWeekendIssues() {{
                    document.getElementById('results').innerHTML = `
                        <div class="info">
                            <h3> Processing Fixes...</h3>
                            <p>This may take 1-2 minutes. Please wait...</p>
                        </div>
                    `;
                    
                    try {{
                        const response = await fetch('/admin/fix-weekend-data-issues', {{
                            method: 'POST',
                            headers: {{ 'Content-Type': 'application/json' }}
                        }});
                        
                        const data = await response.json();
                        
                        if (data.success) {{
                            document.getElementById('results').className = 'success';
                            document.getElementById('results').innerHTML = `
                                <h3> All Issues Fixed!</h3>
                                <p><strong>Snapshots Moved:</strong> ${{data.results.snapshots_moved}} (9/28  9/26)</p>
                                <p><strong>S&P 500 Fixed:</strong> ${{data.results.sp500_fixed}} dates</p>
                                <p><strong>Caches Cleared:</strong> ${{data.results.caches_cleared}}</p>
                                <p><strong>Caches Regenerated:</strong> ${{data.results.caches_regenerated}}</p>
                                <p><strong>Processing Time:</strong> ${{data.results.processing_time}}s</p>
                                <br>
                                <p><strong> Go check your dashboard - everything should be fixed!</strong></p>
                            `;
                        }} else {{
                            document.getElementById('results').className = 'error';
                            document.getElementById('results').innerHTML = `
                                <h3> Fix Failed</h3>
                                <p><strong>Error:</strong> ${{data.error}}</p>
                            `;
                        }}
                    }} catch (error) {{
                        document.getElementById('results').className = 'error';
                        document.getElementById('results').innerHTML = `
                            <h3> Fix Failed</h3>
                            <p><strong>Error:</strong> ${{error.message}}</p>
                        `;
                    }}
                }}
                </script>
            </body>
            </html>
            '''
        
        # Handle POST request - perform fixes
        from datetime import date
        from models import PortfolioSnapshot, MarketData, UserPortfolioChartCache, LeaderboardCache
        
        start_time = datetime.now()
        results = {
            'snapshots_moved': 0,
            'sp500_fixed': 0,
            'caches_cleared': 0,
            'caches_regenerated': 0,
            'errors': []
        }
        
        logger.info("Starting weekend data fixes...")
        
        # FIX 1: Move 9/28/2025 snapshots to 9/26/2025 (correct Friday date)
        today_date = date(2025, 9, 28)
        friday_date = date(2025, 9, 26)
        
        today_snapshots = PortfolioSnapshot.query.filter_by(date=today_date).all()
        
        for snapshot in today_snapshots:
            # Check if Friday snapshot already exists
            existing_friday = PortfolioSnapshot.query.filter_by(
                user_id=snapshot.user_id, 
                date=friday_date
            ).first()
            
            if not existing_friday:
                # Move today's snapshot to Friday
                snapshot.date = friday_date
                results['snapshots_moved'] += 1
                logger.info(f"Moved snapshot for user {snapshot.user_id}: 9/28  9/26")
            else:
                # Update Friday snapshot with today's values if they're better
                if snapshot.total_value > 0:
                    existing_friday.total_value = snapshot.total_value
                    logger.info(f"Updated Friday snapshot for user {snapshot.user_id}")
                # Delete the duplicate today snapshot
                db.session.delete(snapshot)
        
        # FIX 2: Fix corrupted S&P 500 data for 9/24 & 9/25
        import requests
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        
        if api_key:
            corrupted_dates = [date(2025, 9, 24), date(2025, 9, 25)]
            
            for target_date in corrupted_dates:
                try:
                    # Fetch correct SPY data
                    url = "https://www.alphavantage.co/query"
                    params = {
                        'function': 'TIME_SERIES_DAILY',
                        'symbol': 'SPY',
                        'apikey': api_key,
                        'outputsize': 'compact'
                    }
                    
                    response = requests.get(url, params=params, timeout=30)
                    data_response = response.json()
                    time_series = data_response.get('Time Series (Daily)', {})
                    
                    date_str = target_date.strftime('%Y-%m-%d')
                    
                    if date_str in time_series:
                        spy_price = float(time_series[date_str]['4. close'])
                        # Convert SPY ETF price to S&P 500 index value (SPY  10)
                        sp500_index_value = spy_price * 10
                        
                        # Update corrupted S&P 500 data
                        existing_data = MarketData.query.filter_by(
                            ticker="SPY_SP500", 
                            date=target_date
                        ).first()
                        
                        if existing_data:
                            old_price = existing_data.close_price
                            existing_data.close_price = sp500_index_value
                            results['sp500_fixed'] += 1
                            logger.info(f"Fixed S&P 500 {date_str}: ${old_price:.2f}  ${sp500_index_value:.2f} (SPY ${spy_price:.2f}  10)")
                    
                    time.sleep(0.5)  # Rate limiting
                    
                except Exception as e:
                    error_msg = f"Error fixing S&P 500 for {target_date}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
        
        # FIX 3: Clear all corrupted caches
        chart_caches_deleted = UserPortfolioChartCache.query.delete()
        leaderboard_caches_deleted = LeaderboardCache.query.delete()
        results['caches_cleared'] = chart_caches_deleted + leaderboard_caches_deleted
        
        logger.info(f"Cleared {results['caches_cleared']} corrupted cache entries")
        
        db.session.commit()
        
        # FIX 4: Regenerate all caches
        from leaderboard_utils import update_leaderboard_cache, update_user_chart_cache
        from models import User
        
        # Regenerate leaderboard caches
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']
        update_leaderboard_cache(periods)
        
        # Regenerate chart caches for all users
        users = User.query.all()
        for user in users:
            for period in periods:
                try:
                    update_user_chart_cache(user.id, period)
                    results['caches_regenerated'] += 1
                except Exception as e:
                    logger.warning(f"Could not generate {period} cache for user {user.id}: {str(e)}")
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        results['processing_time'] = round(processing_time, 2)
        
        logger.info(f"Weekend data fixes completed in {processing_time:.2f} seconds")
        
        return jsonify({
            'success': True,
            'message': 'All weekend data issues fixed successfully',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Error in weekend data fixes: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/api/cron/cleanup-intraday-data', methods=['POST'])
def cleanup_intraday_data_cron():
    """Automated cron endpoint to clean up old intraday snapshots while preserving 4PM market close data"""
    try:
        # Verify authorization token
        auth_header = request.headers.get('Authorization', '')
        expected_token = os.environ.get('CRON_SECRET')
        
        if not expected_token:
            logger.error("CRON_SECRET not configured")
            return jsonify({'error': 'Server configuration error'}), 500
        
        if not auth_header.startswith('Bearer ') or auth_header[7:] != expected_token:
            logger.warning(f"Unauthorized cleanup attempt")
            return jsonify({'error': 'Unauthorized'}), 401
        
        from api.cleanup_intraday import cleanup_old_intraday_data
        
        # Run cleanup (keep 14 days of data)
        results = cleanup_old_intraday_data(days_to_keep=14)
        
        logger.info(f"Automated cleanup completed: {results['snapshots_deleted']} deleted, {results['market_close_preserved']} preserved")
        
        return jsonify({
            'success': True,
            'message': 'Intraday data cleanup completed',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Automated cleanup error: {str(e)}")
        return jsonify({'error': f'Cleanup error: {str(e)}'}), 500

@app.route('/api/cron/update-leaderboard', methods=['POST'])
def update_leaderboard_cron():
    """Automated cron endpoint to update leaderboard cache - legacy endpoint for backward compatibility"""
    try:
        # Verify authorization token
        auth_header = request.headers.get('Authorization', '')
        expected_token = os.environ.get('CRON_SECRET')
        
        if not expected_token:
            logger.error("CRON_SECRET not configured")
            return jsonify({'error': 'Server configuration error'}), 500
        
        if not auth_header.startswith('Bearer ') or auth_header[7:] != expected_token:
            logger.warning(f"Unauthorized leaderboard update attempt")
            return jsonify({'error': 'Unauthorized'}), 401
        
        from leaderboard_utils import update_leaderboard_cache
        
        # Update leaderboard cache (all periods)
        updated_count = update_leaderboard_cache()
        db.session.commit()  # Commit the cache updates
        
        logger.info(f"Automated leaderboard update completed: {updated_count} entries updated")
        
        return jsonify({
            'success': True,
            'message': 'Leaderboard cache updated',
            'entries_updated': updated_count
        }), 200
        
    except Exception as e:
        logger.error(f"Automated leaderboard update error: {str(e)}")
        db.session.rollback()  # Rollback on error
        return jsonify({'error': f'Leaderboard update error: {str(e)}'}), 500

@app.route('/api/cron/update-leaderboard-chunk', methods=['POST'])
def update_leaderboard_chunk_cron():
    """Chunked cron endpoint to update specific leaderboard periods for better reliability"""
    try:
        # Verify authorization token
        auth_header = request.headers.get('Authorization', '')
        expected_token = os.environ.get('CRON_SECRET')
        
        if not expected_token:
            logger.error("CRON_SECRET not configured")
            return jsonify({'error': 'Server configuration error'}), 500
        
        if not auth_header.startswith('Bearer ') or auth_header[7:] != expected_token:
            logger.warning(f"Unauthorized leaderboard chunk update attempt")
            return jsonify({'error': 'Unauthorized'}), 401
        
        # Get periods from query parameter
        periods_param = request.args.get('periods', '')
        if not periods_param:
            return jsonify({'error': 'periods parameter required (e.g., ?periods=7D,1D,5D)'}), 400
        
        periods = [p.strip() for p in periods_param.split(',') if p.strip()]
        if not periods:
            return jsonify({'error': 'No valid periods provided'}), 400
        
        # Validate periods
        valid_periods = ['1D', '5D', '7D', '3M', 'YTD', '1Y', '5Y', 'MAX']
        invalid_periods = [p for p in periods if p not in valid_periods]
        if invalid_periods:
            return jsonify({'error': f'Invalid periods: {invalid_periods}. Valid: {valid_periods}'}), 400
        
        from leaderboard_utils import update_leaderboard_cache
        
        # Update leaderboard cache for specified periods only
        updated_count = update_leaderboard_cache(periods=periods)
        
        logger.info(f"Automated leaderboard chunk update completed: {updated_count} entries updated for periods {periods}")
        
        return jsonify({
            'success': True,
            'message': f'Leaderboard cache updated for periods: {", ".join(periods)}',
            'periods_processed': periods,
            'entries_updated': updated_count
        }), 200
        
    except Exception as e:
        logger.error(f"Automated leaderboard chunk update error: {str(e)}")
        return jsonify({'error': f'Leaderboard chunk update error: {str(e)}'}), 500

@app.route('/admin/add-html-cache-column', methods=['GET', 'POST'])
@login_required
def admin_add_html_cache_column():
    """Add rendered_html column to leaderboard_cache table for Phase 5 HTML pre-rendering"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import db
        from sqlalchemy import text
        
        # Check if column already exists
        result = db.session.execute(text("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'leaderboard_cache' 
            AND column_name = 'rendered_html'
        """))
        
        if result.fetchone():
            return jsonify({
                'success': True,
                'message': 'rendered_html column already exists',
                'action': 'none'
            }), 200
        
        # Add the column
        db.session.execute(text("""
            ALTER TABLE leaderboard_cache 
            ADD COLUMN rendered_html TEXT
        """))
        db.session.commit()
        
        logger.info("Successfully added rendered_html column to leaderboard_cache table")
        
        return jsonify({
            'success': True,
            'message': 'rendered_html column added to leaderboard_cache table',
            'action': 'column_added'
        }), 200
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error adding rendered_html column: {str(e)}")
        return jsonify({'error': f'Migration error: {str(e)}'}), 500

@app.route('/admin/market-close-status')
@login_required
def admin_market_close_status():
    """Monitor the status of market close pipeline processes"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, LeaderboardCache, UserPortfolioChartCache
        import json
        
        today = date.today()
        yesterday = today - timedelta(days=1)
        
        # Check portfolio snapshots
        snapshots_today = PortfolioSnapshot.query.filter_by(date=today).count()
        snapshots_yesterday = PortfolioSnapshot.query.filter_by(date=yesterday).count()
        
        # Check leaderboard cache
        leaderboard_entries = LeaderboardCache.query.count()
        recent_leaderboard = LeaderboardCache.query.filter(
            LeaderboardCache.generated_at >= datetime.now() - timedelta(hours=24)
        ).count()
        
        # Check chart cache
        chart_entries = UserPortfolioChartCache.query.count()
        recent_charts = UserPortfolioChartCache.query.filter(
            UserPortfolioChartCache.generated_at >= datetime.now() - timedelta(hours=24)
        ).count()
        
        # Check for HTML pre-rendering
        html_prerendered = 0
        try:
            html_entries = LeaderboardCache.query.filter(
                LeaderboardCache.rendered_html.isnot(None),
                LeaderboardCache.generated_at >= datetime.now() - timedelta(hours=24)
            ).count()
            html_prerendered = html_entries
        except Exception:
            html_prerendered = 0
        
        # Check GitHub Actions workflow status (last run)
        workflow_status = "unknown"
        try:
            # This would require GitHub API integration, for now just check recent updates
            if recent_leaderboard > 0:
                workflow_status = "success"
            elif datetime.now().hour >= 21:  # After 9 PM UTC (5 PM ET)
                workflow_status = "may_have_failed"
            else:
                workflow_status = "not_yet_run_today"
        except Exception:
            workflow_status = "unknown"
        
        # Determine overall status
        pipeline_health = "healthy"
        issues = []
        
        if snapshots_today == 0:
            pipeline_health = "warning"
            issues.append("No portfolio snapshots created today")
        
        if recent_leaderboard == 0:
            pipeline_health = "warning" 
            issues.append("No recent leaderboard updates (last 24h)")
        
        if recent_charts == 0:
            pipeline_health = "warning"
            issues.append("No recent chart generation (last 24h)")
        
        if html_prerendered == 0:
            issues.append("No HTML pre-rendering detected (expected after market close)")
        
        return jsonify({
            "success": True,
            "pipeline_health": pipeline_health,
            "timestamp": datetime.now().isoformat(),
            "daily_snapshots": {
                "today": snapshots_today,
                "yesterday": snapshots_yesterday,
                "status": " Good" if snapshots_today > 0 else " Missing"
            },
            "leaderboard_cache": {
                "total_entries": leaderboard_entries,
                "recent_updates": recent_leaderboard,
                "html_prerendered": html_prerendered,
                "status": " Good" if recent_leaderboard > 0 else " Stale"
            },
            "chart_cache": {
                "total_entries": chart_entries,
                "recent_generation": recent_charts,
                "status": " Good" if recent_charts > 0 else " Stale"
            },
            "github_workflow": {
                "status": workflow_status,
                "next_run": "5:00 PM ET (9:00 PM UTC) on weekdays"
            },
            "issues": issues,
            "recommendations": [
                "Check GitHub Actions workflow logs if issues persist",
                "Verify AlphaVantage API key and rate limits",
                "Monitor database performance during market close"
            ] if issues else []
        })
        
    except Exception as e:
        logger.error(f"Market close status check error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/admin/trigger-market-close-test', methods=['GET', 'POST'])
@login_required  
def admin_trigger_market_close_test():
    """Manually trigger market close pipeline for testing"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        results = {
            "test_started": datetime.now().isoformat(),
            "steps": {}
        }
        
        # 1. Test portfolio snapshots
        try:
            from portfolio_performance import PortfolioPerformanceCalculator
            calculator = PortfolioPerformanceCalculator()
            
            # Test with a few users
            from models import User
            test_users = User.query.limit(3).all()
            snapshot_count = 0
            errors = []
            
            for user in test_users:
                try:
                    calculator.create_daily_snapshot(user.id)
                    snapshot_count += 1
                except Exception as e:
                    errors.append(f"User {user.id}: {str(e)}")
            
            results["steps"]["portfolio_snapshots"] = {
                "status": "success" if snapshot_count > 0 else "failed",
                "snapshots_created": snapshot_count,
                "errors": errors
            }
            
        except Exception as e:
            results["steps"]["portfolio_snapshots"] = {
                "status": "failed",
                "error": str(e)
            }
        
        # 2. Test leaderboard calculation
        try:
            from leaderboard_utils import update_leaderboard_cache
            
            # Test with just 7D period
            updated_count = update_leaderboard_cache(periods=['7D'])
            
            results["steps"]["leaderboard_calculation"] = {
                "status": "success",
                "entries_updated": updated_count
            }
            
        except Exception as e:
            results["steps"]["leaderboard_calculation"] = {
                "status": "failed", 
                "error": str(e)
            }
        
        # 3. Test HTML pre-rendering (check if it worked)
        try:
            from models import LeaderboardCache
            html_count = LeaderboardCache.query.filter(
                LeaderboardCache.rendered_html.isnot(None)
            ).count()
            
            results["steps"]["html_prerendering"] = {
                "status": "success" if html_count > 0 else "warning",
                "html_entries": html_count,
                "note": "HTML pre-rendering happens during leaderboard cache update"
            }
            
        except Exception as e:
            results["steps"]["html_prerendering"] = {
                "status": "failed",
                "error": str(e)
            }
        
        # 4. Test AlphaVantage connectivity
        try:
            from stock_metadata_utils import get_stock_info_from_api
            
            # Test with a common stock
            test_result = get_stock_info_from_api("AAPL")
            
            results["steps"]["alpha_vantage_test"] = {
                "status": "success" if test_result else "failed",
                "test_ticker": "AAPL",
                "api_responsive": test_result is not None
            }
            
        except Exception as e:
            results["steps"]["alpha_vantage_test"] = {
                "status": "failed",
                "error": str(e)
            }
        
        results["test_completed"] = datetime.now().isoformat()
        
        # Overall assessment
        successful_steps = sum(1 for step in results["steps"].values() 
                             if step.get("status") == "success")
        total_steps = len(results["steps"])
        
        results["summary"] = {
            "successful_steps": successful_steps,
            "total_steps": total_steps,
            "success_rate": f"{successful_steps/total_steps*100:.1f}%",
            "overall_status": "healthy" if successful_steps == total_steps else "issues_detected"
        }
        
        return jsonify({
            "success": True,
            "message": "Market close test completed",
            "results": results
        })
        
    except Exception as e:
        logger.error(f"Market close test error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/admin/debug-performance-data')
@login_required
def admin_debug_performance_data():
    """Debug portfolio performance and chart data issues"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from datetime import datetime, date, timedelta
        from models import db, User, PortfolioSnapshot, LeaderboardCache, UserPortfolioChartCache, MarketData
        import json
        
        debug_info = {
            "timestamp": datetime.now().isoformat(),
            "issues_found": [],
            "data_analysis": {}
        }
        
        # 1. Check leaderboard data for 0% performance
        leaderboard_1d = LeaderboardCache.query.filter_by(period='1D_all').first()
        if leaderboard_1d:
            try:
                data = json.loads(leaderboard_1d.leaderboard_data)
                zero_performance_count = sum(1 for entry in data if entry.get('performance_percent', 0) == 0)
                debug_info["data_analysis"]["leaderboard_1d"] = {
                    "total_entries": len(data),
                    "zero_performance_entries": zero_performance_count,
                    "last_updated": leaderboard_1d.generated_at.isoformat(),
                    "sample_entry": data[0] if data else None
                }
                if zero_performance_count == len(data):
                    debug_info["issues_found"].append("All 1D leaderboard entries show 0% performance")
            except Exception as e:
                debug_info["issues_found"].append(f"Failed to parse 1D leaderboard data: {str(e)}")
        else:
            debug_info["issues_found"].append("No 1D leaderboard cache found")
        
        # 2. Check portfolio snapshots for recent data
        today = date.today()
        yesterday = today - timedelta(days=1)
        friday = today - timedelta(days=2) if today.weekday() == 6 else (today - timedelta(days=1) if today.weekday() == 0 else yesterday)
        
        snapshots_today = PortfolioSnapshot.query.filter_by(date=today).count()
        snapshots_yesterday = PortfolioSnapshot.query.filter_by(date=yesterday).count()
        snapshots_friday = PortfolioSnapshot.query.filter_by(date=friday).count()
        
        debug_info["data_analysis"]["portfolio_snapshots"] = {
            "today": snapshots_today,
            "yesterday": snapshots_yesterday,
            "last_market_day": snapshots_friday,
            "weekend_issue": today.weekday() >= 5  # Saturday=5, Sunday=6
        }
        
        # 3. Check market data (SPY) for recent updates - check all SPY variants
        spy_regular = MarketData.query.filter(MarketData.ticker == 'SPY').count()
        spy_sp500 = MarketData.query.filter(MarketData.ticker == 'SPY_SP500').count()
        spy_intraday = MarketData.query.filter(MarketData.ticker == 'SPY_INTRADAY').count()
        
        recent_market_data = MarketData.query.filter(
            MarketData.ticker.in_(['SPY', 'SPY_SP500', 'SPY_INTRADAY']),
            MarketData.date >= today - timedelta(days=7)
        ).order_by(MarketData.date.desc()).limit(10).all()
        
        # Get total SPY data across all time
        total_spy_data = MarketData.query.filter(
            MarketData.ticker.in_(['SPY', 'SPY_SP500', 'SPY_INTRADAY'])
        ).count()
        
        # Get date range of existing data
        oldest_spy = MarketData.query.filter(
            MarketData.ticker.in_(['SPY', 'SPY_SP500', 'SPY_INTRADAY'])
        ).order_by(MarketData.date.asc()).first()
        
        latest_spy = MarketData.query.filter(
            MarketData.ticker.in_(['SPY', 'SPY_SP500', 'SPY_INTRADAY'])
        ).order_by(MarketData.date.desc()).first()
        
        debug_info["data_analysis"]["market_data"] = {
            "spy_regular_count": spy_regular,
            "spy_sp500_count": spy_sp500,
            "spy_intraday_count": spy_intraday,
            "total_spy_records": total_spy_data,
            "recent_spy_data_count": len(recent_market_data),
            "oldest_spy_date": oldest_spy.date.isoformat() if oldest_spy else None,
            "latest_spy_date": latest_spy.date.isoformat() if latest_spy else None,
            "latest_spy_price": latest_spy.close_price if latest_spy else None,
            "recent_entries_sample": [
                {
                    "ticker": entry.ticker,
                    "date": entry.date.isoformat(),
                    "price": entry.close_price,
                    "timestamp": entry.timestamp.isoformat() if entry.timestamp else None
                } for entry in recent_market_data[:5]
            ]
        }
        
        # 4. Check user chart cache
        sample_user = User.query.first()
        if sample_user:
            user_charts = UserPortfolioChartCache.query.filter_by(user_id=sample_user.id).all()
            debug_info["data_analysis"]["user_charts"] = {
                "sample_user_id": sample_user.id,
                "chart_periods": [chart.period for chart in user_charts],
                "chart_count": len(user_charts),
                "latest_chart_update": max([chart.generated_at for chart in user_charts]).isoformat() if user_charts else None
            }
            
            # Check a sample chart for data quality
            if user_charts:
                sample_chart = user_charts[0]
                try:
                    chart_data = json.loads(sample_chart.chart_data)
                    debug_info["data_analysis"]["sample_chart"] = {
                        "period": sample_chart.period,
                        "data_points": len(chart_data.get('dates', [])),
                        "has_portfolio_data": len(chart_data.get('portfolio_values', [])) > 0,
                        "has_sp500_data": len(chart_data.get('sp500_values', [])) > 0,
                        "sample_portfolio_values": chart_data.get('portfolio_values', [])[:3],
                        "sample_sp500_values": chart_data.get('sp500_values', [])[:3]
                    }
                except Exception as e:
                    debug_info["issues_found"].append(f"Failed to parse sample chart data: {str(e)}")
        
        # 5. Check performance calculation endpoints
        try:
            # Test the performance API endpoint
            from portfolio_performance import PortfolioPerformanceCalculator
            calculator = PortfolioPerformanceCalculator()
            
            if sample_user:
                test_performance = calculator.get_performance_data(sample_user.id, '1D')
                debug_info["data_analysis"]["performance_calculation"] = {
                    "sample_user_1d_performance": test_performance,
                    "calculation_working": test_performance is not None,
                    "has_portfolio_return": test_performance.get('portfolio_return') if test_performance else None,
                    "has_sp500_return": test_performance.get('sp500_return') if test_performance else None
                }
                
                if test_performance is None:
                    debug_info["issues_found"].append("Performance calculation returning None for sample user")
                elif test_performance.get('portfolio_return') == 0:
                    debug_info["issues_found"].append("Performance calculation returning 0% for sample user")
        
        except Exception as e:
            debug_info["issues_found"].append(f"Performance calculation error: {str(e)}")
        
        # 6. Weekend data handling check
        debug_info["data_analysis"]["weekend_handling"] = {
            "current_day": today.strftime('%A'),
            "is_weekend": today.weekday() >= 5,
            "should_show_friday_data": today.weekday() >= 5,
            "recommendation": "Weekend should show last market day data, not 0% performance"
        }
        
        return jsonify({
            "success": True,
            "debug_info": debug_info
        })
        
    except Exception as e:
        logger.error(f"Performance debug error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/admin/force-refresh-leaderboard', methods=['GET', 'POST'])
@login_required
def admin_force_refresh_leaderboard():
    """Force refresh leaderboard cache with new weekend logic and populate missing data"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from datetime import datetime, date, timedelta
        from models import db, LeaderboardCache, MarketData
        from leaderboard_utils import update_leaderboard_cache
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "actions_taken": [],
            "errors": []
        }
        
        # 1. Clear old leaderboard cache (including 7D entries)
        try:
            deleted_count = LeaderboardCache.query.delete()
            db.session.commit()
            results["actions_taken"].append(f"Cleared {deleted_count} old leaderboard cache entries")
        except Exception as e:
            results["errors"].append(f"Failed to clear cache: {str(e)}")
        
        # 2. Check SPY market data status (don't populate - data exists!)
        try:
            spy_regular = MarketData.query.filter_by(ticker='SPY').count()
            spy_sp500 = MarketData.query.filter_by(ticker='SPY_SP500').count()
            spy_intraday = MarketData.query.filter_by(ticker='SPY_INTRADAY').count()
            total_spy = spy_regular + spy_sp500 + spy_intraday
            
            results["actions_taken"].append(f"SPY data confirmed: {total_spy} total records (SPY: {spy_regular}, SPY_SP500: {spy_sp500}, SPY_INTRADAY: {spy_intraday})")
            
            if total_spy == 0:
                results["errors"].append("WARNING: No SPY data found - charts will be empty")
            else:
                results["actions_taken"].append("SPY data exists - skipping population to avoid duplicates")
        except Exception as e:
            results["errors"].append(f"SPY data check error: {str(e)}")
        
        # 3. Force regenerate leaderboard cache with new logic
        try:
            # Update with new periods (no 7D)
            updated_count = update_leaderboard_cache(periods=['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX'])
            results["actions_taken"].append(f"Regenerated leaderboard cache: {updated_count} entries updated")
        except Exception as e:
            results["errors"].append(f"Leaderboard regeneration error: {str(e)}")
        
        # 4. Check results
        try:
            new_1d_cache = LeaderboardCache.query.filter_by(period='1D_all').first()
            if new_1d_cache:
                import json
                data = json.loads(new_1d_cache.leaderboard_data)
                zero_performance = sum(1 for entry in data if entry.get('performance_percent', 0) == 0)
                results["actions_taken"].append(f"New 1D cache: {len(data)} entries, {zero_performance} with 0% performance")
                results["sample_new_entry"] = data[0] if data else None
            else:
                results["errors"].append("No 1D cache found after regeneration")
        except Exception as e:
            results["errors"].append(f"Cache verification error: {str(e)}")
        
        return jsonify({
            "success": True,
            "message": "Leaderboard refresh completed",
            "results": results
        })
        
    except Exception as e:
        logger.error(f"Force refresh error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/api/portfolio/chart/<username>/<period>')
def get_public_portfolio_chart(username, period):
    """Get portfolio chart data for public portfolio views (blurred/unblurred)"""
    try:
        from models import User, UserPortfolioChartCache
        import json
        
        # Find user by username
        user = User.query.filter_by(username=username).first()
        if not user:
            return jsonify({'error': 'User not found'}), 404
        
        period_upper = period.upper()
        
        # Try to use pre-rendered chart data (Chart.js format for direct use)
        chart_cache = UserPortfolioChartCache.query.filter_by(
            user_id=user.id, period=period_upper
        ).first()
        
        if chart_cache:
            try:
                # Return Chart.js format directly for public portfolio pages
                cached_data = json.loads(chart_cache.chart_data)
                cached_data['data_source'] = 'pre_rendered_cache'
                cached_data['username'] = username
                
                logger.info(f"Using pre-rendered chart data for public view: {username}, period {period_upper}")
                return jsonify(cached_data)
                
            except Exception as e:
                logger.warning(f"Failed to parse pre-rendered chart data for {username}: {e}")
        
        # Fallback: Generate chart on-demand for non-leaderboard users
        from portfolio_performance import PortfolioPerformanceCalculator
        calculator = PortfolioPerformanceCalculator()
        
        logger.info(f"Generating chart on-demand for public view: {username}, period {period_upper}")
        performance_data = calculator.get_performance_data(user.id, period_upper)
        
        # Convert to Chart.js format for consistency
        if 'chart_data' in performance_data:
            chart_js_format = {
                'labels': [item['date'] for item in performance_data['chart_data']],
                'datasets': [
                    {
                        'label': f'{username} Portfolio',
                        'data': [item['portfolio'] for item in performance_data['chart_data']],
                        'borderColor': 'rgb(75, 192, 192)',
                        'backgroundColor': 'rgba(75, 192, 192, 0.2)',
                        'tension': 0.1
                    },
                    {
                        'label': 'S&P 500',
                        'data': [item['sp500'] for item in performance_data['chart_data']],
                        'borderColor': 'rgb(255, 99, 132)',
                        'backgroundColor': 'rgba(255, 99, 132, 0.2)',
                        'tension': 0.1
                    }
                ],
                'period': period,
                'username': username,
                'data_source': 'live_calculation'
            }
            return jsonify(chart_js_format)
        
        return jsonify({'error': 'No chart data available'}), 404
        
    except Exception as e:
        logger.error(f"Public portfolio chart error for {username}: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-dashboard-apis')
@login_required
def admin_debug_dashboard_apis():
    """Debug the specific API endpoints that dashboard calls"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from datetime import datetime
        import requests
        
        debug_info = {
            "timestamp": datetime.now().isoformat(),
            "user_id": current_user.id,
            "api_tests": {},
            "issues_found": []
        }
        
        # Test the exact endpoints dashboard calls
        periods_to_test = ['1D', '5D', '1M', '3M', 'YTD', '1Y']
        
        for period in periods_to_test:
            try:
                # Determine which endpoint dashboard would call
                if period in ['1D', '5D']:
                    endpoint = f'/api/portfolio/performance-intraday/{period}'
                else:
                    endpoint = f'/api/portfolio/performance/{period}'
                
                # Make internal API call
                from flask import url_for
                import json
                
                # Simulate the API call internally
                if period in ['1D', '5D']:
                    # Test intraday endpoint
                    with app.test_client() as client:
                        # Login as current user
                        with client.session_transaction() as sess:
                            sess['user_id'] = current_user.id
                        
                        response = client.get(endpoint)
                        response_data = json.loads(response.data) if response.data else {}
                        
                        debug_info["api_tests"][f"{period}_intraday"] = {
                            "endpoint": endpoint,
                            "status_code": response.status_code,
                            "has_portfolio_return": "portfolio_return" in response_data,
                            "has_sp500_return": "sp500_return" in response_data,
                            "has_chart_data": "chart_data" in response_data,
                            "portfolio_return_value": response_data.get("portfolio_return"),
                            "sp500_return_value": response_data.get("sp500_return"),
                            "chart_data_length": len(response_data.get("chart_data", [])),
                            "error": response_data.get("error"),
                            "sample_response": response_data
                        }
                        
                        if response.status_code != 200:
                            debug_info["issues_found"].append(f"{period} intraday API returned {response.status_code}")
                        if "portfolio_return" not in response_data:
                            debug_info["issues_found"].append(f"{period} intraday API missing portfolio_return field")
                        if response_data.get("portfolio_return") is None:
                            debug_info["issues_found"].append(f"{period} intraday API portfolio_return is None")
                
                else:
                    # Test regular performance endpoint
                    with app.test_client() as client:
                        with client.session_transaction() as sess:
                            sess['user_id'] = current_user.id
                        
                        response = client.get(endpoint)
                        response_data = json.loads(response.data) if response.data else {}
                        
                        debug_info["api_tests"][f"{period}_regular"] = {
                            "endpoint": endpoint,
                            "status_code": response.status_code,
                            "has_portfolio_return": "portfolio_return" in response_data,
                            "has_sp500_return": "sp500_return" in response_data,
                            "has_chart_data": "chart_data" in response_data,
                            "portfolio_return_value": response_data.get("portfolio_return"),
                            "sp500_return_value": response_data.get("sp500_return"),
                            "chart_data_length": len(response_data.get("chart_data", [])),
                            "error": response_data.get("error"),
                            "sample_response": response_data
                        }
                        
                        if response.status_code != 200:
                            debug_info["issues_found"].append(f"{period} regular API returned {response.status_code}")
                        if "portfolio_return" not in response_data:
                            debug_info["issues_found"].append(f"{period} regular API missing portfolio_return field")
                        if response_data.get("portfolio_return") is None:
                            debug_info["issues_found"].append(f"{period} regular API portfolio_return is None")
                            
            except Exception as e:
                debug_info["issues_found"].append(f"Error testing {period}: {str(e)}")
        
        return jsonify({
            "success": True,
            "debug_info": debug_info
        })
        
    except Exception as e:
        logger.error(f"Dashboard API debug error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/admin/debug-user-data/<username>')
@login_required
def debug_user_data(username):
    """Debug endpoint to check user data availability"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import User, Stock, PortfolioSnapshot, UserActivity, UserPortfolioChartCache
        from admin_metrics import get_active_users_count
        from datetime import datetime, timedelta
        import json
        
        # Find the user
        user = User.query.filter_by(username=username).first()
        if not user:
            return f"<h1>User '{username}' not found</h1><p><a href='/admin'>Back to Admin</a></p>"
        
        # Get user's stocks
        stocks = Stock.query.filter_by(user_id=user.id).all()
        
        # Get recent portfolio snapshots
        snapshots = PortfolioSnapshot.query.filter_by(user_id=user.id).order_by(PortfolioSnapshot.date.desc()).limit(10).all()
        
        # Get recent user activity
        recent_activity = UserActivity.query.filter_by(user_id=user.id).order_by(UserActivity.timestamp.desc()).limit(5).all()
        
        # Get cached chart data
        cached_charts = UserPortfolioChartCache.query.filter_by(user_id=user.id).all()
        
        # Check active users count
        active_count = get_active_users_count(1)  # 1 day
        
        html = f"""
        <h1>Debug Data for User: {username}</h1>
        
        <h2>User Info:</h2>
        <p>ID: {user.id}</p>
        <p>Email: {user.email}</p>
        <p>Username: {user.username}</p>
        
        <h2>Stocks ({len(stocks)}):</h2>
        <ul>
        """
        
        for stock in stocks:
            html += f"<li>{stock.ticker}: {stock.quantity} shares @ ${stock.purchase_price}</li>"
        
        html += f"""
        </ul>
        
        <h2>Recent Portfolio Snapshots ({len(snapshots)}):</h2>
        <ul>
        """
        
        for snapshot in snapshots:
            html += f"<li>{snapshot.date}: ${snapshot.total_value:.2f}</li>"
        
        html += f"""
        </ul>
        
        <h2>Cached Chart Data ({len(cached_charts)}):</h2>
        <ul>
        """
        
        for chart in cached_charts:
            try:
                chart_data = json.loads(chart.chart_data)
                data_points = len(chart_data.get('dates', []))
                html += f"<li>{chart.period}: {data_points} data points (generated {chart.generated_at})</li>"
            except:
                html += f"<li>{chart.period}: Invalid JSON data (generated {chart.generated_at})</li>"
        
        html += f"""
        </ul>
        
        <h2>Recent Activity ({len(recent_activity)}):</h2>
        <ul>
        """
        
        for activity in recent_activity:
            html += f"<li>{activity.timestamp}: {activity.activity_type}</li>"
        
        html += f"""
        </ul>
        
        <h2>Active Users (1D): {active_count}</h2>
        
        <p><a href="/admin">Back to Admin</a></p>
        """
        
        return html
        
    except Exception as e:
        return f"<h1>Error: {str(e)}</h1><p><a href='/admin'>Back to Admin</a></p>"

@app.route('/admin/test-performance-api-user/<username>')
@login_required
def test_performance_api_for_user(username):
    """Test portfolio performance API endpoints for a specific user"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from models import User
        
        # Find the user
        user = User.query.filter_by(username=username).first()
        if not user:
            return f"<h1>User '{username}' not found</h1>"
        
        # Test different time periods
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y']
        results = {}
        
        for period in periods:
            try:
                # Import performance calculator
                from portfolio_performance import PortfolioPerformanceCalculator
                calculator = PortfolioPerformanceCalculator()
                
                # Test the performance calculation directly
                performance_data = calculator.get_performance_data(user.id, period)
                
                if 'error' in performance_data:
                    results[period] = f"ERROR: {performance_data['error']}"
                else:
                    # Summarize the results
                    chart_data_count = len(performance_data.get('chart_data', []))
                    portfolio_return = performance_data.get('portfolio_return', 'N/A')
                    sp500_return = performance_data.get('sp500_return', 'N/A')
                    
                    results[period] = f" SUCCESS - {chart_data_count} data points, Portfolio: {portfolio_return}%, S&P500: {sp500_return}%"
                    
            except Exception as e:
                results[period] = f"EXCEPTION: {str(e)}"
        
        html = f"""
        <h1>Performance API Test for User: {username} (ID: {user.id})</h1>
        
        <h2>API Endpoint Results:</h2>
        <ul>
        """
        
        for period, result in results.items():
            color = "green" if "SUCCESS" in result else "red"
            html += f'<li style="color: {color}"><strong>{period}:</strong> {result}</li>'
        
        html += f"""
        </ul>
        
        <p><a href="/admin">Back to Admin</a></p>
        """
        
        return html
        
    except Exception as e:
        return f"<h1>Error: {str(e)}</h1><p><a href='/admin'>Back to Admin</a></p>"

@app.route('/admin/test-api-logging')
@login_required
def test_api_logging():
    """Admin endpoint to test Alpha Vantage API logging"""
    if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
        flash('Admin access required', 'danger')
        return redirect(url_for('login'))
    
    try:
        from portfolio_performance import PortfolioPerformanceCalculator
        
        calculator = PortfolioPerformanceCalculator()
        
        # Make a test API call for SPY
        result = calculator.get_stock_data('SPY')
        
        if result and result.get('price'):
            flash(f'API call successful! SPY price: ${result["price"]:.2f}. Check admin dashboard for logging.', 'success')
        else:
            flash('API call failed but should still be logged. Check admin dashboard.', 'warning')
        
        return redirect(url_for('admin_dashboard'))
            
    except Exception as e:
        flash(f'API test error: {str(e)}. Check admin dashboard for logging.', 'danger')
        return redirect(url_for('admin_dashboard'))

@app.route('/admin/snapshot-diagnostics')
@login_required
def snapshot_diagnostics():
    """Comprehensive snapshot diagnostics for database tables, counts, and model validation"""
    if not current_user.is_admin:
        return redirect(url_for('index'))
    
    try:
        from datetime import datetime, date, timedelta
        from models import User, PortfolioSnapshotIntraday, PortfolioSnapshot, LeaderboardEntry
        from sqlalchemy import func, text
        
        today = date.today()
        yesterday = today - timedelta(days=1)
        
        diagnostics = {
            'timestamp': datetime.now().isoformat(),
            'database_tables': {},
            'snapshot_counts': {},
            'model_validation': {},
            'recent_data': {},
            'errors': []
        }
        
        # Check if tables exist
        try:
            # Test intraday snapshots table
            intraday_count = db.session.execute(text("SELECT COUNT(*) FROM portfolio_snapshot_intraday")).fetchone()[0]
            diagnostics['database_tables']['portfolio_snapshot_intraday'] = {'exists': True, 'total_records': intraday_count}
            
            # Test EOD snapshots table  
            eod_count = db.session.execute(text("SELECT COUNT(*) FROM portfolio_snapshot")).fetchone()[0]
            diagnostics['database_tables']['portfolio_snapshot'] = {'exists': True, 'total_records': eod_count}
            
            # Test leaderboard table
            leaderboard_count = db.session.execute(text("SELECT COUNT(*) FROM leaderboard_entry")).fetchone()[0]
            diagnostics['database_tables']['leaderboard_entry'] = {'exists': True, 'total_records': leaderboard_count}
            
        except Exception as e:
            diagnostics['errors'].append(f"Database table check error: {str(e)}")
        
        # Count snapshots by date
        try:
            # Intraday snapshots today
            intraday_today = PortfolioSnapshotIntraday.query.filter(
                func.date(PortfolioSnapshotIntraday.timestamp) == today
            ).count()
            
            # Intraday snapshots yesterday
            intraday_yesterday = PortfolioSnapshotIntraday.query.filter(
                func.date(PortfolioSnapshotIntraday.timestamp) == yesterday
            ).count()
            
            # EOD snapshots today
            eod_today = PortfolioSnapshot.query.filter_by(date=today).count()
            
            # EOD snapshots yesterday
            eod_yesterday = PortfolioSnapshot.query.filter_by(date=yesterday).count()
            
            diagnostics['snapshot_counts'] = {
                'intraday_today': intraday_today,
                'intraday_yesterday': intraday_yesterday,
                'eod_today': eod_today,
                'eod_yesterday': eod_yesterday
            }
            
        except Exception as e:
            diagnostics['errors'].append(f"Snapshot count error: {str(e)}")
        
        # Validate model attributes
        try:
            # Test User model
            user_count = User.query.count()
            diagnostics['model_validation']['User'] = {'accessible': True, 'count': user_count}
            
            # Test PortfolioSnapshotIntraday model
            sample_intraday = PortfolioSnapshotIntraday.query.first()
            if sample_intraday:
                diagnostics['model_validation']['PortfolioSnapshotIntraday'] = {
                    'accessible': True,
                    'sample_fields': {
                        'id': hasattr(sample_intraday, 'id'),
                        'user_id': hasattr(sample_intraday, 'user_id'),
                        'timestamp': hasattr(sample_intraday, 'timestamp'),
                        'total_value': hasattr(sample_intraday, 'total_value')
                    }
                }
            
            # Test PortfolioSnapshot model
            sample_eod = PortfolioSnapshot.query.first()
            if sample_eod:
                diagnostics['model_validation']['PortfolioSnapshot'] = {
                    'accessible': True,
                    'sample_fields': {
                        'id': hasattr(sample_eod, 'id'),
                        'user_id': hasattr(sample_eod, 'user_id'),
                        'date': hasattr(sample_eod, 'date'),
                        'total_value': hasattr(sample_eod, 'total_value')
                    }
                }
            
        except Exception as e:
            diagnostics['errors'].append(f"Model validation error: {str(e)}")
        
        # Get recent data samples
        try:
            # Recent intraday snapshots
            recent_intraday = PortfolioSnapshotIntraday.query.order_by(
                PortfolioSnapshotIntraday.timestamp.desc()
            ).limit(5).all()
            
            diagnostics['recent_data']['intraday_snapshots'] = [
                {
                    'user_id': snap.user_id,
                    'timestamp': snap.timestamp.isoformat(),
                    'total_value': snap.total_value
                } for snap in recent_intraday
            ]
            
            # Recent EOD snapshots
            recent_eod = PortfolioSnapshot.query.order_by(
                PortfolioSnapshot.date.desc()
            ).limit(5).all()
            
            diagnostics['recent_data']['eod_snapshots'] = [
                {
                    'user_id': snap.user_id,
                    'date': snap.date.isoformat(),
                    'total_value': snap.total_value
                } for snap in recent_eod
            ]
            
        except Exception as e:
            diagnostics['errors'].append(f"Recent data error: {str(e)}")
        
        return f"""
        <h1>Snapshot Diagnostics</h1>
        <h2>Database Tables</h2>
        <pre>{diagnostics['database_tables']}</pre>
        
        <h2>Snapshot Counts</h2>
        <pre>{diagnostics['snapshot_counts']}</pre>
        
        <h2>Model Validation</h2>
        <pre>{diagnostics['model_validation']}</pre>
        
        <h2>Recent Data Samples</h2>
        <h3>Recent Intraday Snapshots</h3>
        <pre>{diagnostics['recent_data'].get('intraday_snapshots', [])}</pre>
        
        <h3>Recent EOD Snapshots</h3>
        <pre>{diagnostics['recent_data'].get('eod_snapshots', [])}</pre>
        
        <h2>Errors</h2>
        <pre>{diagnostics['errors']}</pre>
        
        <p><a href="/admin">Back to Admin</a></p>
        """
        
    except Exception as e:
        return f"""
        <h1>Snapshot Diagnostics - Error</h1>
        <p><strong>Error:</strong> {str(e)}</p>
        <p><a href="/admin">Back to Admin</a></p>
        """

@app.route('/api/debug/intraday-snapshots')
def debug_intraday_snapshots():
    """Debug endpoint to check actual intraday snapshot data"""
    try:
        user_id = session.get('user_id')
        if not user_id:
            return jsonify({'error': 'User not authenticated'}), 401
        
        from datetime import datetime, date, timedelta
        from models import PortfolioSnapshotIntraday
        
        # Get today's snapshots
        today = date.today()
        today_snapshots = PortfolioSnapshotIntraday.query.filter(
            PortfolioSnapshotIntraday.user_id == user_id,
            PortfolioSnapshotIntraday.timestamp >= datetime.combine(today, datetime.min.time()),
            PortfolioSnapshotIntraday.timestamp < datetime.combine(today, datetime.min.time()) + timedelta(days=1)
        ).order_by(PortfolioSnapshotIntraday.timestamp).all()
        
        # Get last 5 days of snapshots for comparison
        five_days_ago = today - timedelta(days=5)
        recent_snapshots = PortfolioSnapshotIntraday.query.filter(
            PortfolioSnapshotIntraday.user_id == user_id,
            PortfolioSnapshotIntraday.timestamp >= datetime.combine(five_days_ago, datetime.min.time())
        ).order_by(PortfolioSnapshotIntraday.timestamp).all()
        
        # Analyze the data
        today_data = []
        for snapshot in today_snapshots:
            today_data.append({
                'timestamp': snapshot.timestamp.isoformat(),
                'time': snapshot.timestamp.strftime('%H:%M'),
                'total_value': snapshot.total_value
            })
        
        # Group by day for analysis
        from collections import defaultdict
        daily_analysis = defaultdict(list)
        for snapshot in recent_snapshots:
            day_key = snapshot.timestamp.date().isoformat()
            daily_analysis[day_key].append({
                'time': snapshot.timestamp.strftime('%H:%M'),
                'value': snapshot.total_value
            })
        
        # Check for identical values within each day
        daily_summary = {}
        for day, snapshots in daily_analysis.items():
            values = [s['value'] for s in snapshots]
            unique_values = list(set(values))
            daily_summary[day] = {
                'snapshot_count': len(snapshots),
                'unique_values': len(unique_values),
                'all_identical': len(unique_values) == 1,
                'value_range': {
                    'min': min(values) if values else 0,
                    'max': max(values) if values else 0
                },
                'sample_snapshots': snapshots[:5]  # First 5 snapshots
            }
        
        return jsonify({
            'user_id': user_id,
            'today': today.isoformat(),
            'today_snapshots': today_data,
            'daily_analysis': daily_summary,
            'total_recent_snapshots': len(recent_snapshots),
            'diagnosis': {
                'today_has_data': len(today_data) > 0,
                'today_all_identical': len(set([s['total_value'] for s in today_data])) <= 1 if today_data else True,
                'market_is_open': datetime.now().weekday() < 5 and 9 <= datetime.now().hour <= 16
            }
        })
        
    except Exception as e:
        logger.error(f"Error in debug intraday snapshots: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/portfolio/intraday/<period>', methods=['GET'])
@login_required
def portfolio_performance_intraday(period):
    """Get intraday portfolio performance data using actual intraday snapshots"""
    logger.info(f"INTRADAY ROUTE HIT: /api/portfolio/intraday/{period}")
    logger.info(f"Request method: {request.method}")
    logger.info(f"Request headers: {dict(request.headers)}")
    try:
        user_id = session.get('user_id')
        if not user_id:
            return jsonify({'error': 'User not authenticated'}), 401
        from datetime import datetime, date, timedelta
        from models import PortfolioSnapshotIntraday, MarketData
        from sqlalchemy import func, cast, Date
        
        # Calculate date range based on period - use last market day for weekends
        # CRITICAL: Use Eastern Time, not UTC!
        current_time_et = get_market_time()
        today = current_time_et.date()
        
        # Use last market day for weekend handling (in ET)
        if today.weekday() == 5:  # Saturday
            market_day = today - timedelta(days=1)  # Friday
        elif today.weekday() == 6:  # Sunday
            market_day = today - timedelta(days=2)  # Friday
        else:
            market_day = today  # Monday-Friday
            
        logger.info(f"Date calculation (ET): current_time={current_time_et}, today={today}, market_day={market_day}")
        logger.info(f"Timezone: America/New_York")
        
        if period == '1D':
            start_date = market_day
            end_date = market_day
        elif period == '5D':
            # For 5D, we want EXACTLY the last 5 business days including today
            # Count backwards to find the 5th business day
            business_days_found = 0
            current_date = market_day
            
            while business_days_found < 5:
                if current_date.weekday() < 5:  # Monday=0, Friday=4
                    business_days_found += 1
                    if business_days_found == 5:
                        start_date = current_date
                        break
                current_date -= timedelta(days=1)
            
            end_date = market_day
            logger.info(f"5D Date Range: {start_date} to {end_date} (exactly 5 business days)")
        else:
            # Fallback to regular performance API for other periods
            from portfolio_performance import PortfolioPerformanceCalculator
            calculator = PortfolioPerformanceCalculator()
            return jsonify(calculator.get_performance_data(user_id, period)), 200
        
        # Get intraday snapshots for the user in the date range (using ET date extraction)
        # CRITICAL: Convert to ET timezone BEFORE casting to date to avoid UTC session timezone issues
        # FIX (Grok-validated): For 1D, use exact date match to prevent including previous day's data
        if period == '1D':
            # Use exact date match for single day - avoids edge cases with range queries
            # at_time_zone() is more explicit than func.timezone() and avoids cast issues
            snapshots = PortfolioSnapshotIntraday.query.filter(
                PortfolioSnapshotIntraday.user_id == user_id,
                func.date(func.timezone('America/New_York', PortfolioSnapshotIntraday.timestamp)) == market_day
            ).order_by(PortfolioSnapshotIntraday.timestamp).all()
            logger.info(f"1D Chart: Querying for exact date {market_day} (ET)")
        else:
            # For 5D and other periods, use date range
            snapshots = PortfolioSnapshotIntraday.query.filter(
                PortfolioSnapshotIntraday.user_id == user_id,
                cast(func.timezone('America/New_York', PortfolioSnapshotIntraday.timestamp), Date) >= start_date,
                cast(func.timezone('America/New_York', PortfolioSnapshotIntraday.timestamp), Date) <= end_date
            ).order_by(PortfolioSnapshotIntraday.timestamp).all()
            logger.info(f"{period} Chart: Querying date range {start_date} to {end_date} (ET)")
        
        # Debug logging for 5D chart issue
        logger.info(f"5D Chart Debug - Period: {period}, User: {user_id}")
        logger.info(f"Date range: {start_date} to {end_date}")
        logger.info(f"Found {len(snapshots)} snapshots")
        
        # Check what today's date looks like in the database (using ET date)
        today_snapshots = PortfolioSnapshotIntraday.query.filter(
            PortfolioSnapshotIntraday.user_id == user_id,
            cast(func.timezone('America/New_York', PortfolioSnapshotIntraday.timestamp), Date) == today
        ).count()
        logger.info(f"Snapshots specifically for today ({today} ET): {today_snapshots}")
        
        # Also check yesterday for comparison
        yesterday = today - timedelta(days=1)
        yesterday_snapshots = PortfolioSnapshotIntraday.query.filter(
            PortfolioSnapshotIntraday.user_id == user_id,
            cast(func.timezone('America/New_York', PortfolioSnapshotIntraday.timestamp), Date) == yesterday
        ).count()
        logger.info(f"Snapshots for yesterday ({yesterday} ET): {yesterday_snapshots}")
        
        if snapshots:
            logger.info(f"First snapshot: {snapshots[0].timestamp}")
            logger.info(f"Last snapshot: {snapshots[-1].timestamp}")
            # Group by date to see distribution
            from collections import defaultdict
            by_date = defaultdict(int)
            for snap in snapshots:
                by_date[snap.timestamp.date()] += 1
            logger.info(f"Snapshots by date: {dict(by_date)}")
            
            # Check if we have any snapshots from today
            today_in_results = any(snap.timestamp.date() == today for snap in snapshots)
            logger.info(f"Today's data in results: {today_in_results}")
        
        if not snapshots:
            return jsonify({
                'error': 'No intraday data available for this period',
                'period': period,
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat(),
                'debug_info': f'Checked for user_id={user_id} between {start_date} and {end_date}'
            }), 404
        
        # Get S&P 500 data for the same period
        if period in ['1D', '5D']:
            # Use intraday S&P 500 data for short periods
            spy_data = MarketData.query.filter(
                MarketData.ticker == 'SPY_INTRADAY',
                MarketData.date >= start_date,
                MarketData.date <= end_date,
                MarketData.timestamp.isnot(None)
            ).order_by(MarketData.timestamp).all()
        else:
            # Use daily S&P 500 data for longer periods
            spy_data = MarketData.query.filter(
                MarketData.ticker == 'SPY_SP500',
                MarketData.date >= start_date,
                MarketData.date <= end_date
            ).order_by(MarketData.date).all()
        
        # Build chart data
        chart_data = []
        first_portfolio_value = snapshots[0].total_value if snapshots else 0
        first_spy_value = spy_data[0].close_price if spy_data else 0
        
        for snapshot in snapshots:
            # Find corresponding S&P 500 value
            spy_value = first_spy_value
            
            if period in ['1D', '5D']:
                # For intraday data, match by timestamp
                for spy_point in spy_data:
                    if spy_point.timestamp <= snapshot.timestamp:
                        spy_value = spy_point.close_price
                    else:
                        break
            else:
                # For daily data, match by date
                snapshot_date = snapshot.timestamp.date()
                for spy_point in spy_data:
                    if spy_point.date <= snapshot_date:
                        spy_value = spy_point.close_price
                    else:
                        break
            
            # Calculate returns
            portfolio_return = ((snapshot.total_value - first_portfolio_value) / first_portfolio_value * 100) if first_portfolio_value > 0 else 0
            sp500_return = ((spy_value - first_spy_value) / first_spy_value * 100) if first_spy_value > 0 else 0
            
            # Format date label based on period
            # CRITICAL FIX: Convert timestamp to ET before sending to frontend
            # PostgreSQL returns timestamps in UTC; convert to ET to display correct market hours
            if period == '1D':
                # For 1D charts, use date + time format (e.g., "Oct 18 9:30 AM")
                et_timestamp = snapshot.timestamp.astimezone(MARKET_TZ)
                date_label = et_timestamp.strftime('%b %d %I:%M %p')
            elif period == '5D':
                # For 5D charts, use date-only format since we now show only one snapshot per day
                et_timestamp = snapshot.timestamp.astimezone(MARKET_TZ)
                date_label = et_timestamp.strftime('%b %d')
            else:
                # For longer periods, use short date format
                date_label = snapshot.timestamp.date().strftime('%b %d')
            
            chart_data.append({
                'date': date_label,
                'portfolio': round(portfolio_return, 2),
                'sp500': round(sp500_return, 2)
            })
        
        # Calculate overall returns
        final_portfolio_value = snapshots[-1].total_value if snapshots else 0
        final_spy_value = spy_data[-1].close_price if spy_data else first_spy_value
        
        portfolio_return = ((final_portfolio_value - first_portfolio_value) / first_portfolio_value * 100) if first_portfolio_value > 0 else 0
        sp500_return = ((final_spy_value - first_spy_value) / first_spy_value * 100) if first_spy_value > 0 else 0
        
        # Debug the final chart data
        logger.info(f"Final chart data: {len(chart_data)} points")
        if chart_data:
            logger.info(f"First chart point: {chart_data[0]}")
            logger.info(f"Last chart point: {chart_data[-1]}")
        
        return jsonify({
            'portfolio_return': round(portfolio_return, 2),
            'sp500_return': round(sp500_return, 2),
            'chart_data': chart_data,
            'period': period,
            'start_date': start_date.isoformat(),
            'end_date': end_date.isoformat(),
            'data_points': len(chart_data),
            'debug_info': {
                'snapshots_found': len(snapshots),
                'spy_data_found': len(spy_data),
                'first_portfolio_value': first_portfolio_value,
                'first_spy_value': first_spy_value,
                'sample_timestamps': [s.timestamp.isoformat() for s in snapshots[:3]]
            }
        }), 200
    
    except Exception as e:
        logger.error(f"Error in performance-intraday API: {str(e)}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

@app.route('/admin/test-imports')
@login_required
def admin_test_imports():
    """Test that all imports work correctly"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime as dt
        
        results = {
            'timestamp': dt.now().isoformat(),
            'tests': {}
        }
        
        # Test 1: Basic imports
        try:
            from datetime import datetime, date, timedelta
            from typing import Dict, List
            results['tests']['basic_imports'] = {'status': 'success', 'message': 'Basic imports working'}
        except Exception as e:
            results['tests']['basic_imports'] = {'status': 'error', 'message': str(e)}
        
        # Test 2: Models import
        try:
            from models import db, MarketData, PortfolioSnapshot
            results['tests']['models_import'] = {'status': 'success', 'message': 'Models import working'}
        except Exception as e:
            results['tests']['models_import'] = {'status': 'error', 'message': str(e)}
        
        # Test 3: Portfolio performance import
        try:
            from portfolio_performance import PortfolioPerformanceCalculator
            results['tests']['portfolio_performance_import'] = {'status': 'success', 'message': 'Portfolio performance import working'}
        except Exception as e:
            results['tests']['portfolio_performance_import'] = {'status': 'error', 'message': str(e)}
        
        # Test 4: Calculator instantiation
        try:
            from portfolio_performance import PortfolioPerformanceCalculator
            calculator = PortfolioPerformanceCalculator()
            results['tests']['calculator_instantiation'] = {'status': 'success', 'message': 'Calculator instantiation working'}
        except Exception as e:
            results['tests']['calculator_instantiation'] = {'status': 'error', 'message': str(e)}
        
        return jsonify(results)
        
    except Exception as e:
        logger.error(f"Error in import test: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-chart-data')
@login_required
def admin_debug_chart_data():
    """Debug chart data format to understand timestamp issue"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime
        from portfolio_performance import PortfolioPerformanceCalculator
        
        user_id = session.get('user_id')
        if not user_id:
            return jsonify({'error': 'User not authenticated'}), 401
        
        calculator = PortfolioPerformanceCalculator()
        
        # Test both 1D and 5D data
        results = {}
        
        for period in ['1D', '5D']:
            try:
                data = calculator.get_performance_data(user_id, period)
                
                # Sample the first few data points
                sample_data = data.get('chart_data', [])[:3] if data.get('chart_data') else []
                
                results[period] = {
                    'success': True,
                    'total_points': len(data.get('chart_data', [])),
                    'sample_data': sample_data,
                    'sample_analysis': []
                }
                
                # Analyze each sample point
                for i, item in enumerate(sample_data):
                    analysis = {
                        'index': i,
                        'raw_date': item.get('date'),
                        'date_type': type(item.get('date')).__name__,
                        'portfolio_value': item.get('portfolio'),
                        'sp500_value': item.get('sp500')
                    }
                    
                    # Try to parse the date
                    try:
                        if isinstance(item.get('date'), str):
                            parsed_date = datetime.fromisoformat(item['date'].replace('Z', '+00:00'))
                            analysis['parsed_date'] = parsed_date.isoformat()
                            analysis['timestamp_ms'] = int(parsed_date.timestamp() * 1000)
                        else:
                            analysis['parse_error'] = 'Date is not a string'
                    except Exception as e:
                        analysis['parse_error'] = str(e)
                    
                    results[period]['sample_analysis'].append(analysis)
                    
            except Exception as e:
                results[period] = {
                    'success': False,
                    'error': str(e)
                }
        
        return jsonify({
            'timestamp': datetime.now().isoformat(),
            'user_id': user_id,
            'results': results
        })
        
    except Exception as e:
        logger.error(f"Error in chart data debug: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-snapshot-dates')
@login_required
def admin_debug_snapshot_dates():
    """Debug missing Friday snapshot issue"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, PortfolioSnapshot, PortfolioSnapshotIntraday
        from sqlalchemy import func, desc
        
        user_id = session.get('user_id')
        if not user_id:
            return jsonify({'error': 'User not authenticated'}), 401
        
        today = date.today()
        
        # Check recent portfolio snapshots (daily)
        recent_snapshots = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.user_id == user_id,
            PortfolioSnapshot.date >= today - timedelta(days=7)
        ).order_by(desc(PortfolioSnapshot.date)).all()
        
        # Check recent intraday snapshots
        recent_intraday = PortfolioSnapshotIntraday.query.filter(
            PortfolioSnapshotIntraday.user_id == user_id,
            func.date(PortfolioSnapshotIntraday.timestamp) >= today - timedelta(days=7)
        ).order_by(desc(PortfolioSnapshotIntraday.timestamp)).limit(20).all()
        
        # Expected dates (weekdays only)
        expected_dates = []
        check_date = today - timedelta(days=7)
        while check_date <= today:
            if check_date.weekday() < 5:  # Monday-Friday
                expected_dates.append(check_date)
            check_date += timedelta(days=1)
        
        # Analyze snapshots
        snapshot_analysis = {
            'today': today.isoformat(),
            'expected_trading_days': [d.isoformat() for d in expected_dates],
            'daily_snapshots': [],
            'intraday_snapshots': [],
            'missing_dates': [],
            'friday_status': {}
        }
        
        # Daily snapshots analysis
        snapshot_dates = set()
        for snapshot in recent_snapshots:
            snapshot_dates.add(snapshot.date)
            snapshot_analysis['daily_snapshots'].append({
                'date': snapshot.date.isoformat(),
                'total_value': float(snapshot.total_value),
                'created_at': snapshot.created_at.isoformat() if snapshot.created_at else None
            })
        
        # Find missing dates
        for expected_date in expected_dates:
            if expected_date not in snapshot_dates and expected_date < today:
                snapshot_analysis['missing_dates'].append(expected_date.isoformat())
        
        # Intraday snapshots analysis
        intraday_by_date = {}
        for intraday in recent_intraday:
            snapshot_date = intraday.timestamp.date()
            if snapshot_date not in intraday_by_date:
                intraday_by_date[snapshot_date] = []
            intraday_by_date[snapshot_date].append({
                'timestamp': intraday.timestamp.isoformat(),
                'total_value': float(intraday.total_value)
            })
        
        for date_key, snapshots in intraday_by_date.items():
            snapshot_analysis['intraday_snapshots'].append({
                'date': date_key.isoformat(),
                'count': len(snapshots),
                'snapshots': snapshots[:3]  # First 3 for brevity
            })
        
        # Special focus on Friday 9/26/2025
        friday_date = date(2025, 9, 26)
        snapshot_analysis['friday_status'] = {
            'date': friday_date.isoformat(),
            'is_trading_day': friday_date.weekday() < 5,
            'has_daily_snapshot': friday_date in snapshot_dates,
            'has_intraday_snapshots': friday_date in intraday_by_date,
            'intraday_count': len(intraday_by_date.get(friday_date, []))
        }
        
        return jsonify(snapshot_analysis)
        
    except Exception as e:
        logger.error(f"Error in snapshot dates debug: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-leaderboard-calculations')
@login_required
def admin_debug_leaderboard_calculations():
    """Debug leaderboard calculation issues - why only 1 user on 1D, wrong percentages on 5D"""
    from datetime import datetime, date, timedelta
    from models import db, User, LeaderboardCache, LeaderboardEntry
    from portfolio_performance import PortfolioPerformanceCalculator
    from sqlalchemy import desc
    import json
    
    try:
        # Rollback any failed transaction first
        db.session.rollback()
        
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        calculator = PortfolioPerformanceCalculator()
        
        # Get all users with stocks for testing
        users_with_stocks = User.query.join(User.stocks).distinct().limit(5).all()
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'periods_tested': ['1D', '5D'],
            'users_analyzed': [],
            'leaderboard_cache_status': {},
            'calculation_comparison': {},
            'database_schema_check': {}
        }
        
        # Check database schema for LeaderboardEntry table
        try:
            from sqlalchemy import inspect
            inspector = inspect(db.engine)
            leaderboard_columns = [col['name'] for col in inspector.get_columns('leaderboard_entry')]
            results['database_schema_check'] = {
                'leaderboard_entry_columns': leaderboard_columns,
                'has_period_column': 'period' in leaderboard_columns,
                'has_performance_percent_column': 'performance_percent' in leaderboard_columns
            }
        except Exception as e:
            results['database_schema_check'] = {
                'error': f'Schema check failed: {str(e)}'
            }
        
        # Check leaderboard cache for each period
        for period in ['1D', '5D']:
            # Check LeaderboardCache (JSON cache)
            cache_entry = LeaderboardCache.query.filter_by(period=period).first()
            cache_data = None
            if cache_entry:
                try:
                    cache_data = json.loads(cache_entry.leaderboard_data)
                except:
                    cache_data = None
            
            # Check LeaderboardEntry (individual user records)
            # Handle case where period column might not exist in database yet
            try:
                entry_records = LeaderboardEntry.query.filter_by(period=period).all()
            except Exception as e:
                logger.warning(f"LeaderboardEntry period filter failed: {str(e)}")
                db.session.rollback()  # Rollback failed transaction
                # Try to get all records if period column doesn't exist
                try:
                    entry_records = LeaderboardEntry.query.all()
                except Exception as e2:
                    logger.error(f"LeaderboardEntry query failed entirely: {str(e2)}")
                    db.session.rollback()  # Rollback failed transaction again
                    entry_records = []
            
            results['leaderboard_cache_status'][period] = {
                'cache_exists': cache_entry is not None,
                'cache_generated_at': cache_entry.generated_at.isoformat() if cache_entry else None,
                'cache_user_count': len(cache_data) if cache_data else 0,
                'entry_records_count': len(entry_records),
                'entry_records': []
            }
            
            for entry in entry_records[:3]:  # First 3 entries
                results['leaderboard_cache_status'][period]['entry_records'].append({
                    'user_id': entry.user_id,
                    'performance_percent': float(entry.performance_percent) if entry.performance_percent else None,
                    'calculated_at': entry.calculated_at.isoformat() if entry.calculated_at else None
                })
        
        # Test calculations for each user
        for user in users_with_stocks:
            user_analysis = {
                'user_id': user.id,
                'username': user.username,
                'stock_count': len(user.stocks.all()),
                'calculations': {}
            }
            
            for period in ['1D', '5D']:
                try:
                    # Get performance data using the same method as charts
                    perf_data = calculator.get_performance_data(user.id, period)
                    
                    user_analysis['calculations'][period] = {
                        'success': True,
                        'portfolio_return': perf_data.get('portfolio_return'),
                        'sp500_return': perf_data.get('sp500_return'),
                        'chart_data_points': len(perf_data.get('chart_data', [])),
                        'has_chart_data': bool(perf_data.get('chart_data')),
                        'first_data_point': perf_data.get('chart_data', [{}])[0] if perf_data.get('chart_data') else None,
                        'last_data_point': perf_data.get('chart_data', [{}])[-1] if perf_data.get('chart_data') else None
                    }
                    
                except Exception as e:
                    user_analysis['calculations'][period] = {
                        'success': False,
                        'error': str(e)
                    }
            
            results['users_analyzed'].append(user_analysis)
        
        # Compare with actual leaderboard cache
        for period in ['1D', '5D']:
            cache_data = {}
            live_data = {}
            
            for user in users_with_stocks:
                # Get cached data from LeaderboardEntry
                try:
                    entry_record = LeaderboardEntry.query.filter_by(
                        user_id=user.id, period=period
                    ).first()
                    if entry_record:
                        cache_data[user.id] = float(entry_record.performance_percent) if entry_record.performance_percent else 0.0
                except Exception as e:
                    logger.warning(f"LeaderboardEntry lookup failed for user {user.id}, period {period}: {str(e)}")
                    db.session.rollback()  # Rollback failed transaction
                    # If period column doesn't exist, skip cached data comparison
                    cache_data[user.id] = 'db_error'
                
                # Get live calculation
                user_calc = next((u for u in results['users_analyzed'] if u['user_id'] == user.id), None)
                if user_calc and user_calc['calculations'].get(period, {}).get('success'):
                    live_data[user.id] = user_calc['calculations'][period]['portfolio_return']
            
            results['calculation_comparison'][period] = {
                'cached_data': cache_data,
                'live_data': live_data,
                'discrepancies': []
            }
            
            # Find discrepancies
            for user_id in set(list(cache_data.keys()) + list(live_data.keys())):
                cached_val = cache_data.get(user_id, 'missing')
                live_val = live_data.get(user_id, 'missing')
                
                if cached_val != live_val:
                    # Calculate difference safely, handling string values
                    difference = 'N/A'
                    if (cached_val not in ['missing', 'db_error'] and live_val not in ['missing', 'db_error'] 
                        and cached_val is not None and live_val is not None):
                        try:
                            difference = abs(float(cached_val) - float(live_val))
                        except (ValueError, TypeError):
                            difference = 'conversion_error'
                    
                    results['calculation_comparison'][period]['discrepancies'].append({
                        'user_id': user_id,
                        'cached': cached_val,
                        'live': live_val,
                        'difference': difference
                    })
        
        return jsonify(results)
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error in leaderboard debug: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/check-leaderboard-schema')
@login_required
def admin_check_leaderboard_schema():
    """Check actual database schema for leaderboard tables"""
    from sqlalchemy import inspect
    from models import db
    
    try:
        db.session.rollback()
        
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        inspector = inspect(db.engine)
        
        # Check if tables exist
        tables = inspector.get_table_names()
        
        result = {
            'timestamp': datetime.now().isoformat(),
            'all_tables': tables,
            'leaderboard_tables': {}
        }
        
        # Check leaderboard_entry table
        if 'leaderboard_entry' in tables:
            columns = inspector.get_columns('leaderboard_entry')
            result['leaderboard_tables']['leaderboard_entry'] = {
                'exists': True,
                'columns': [{'name': col['name'], 'type': str(col['type'])} for col in columns]
            }
        else:
            result['leaderboard_tables']['leaderboard_entry'] = {'exists': False}
        
        # Check leaderboard_cache table
        if 'leaderboard_cache' in tables:
            columns = inspector.get_columns('leaderboard_cache')
            result['leaderboard_tables']['leaderboard_cache'] = {
                'exists': True,
                'columns': [{'name': col['name'], 'type': str(col['type'])} for col in columns]
            }
        else:
            result['leaderboard_tables']['leaderboard_cache'] = {'exists': False}
        
        return jsonify(result)
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error checking schema: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/fix-leaderboard-schema')
@login_required
def admin_fix_leaderboard_schema():
    """Fix the leaderboard_entry table schema by adding missing columns"""
    from models import db
    
    try:
        db.session.rollback()
        
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        logger.info("Starting fast leaderboard schema fix...")
        
        actions_performed = []
        
        # Add missing columns one by one (faster than drop/recreate)
        try:
            db.session.execute('ALTER TABLE leaderboard_entry ADD COLUMN IF NOT EXISTS period VARCHAR(10)')
            actions_performed.append('Added period column')
        except Exception as e:
            logger.warning(f"Period column add failed: {e}")
        
        try:
            db.session.execute('ALTER TABLE leaderboard_entry ADD COLUMN IF NOT EXISTS performance_percent DOUBLE PRECISION')
            actions_performed.append('Added performance_percent column')
        except Exception as e:
            logger.warning(f"Performance_percent column add failed: {e}")
        
        try:
            db.session.execute('ALTER TABLE leaderboard_entry ADD COLUMN IF NOT EXISTS small_cap_percent DOUBLE PRECISION')
            actions_performed.append('Added small_cap_percent column')
        except Exception as e:
            logger.warning(f"Small_cap_percent column add failed: {e}")
        
        try:
            db.session.execute('ALTER TABLE leaderboard_entry ADD COLUMN IF NOT EXISTS large_cap_percent DOUBLE PRECISION')
            actions_performed.append('Added large_cap_percent column')
        except Exception as e:
            logger.warning(f"Large_cap_percent column add failed: {e}")
        
        try:
            db.session.execute('ALTER TABLE leaderboard_entry ADD COLUMN IF NOT EXISTS avg_trades_per_week DOUBLE PRECISION')
            actions_performed.append('Added avg_trades_per_week column')
        except Exception as e:
            logger.warning(f"Avg_trades_per_week column add failed: {e}")
        
        try:
            # Check if calculated_at column exists first
            result = db.session.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'leaderboard_entry' AND column_name = 'calculated_at'")
            exists = result.fetchone() is not None
            
            if not exists:
                db.session.execute('ALTER TABLE leaderboard_entry ADD COLUMN calculated_at TIMESTAMP')
                actions_performed.append('Added calculated_at column (was missing)')
            else:
                actions_performed.append('calculated_at column already exists')
        except Exception as e:
            logger.error(f"Calculated_at column add failed: {e}")
            actions_performed.append(f'Calculated_at column add FAILED: {str(e)}')
        
        # Try to add unique constraint (may fail if data exists)
        try:
            db.session.execute('ALTER TABLE leaderboard_entry ADD CONSTRAINT unique_user_period_leaderboard UNIQUE (user_id, period)')
            actions_performed.append('Added unique constraint on user_id + period')
        except Exception as e:
            logger.warning(f"Unique constraint add failed: {e}")
            actions_performed.append('Unique constraint add failed (may already exist or conflicting data)')
        
        db.session.commit()
        
        logger.info("Fast leaderboard schema fix completed!")
        
        return jsonify({
            'success': True,
            'message': 'LeaderboardEntry table schema updated (added missing columns)',
            'timestamp': datetime.now().isoformat(),
            'actions_performed': actions_performed,
            'note': 'Old columns (date, daily_return, etc.) still exist but new columns added'
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error fixing leaderboard schema: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/force-calculated-at-column')
@login_required
def admin_force_calculated_at_column():
    """Force add calculated_at column with direct SQL and immediate verification"""
    from models import db
    
    try:
        db.session.rollback()
        
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        logger.info("Starting FORCE calculated_at column addition...")
        
        # Step 1: Check current state
        check_result = db.session.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'leaderboard_entry' AND column_name = 'calculated_at'")
        exists_before = check_result.fetchone() is not None
        
        actions = []
        actions.append(f'calculated_at exists before: {exists_before}')
        
        if not exists_before:
            # Step 2: Add column with explicit commit
            try:
                db.session.execute('ALTER TABLE leaderboard_entry ADD COLUMN calculated_at TIMESTAMP')
                db.session.commit()  # Force immediate commit
                actions.append('ALTER TABLE executed and committed')
                
                # Step 3: Verify immediately after commit
                verify_result = db.session.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'leaderboard_entry' AND column_name = 'calculated_at'")
                exists_after = verify_result.fetchone() is not None
                actions.append(f'calculated_at exists after commit: {exists_after}')
                
                if exists_after:
                    actions.append('SUCCESS: Column added and verified')
                else:
                    actions.append('FAILURE: Column not found after commit - possible database issue')
                
            except Exception as e:
                db.session.rollback()
                actions.append(f'ALTER TABLE failed: {str(e)}')
                
        else:
            actions.append('Column already exists - no action needed')
        
        # Step 4: Final verification with fresh connection
        final_check = db.session.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'leaderboard_entry' AND column_name = 'calculated_at'")
        final_exists = final_check.fetchone() is not None
        actions.append(f'Final verification: calculated_at exists = {final_exists}')
        
        # Step 5: Test a simple query
        try:
            test_query = db.session.execute('SELECT calculated_at FROM leaderboard_entry LIMIT 1')
            actions.append('Test query SUCCESS: calculated_at column is queryable')
        except Exception as e:
            actions.append(f'Test query FAILED: {str(e)}')
        
        return jsonify({
            'success': True,
            'message': 'Force calculated_at column addition completed',
            'timestamp': datetime.now().isoformat(),
            'actions_performed': actions,
            'column_exists_final': final_exists
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error in force calculated_at column: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/test-weekend-protection')
@login_required
def admin_test_weekend_protection():
    """Test weekend protection by attempting an API call"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime
        from portfolio_performance import PortfolioPerformanceCalculator
        
        calculator = PortfolioPerformanceCalculator()
        current_time = datetime.now()
        
        # Test API call
        result = calculator.get_stock_data('AAPL')
        
        return jsonify({
            'current_time': current_time.isoformat(),
            'weekday': current_time.weekday(),
            'is_weekend': current_time.weekday() >= 5,
            'api_call_result': result,
            'protection_working': result is None if current_time.weekday() >= 5 else result is not None
        })
        
    except Exception as e:
        logger.error(f"Error in weekend protection test: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/admin/debug-ytd-sp500')
@login_required
def admin_debug_ytd_sp500():
    """Debug YTD S&P 500 calculation vs other periods"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import db, MarketData
        from sqlalchemy import and_
        from portfolio_performance import PortfolioPerformanceCalculator
        
        calculator = PortfolioPerformanceCalculator()
        today = date.today()
        
        # Calculate end_date (same logic as portfolio_performance.py)
        if today.weekday() == 5:  # Saturday
            end_date = today - timedelta(days=1)  # Friday
        elif today.weekday() == 6:  # Sunday
            end_date = today - timedelta(days=2)  # Friday
        else:
            end_date = today  # Monday-Friday
        
        # Define periods to compare
        periods = {
            'YTD': {
                'start_date': date(end_date.year, 1, 1),
                'description': f"January 1, {end_date.year} to {end_date}"
            },
            '3M': {
                'start_date': end_date - timedelta(days=90),
                'description': f"90 days ago to {end_date}"
            },
            '1Y': {
                'start_date': end_date - timedelta(days=365),
                'description': f"365 days ago to {end_date}"
            }
        }
        
        results = {
            'today': today.isoformat(),
            'end_date': end_date.isoformat(),
            'periods': {}
        }
        
        # Analyze each period
        for period_name, period_info in periods.items():
            start_date = period_info['start_date']
            
            # Get S&P 500 data for this period
            sp500_data = MarketData.query.filter(
                and_(
                    MarketData.ticker == 'SPY_SP500',
                    MarketData.date >= start_date,
                    MarketData.date <= end_date
                )
            ).order_by(MarketData.date).all()
            
            period_result = {
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat(),
                'days': (end_date - start_date).days,
                'description': period_info['description'],
                'data_points': len(sp500_data),
                'start_price': None,
                'end_price': None,
                'return_percent': 0.0,
                'calculation_status': 'failed'
            }
            
            if sp500_data:
                # Find start and end prices using same logic as portfolio_performance.py
                available_dates = [data.date for data in sp500_data]
                sp500_dict = {data.date: data.close_price for data in sp500_data}
                
                start_price = None
                end_price = None
                
                # Find start price (closest date >= start_date)
                for d in sorted(available_dates):
                    if d >= start_date:
                        start_price = sp500_dict[d]
                        period_result['start_price'] = start_price
                        period_result['start_date_actual'] = d.isoformat()
                        break
                
                # Find end price (closest date <= end_date)
                for d in reversed(sorted(available_dates)):
                    if d <= end_date:
                        end_price = sp500_dict[d]
                        period_result['end_price'] = end_price
                        period_result['end_date_actual'] = d.isoformat()
                        break
                
                if start_price and end_price and start_price > 0:
                    sp500_return = (end_price - start_price) / start_price
                    period_result['return_percent'] = round(sp500_return * 100, 2)
                    period_result['calculation_status'] = 'success'
                    
                    # Add warnings for suspicious data
                    if start_price < 100:
                        period_result['warning'] = f"Start price ${start_price:.2f} seems unusually low"
                    if sp500_return == 0:
                        period_result['warning'] = "Zero return calculated"
                else:
                    period_result['error'] = f"Invalid prices: start={start_price}, end={end_price}"
            else:
                period_result['error'] = "No S&P 500 data found for this period"
            
            # Test calculator method
            try:
                calc_return = calculator.calculate_sp500_return(start_date, end_date)
                period_result['calculator_return_percent'] = round(calc_return * 100, 2)
            except Exception as e:
                period_result['calculator_error'] = str(e)
            
            results['periods'][period_name] = period_result
        
        # Add data quality checks
        all_sp500_count = MarketData.query.filter(MarketData.ticker == 'SPY_SP500').count()
        zero_prices_count = MarketData.query.filter(
            and_(MarketData.ticker == 'SPY_SP500', MarketData.close_price == 0)
        ).count()
        low_prices_count = MarketData.query.filter(
            and_(
                MarketData.ticker == 'SPY_SP500',
                MarketData.close_price < 100,
                MarketData.close_price > 0
            )
        ).count()
        
        results['data_quality'] = {
            'total_sp500_records': all_sp500_count,
            'zero_prices': zero_prices_count,
            'suspiciously_low_prices': low_prices_count
        }
        
        return jsonify(results)
        
    except Exception as e:
        logger.error(f"Error in YTD S&P 500 debug: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/debug/routes')
def debug_routes():
    """Debug endpoint to list all registered routes"""
    routes = []
    for rule in app.url_map.iter_rules():
        routes.append({
            'endpoint': rule.endpoint,
            'methods': list(rule.methods),
            'rule': rule.rule
        })
    return jsonify({
        'total_routes': len(routes),
        'routes': sorted(routes, key=lambda x: x['rule']),
        'intraday_routes': [r for r in routes if 'intraday' in r['rule']],
        'performance_routes': [r for r in routes if 'performance' in r['rule']]
    })

@app.route('/admin/simulate-intraday-data')
@login_required
def simulate_intraday_data():
    """Create sample intraday data for testing charts"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, timedelta
        from models import User, PortfolioSnapshotIntraday
        from portfolio_performance import PortfolioPerformanceCalculator
        
        calculator = PortfolioPerformanceCalculator()
        today = datetime.now().date()
        
        # Create sample intraday snapshots for today (every 30 minutes from 9:30 AM to 4:00 PM)
        market_times = []
        start_time = datetime.combine(today, datetime.strptime('09:30', '%H:%M').time())
        end_time = datetime.combine(today, datetime.strptime('16:00', '%H:%M').time())
        
        current_time = start_time
        while current_time <= end_time:
            market_times.append(current_time)
            current_time += timedelta(minutes=30)
        
        users = User.query.all()
        snapshots_created = 0
        
        for user in users:
            base_portfolio_value = calculator.calculate_portfolio_value(user.id)
            
            for i, timestamp in enumerate(market_times):
                # Add some realistic variation (2% throughout the day)
                variation = (i - len(market_times)/2) * 0.001  # Gradual trend
                random_factor = 0.995 + (i % 3) * 0.005  # Small random variation
                simulated_value = base_portfolio_value * (1 + variation) * random_factor
                
                # Check if snapshot already exists
                existing = PortfolioSnapshotIntraday.query.filter_by(
                    user_id=user.id,
                    timestamp=timestamp
                ).first()
                
                if not existing:
                    snapshot = PortfolioSnapshotIntraday(
                        user_id=user.id,
                        timestamp=timestamp,
                        total_value=simulated_value
                    )
                    db.session.add(snapshot)
                    snapshots_created += 1
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': 'Sample intraday data created',
            'snapshots_created': snapshots_created,
            'time_points': len(market_times),
            'users_processed': len(users)
        }), 200
    
    except Exception as e:
        logger.error(f"Error creating sample intraday data: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/api/cron/collect-intraday-data', methods=['POST', 'GET'])
def collect_intraday_data():
    """Collect intraday data for all users (called by GitHub Actions)"""
    try:
        # Verify authorization token
        auth_header = request.headers.get('Authorization', '')
        expected_token = os.environ.get('INTRADAY_CRON_TOKEN')
        
        if not expected_token:
            logger.error("INTRADAY_CRON_TOKEN not configured")
            return jsonify({'error': 'Server configuration error'}), 500
        
        # Allow GET requests for manual testing (bypass auth for debugging)
        if request.method == 'GET':
            logger.warning("Manual intraday collection triggered via GET (bypassing auth)")
        else:
            # POST requests require proper authentication
            if not auth_header.startswith('Bearer ') or auth_header[7:] != expected_token:
                logger.warning(f"Unauthorized intraday collection attempt")
                return jsonify({'error': 'Unauthorized'}), 401
        
        from models import User, PortfolioSnapshotIntraday
        from portfolio_performance import PortfolioPerformanceCalculator
        import time
        
        # Use eastern Time for all market operations
        current_time = get_market_time()
        today_et = current_time.date()
        
        # Don't collect data on weekends
        if current_time.weekday() >= 5:  # Saturday = 5, Sunday = 6
            logger.info(f"Weekend detected ({current_time.strftime('%A')}) - skipping intraday data collection")
            return jsonify({
                'success': True,
                'message': f'Skipped collection - market closed on {current_time.strftime("%A")}',
                'timestamp': current_time.isoformat(),
                'timezone': 'America/New_York',
                'weekend': True
            })
        
        start_time = time.time()
        calculator = PortfolioPerformanceCalculator()
        results = {
            'timestamp': current_time.isoformat(),
            'current_time_et': current_time.strftime('%Y-%m-%d %H:%M:%S ET'),
            'market_status': 'CLOSED' if current_time.hour >= 16 else 'OPEN',
            'spy_data_collected': False,
            'users_processed': 0,
            'snapshots_created': 0,
            'charts_generated': 0,
            'errors': [],
            'execution_time_seconds': 0,
            'detailed_logging': {
                'unique_stocks_analysis': {},
                'api_call_tracking': {},
                'cache_efficiency': {},
                'portfolio_calculations': {}
            }
        }
        
        # Step 1: Get all users and collect unique tickers (for batch API call)
        users = User.query.all()
        
        # Collect all unique tickers across all users
        from models import Stock
        unique_tickers = set()
        unique_tickers.add('SPY')  # Always include SPY for S&P 500 benchmark
        
        for user in users:
            user_stocks = Stock.query.filter_by(user_id=user.id).all()
            for stock in user_stocks:
                if stock.quantity > 0:
                    unique_tickers.add(stock.ticker.upper())
        
        logger.info(f" Batch API: Fetching {len(unique_tickers)} unique tickers for {len(users)} users")
        
        # Step 2: BATCH API CALL - Fetch all prices in ONE call (12-25x more efficient!)
        try:
            batch_prices = calculator.get_batch_stock_data(list(unique_tickers))
            
            # Log results
            if batch_prices:
                logger.info(f" Batch API Success: Retrieved {len(batch_prices)} prices in 1-2 calls")
                results['api_calls_made'] = 1 if len(unique_tickers) <= 256 else (len(unique_tickers) // 256) + 1
                results['tickers_fetched'] = len(batch_prices)
                
                # Store SPY data for S&P 500 tracking
                if 'SPY' in batch_prices:
                    spy_price = batch_prices['SPY']
                    sp500_value = spy_price * 10
                    
                    from models import MarketData
                    market_data = MarketData(
                        ticker='SPY_INTRADAY',
                        date=today_et,
                        timestamp=current_time,
                        close_price=sp500_value
                    )
                    db.session.add(market_data)
                    results['spy_data_collected'] = True
                    logger.info(f"SPY: ${spy_price} (S&P 500: ${sp500_value})")
            else:
                results['errors'].append("Batch API returned no prices")
                logger.error(" Batch API failed - no prices returned")
        
        except Exception as e:
            error_msg = f"Batch API error: {str(e)}"
            results['errors'].append(error_msg)
            logger.error(error_msg)
        
        # Step 3: Calculate portfolio values (now using CACHED data from batch call)
        # All stock prices are already in cache - no additional API calls needed!
        intraday_snapshots = []
        
        for user in users:
            try:
                # Calculate current portfolio value WITH cash tracking
                from cash_tracking import calculate_portfolio_value_with_cash
                portfolio_data = calculate_portfolio_value_with_cash(user.id)
                
                total_value = portfolio_data['total_value']
                stock_value = portfolio_data['stock_value']
                cash_proceeds = portfolio_data['cash_proceeds']
                
                if total_value > 0:  # Only create snapshots for users with portfolios
                    # Create intraday snapshot with ALL fields (add to batch)
                    intraday_snapshot = PortfolioSnapshotIntraday(
                        user_id=user.id,
                        timestamp=current_time,
                        total_value=total_value,
                        stock_value=stock_value,
                        cash_proceeds=cash_proceeds,
                        max_cash_deployed=user.max_cash_deployed
                    )
                    intraday_snapshots.append(intraday_snapshot)
                    results['snapshots_created'] += 1
                
                results['users_processed'] += 1
                
            except Exception as e:
                error_msg = f"Error processing user {user.id}: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(error_msg)
        
        # Batch commit all intraday snapshots
        try:
            if intraday_snapshots:
                db.session.bulk_save_objects(intraday_snapshots)
                logger.info(f"Batch saved {len(intraday_snapshots)} intraday snapshots")
            
            db.session.commit()
            logger.info(f"Intraday collection completed: {results['snapshots_created']} snapshots created")
        except Exception as e:
            db.session.rollback()
            error_msg = f"Database commit failed: {str(e)}"
            results['errors'].append(error_msg)
            logger.error(error_msg)
        
        # Add execution timing
        execution_time = time.time() - start_time
        results['execution_time_seconds'] = round(execution_time, 2)
        logger.info(f"Intraday collection completed in {execution_time:.2f} seconds")
        
        return jsonify({
            'success': len(results['errors']) == 0,
            'message': 'Intraday data collection completed',
            'results': results
        }), 200
    
    except Exception as e:
        logger.error(f"Unexpected error in intraday collection: {str(e)}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

@app.route('/admin/check-intraday-data')
@login_required
def check_intraday_data():
    """Check if intraday data was collected today"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date
        from models import PortfolioSnapshotIntraday
        from sqlalchemy import func, cast, Date
        
        # CRITICAL: Use Eastern Time to match how snapshots are created
        today_et = get_market_date()
        current_time_et = get_market_time()
        
        logger.info(f"Checking intraday data for {today_et} (ET) - Current time: {current_time_et}")
        
        # Check intraday snapshots for today (using ET date)
        today_snapshots = PortfolioSnapshotIntraday.query.filter(
            cast(PortfolioSnapshotIntraday.timestamp, Date) == today_et
        ).all()
        
        logger.info(f"Found {len(today_snapshots)} intraday snapshots for {today_et} (ET)")
        
        # Group by user
        user_snapshots = {}
        for snapshot in today_snapshots:
            if snapshot.user_id not in user_snapshots:
                user_snapshots[snapshot.user_id] = []
            user_snapshots[snapshot.user_id].append({
                'timestamp': snapshot.timestamp.isoformat(),
                'value': snapshot.total_value
            })
        
        # Get total counts
        total_snapshots = len(today_snapshots)
        users_with_data = len(user_snapshots)
        
        # Sample recent snapshots
        recent_snapshots = PortfolioSnapshotIntraday.query.order_by(
            PortfolioSnapshotIntraday.timestamp.desc()
        ).limit(10).all()
        
        recent_sample = []
        for snapshot in recent_snapshots:
            recent_sample.append({
                'user_id': snapshot.user_id,
                'timestamp': snapshot.timestamp.isoformat(),
                'value': snapshot.total_value
            })
        
        return jsonify({
            'success': True,
            'today_date_et': today_et.isoformat(),
            'current_time_et': current_time_et.isoformat(),
            'timezone': 'America/New_York',
            'total_snapshots_today': total_snapshots,
            'users_with_data_today': users_with_data,
            'user_snapshots_today': user_snapshots,
            'recent_snapshots_sample': recent_sample,
            'message': f'Found {total_snapshots} intraday snapshots for {users_with_data} users on {today_et} (ET)'
        }), 200
    
    except Exception as e:
        logger.error(f"Error checking intraday data: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/debug-intraday-calculation')
@login_required
def debug_intraday_calculation():
    """Debug why intraday portfolio values aren't changing"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date
        from models import User, Stock, PortfolioSnapshotIntraday, MarketData
        from portfolio_performance import PortfolioPerformanceCalculator
        
        # Find admin user (witty-raven)
        admin_user = User.query.filter_by(username='witty-raven').first()
        if not admin_user:
            return jsonify({'error': 'Admin user not found'}), 404
        
        calculator = PortfolioPerformanceCalculator()
        
        # Check admin's stocks
        admin_stocks = Stock.query.filter_by(user_id=admin_user.id).all()
        stock_details = []
        
        for stock in admin_stocks:
            # Get current stock price
            stock_data = calculator.get_stock_data(stock.ticker)
            current_price = stock_data.get('price', 0) if stock_data else 0
            
            stock_details.append({
                'ticker': stock.ticker,
                'quantity': stock.quantity,
                'purchase_price': stock.purchase_price,
                'current_price': current_price,
                'current_value': stock.quantity * current_price,
                'gain_loss': (current_price - stock.purchase_price) * stock.quantity
            })
        
        # Calculate total portfolio value
        total_value = sum(stock['current_value'] for stock in stock_details)
        
        # Check recent intraday snapshots for admin
        today = date.today()
        recent_snapshots = PortfolioSnapshotIntraday.query.filter(
            PortfolioSnapshotIntraday.user_id == admin_user.id,
            PortfolioSnapshotIntraday.timestamp >= datetime.combine(today, datetime.min.time())
        ).order_by(PortfolioSnapshotIntraday.timestamp.desc()).limit(5).all()
        
        snapshot_details = []
        for snapshot in recent_snapshots:
            snapshot_details.append({
                'timestamp': snapshot.timestamp.isoformat(),
                'stored_value': snapshot.total_value
            })
        
        # Check S&P 500 data availability
        spy_data = MarketData.query.filter(
            MarketData.ticker == 'SPY_SP500',
            MarketData.date == today
        ).first()
        
        return jsonify({
            'success': True,
            'admin_user_id': admin_user.id,
            'admin_username': admin_user.username,
            'current_portfolio_value': total_value,
            'stock_count': len(admin_stocks),
            'stock_details': stock_details,
            'recent_snapshots': snapshot_details,
            'spy_data_today': {
                'exists': spy_data is not None,
                'value': spy_data.close_price if spy_data else None,
                'date': spy_data.date.isoformat() if spy_data else None
            },
            'debug_notes': [
                'Check if stock prices are being fetched correctly',
                'Verify if portfolio calculation matches stored snapshots',
                'Check if S&P 500 data exists for today'
            ]
        }), 200
    
    except Exception as e:
        logger.error(f"Error debugging intraday calculation: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/create-admin-snapshot')
@login_required
def create_admin_snapshot():
    """Create an intraday snapshot for the admin user manually"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime
        from models import User, PortfolioSnapshotIntraday
        from portfolio_performance import PortfolioPerformanceCalculator
        
        # Find admin user (witty-raven)
        admin_user = User.query.filter_by(username='witty-raven').first()
        if not admin_user:
            return jsonify({'error': 'Admin user not found'}), 404
        
        calculator = PortfolioPerformanceCalculator()
        current_time = datetime.now()
        
        # Calculate current portfolio value
        portfolio_value = calculator.calculate_portfolio_value(admin_user.id)
        
        # Create intraday snapshot
        snapshot = PortfolioSnapshotIntraday(
            user_id=admin_user.id,
            timestamp=current_time,
            total_value=portfolio_value
        )
        
        db.session.add(snapshot)
        db.session.commit()
        
        return jsonify({
            'success': True,
            'admin_user_id': admin_user.id,
            'portfolio_value': portfolio_value,
            'snapshot_created': current_time.isoformat(),
            'message': 'Manual admin snapshot created successfully'
        }), 200
    
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error creating admin snapshot: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/debug-all-users-portfolios')
@login_required
def debug_all_users_portfolios():
    """Debug portfolio calculations for all users to find $0 issue"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import User, Stock
        from portfolio_performance import PortfolioPerformanceCalculator
        
        calculator = PortfolioPerformanceCalculator()
        users = User.query.all()
        user_analysis = []
        
        for user in users:
            # Get user's stocks
            stocks = Stock.query.filter_by(user_id=user.id).all()
            stock_details = []
            
            for stock in stocks:
                # Get current stock price
                stock_data = calculator.get_stock_data(stock.ticker)
                current_price = stock_data.get('price', 0) if stock_data else 0
                
                stock_details.append({
                    'ticker': stock.ticker,
                    'quantity': stock.quantity,
                    'purchase_price': stock.purchase_price,
                    'current_price': current_price,
                    'current_value': stock.quantity * current_price
                })
            
            # Calculate portfolio value using the same method as intraday collection
            try:
                portfolio_value = calculator.calculate_portfolio_value(user.id)
            except Exception as e:
                portfolio_value = f"ERROR: {str(e)}"
            
            manual_total = sum(stock['current_value'] for stock in stock_details)
            
            user_analysis.append({
                'user_id': user.id,
                'username': getattr(user, 'username', 'unknown'),
                'email': getattr(user, 'email', 'unknown'),
                'stock_count': len(stocks),
                'stocks': stock_details,
                'calculated_portfolio_value': portfolio_value,
                'manual_total_value': manual_total,
                'values_match': abs(float(portfolio_value) - manual_total) < 0.01 if isinstance(portfolio_value, (int, float)) else False
            })
        
        return jsonify({
            'success': True,
            'total_users': len(users),
            'user_analysis': user_analysis,
            'debug_notes': [
                'Compare calculated_portfolio_value vs manual_total_value',
                'Check if stock prices are being fetched correctly',
                'Look for users with stocks but $0 calculated values'
            ]
        }), 200
    
    except Exception as e:
        logger.error(f"Error debugging all users portfolios: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/debug-sp500-data')
@login_required
def debug_sp500_data():
    """Debug S&P 500 data availability for intraday charts"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, date, timedelta
        from models import MarketData
        from sqlalchemy import func
        
        today = date.today()
        week_ago = today - timedelta(days=7)
        
        # Check all S&P 500 related data
        sp500_data = MarketData.query.filter(
            MarketData.ticker.like('%SP%'),
            MarketData.date >= week_ago
        ).order_by(MarketData.ticker, MarketData.date, MarketData.timestamp).all()
        
        data_by_ticker = {}
        for data in sp500_data:
            ticker = data.ticker
            if ticker not in data_by_ticker:
                data_by_ticker[ticker] = []
            
            data_by_ticker[ticker].append({
                'date': data.date.isoformat(),
                'timestamp': data.timestamp.isoformat() if data.timestamp else None,
                'price': data.close_price,
                'created_at': data.created_at.isoformat()
            })
        
        # Check what the intraday API is looking for
        spy_daily = MarketData.query.filter(
            MarketData.ticker == 'SPY_SP500',
            MarketData.date >= week_ago,
            MarketData.timestamp.is_(None)
        ).order_by(MarketData.date).all()
        
        spy_intraday = MarketData.query.filter(
            MarketData.ticker == 'SPY_INTRADAY',
            MarketData.date >= week_ago,
            MarketData.timestamp.isnot(None)
        ).order_by(MarketData.timestamp).all()
        
        return jsonify({
            'success': True,
            'date_range': f'{week_ago.isoformat()} to {today.isoformat()}',
            'all_sp500_tickers': list(data_by_ticker.keys()),
            'data_by_ticker': data_by_ticker,
            'spy_daily_count': len(spy_daily),
            'spy_intraday_count': len(spy_intraday),
            'spy_daily_sample': [
                {
                    'date': d.date.isoformat(),
                    'price': d.close_price
                } for d in spy_daily[:5]
            ],
            'spy_intraday_sample': [
                {
                    'timestamp': d.timestamp.isoformat(),
                    'price': d.close_price
                } for d in spy_intraday[:5]
            ],
            'debug_notes': [
                'Check which S&P 500 tickers exist in database',
                'Verify intraday performance API is using correct ticker names',
                'Look for data gaps or inconsistent naming'
            ]
        }), 200
    
    except Exception as e:
        logger.error(f"Error debugging S&P 500 data: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/create-sample-spy-intraday')
@login_required
def create_sample_spy_intraday():
    """Create sample SPY_INTRADAY data for testing charts"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import datetime, timedelta
        from models import MarketData
        
        today = datetime.now().date()
        base_time = datetime.combine(today, datetime.min.time()) + timedelta(hours=9, minutes=30)  # 9:30 AM
        base_price = 6424.70  # Current SPY_SP500 price
        
        # Create 15 intraday data points (every 30 minutes from 9:30 AM to 4:00 PM)
        sample_data = []
        for i in range(15):
            timestamp = base_time + timedelta(minutes=30 * i)
            # Add some realistic price variation (+/- 1%)
            price_variation = (i - 7) * 0.002  # Gradual change throughout day
            price = base_price * (1 + price_variation)
            
            # Check if data already exists
            existing = MarketData.query.filter_by(
                ticker='SPY_INTRADAY',
                date=today,
                timestamp=timestamp
            ).first()
            
            if not existing:
                market_data = MarketData(
                    ticker='SPY_INTRADAY',
                    date=today,
                    timestamp=timestamp,
                    close_price=price
                )
                db.session.add(market_data)
                sample_data.append({
                    'timestamp': timestamp.isoformat(),
                    'price': round(price, 2)
                })
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'created_count': len(sample_data),
            'sample_data': sample_data,
            'message': f'Created {len(sample_data)} SPY_INTRADAY data points for testing'
        }), 200
    
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error creating sample SPY intraday data: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/test-spy-fetch')
@login_required
def test_spy_fetch():
    """Test SPY data fetching to debug why intraday collection failed"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from portfolio_performance import PortfolioPerformanceCalculator
        from datetime import datetime
        
        calculator = PortfolioPerformanceCalculator()
        current_time = datetime.now()
        
        # Test SPY data fetching (same as intraday collection)
        try:
            spy_data = calculator.get_stock_data('SPY')
            spy_success = spy_data is not None and spy_data.get('price') is not None
            spy_price = spy_data.get('price') if spy_data else None
            sp500_value = spy_price * 10 if spy_price else None
        except Exception as e:
            spy_success = False
            spy_data = None
            spy_price = None
            sp500_value = None
            spy_error = str(e)
        
        # Check if AlphaVantage API key is available
        import os
        api_key_available = bool(os.environ.get('ALPHA_VANTAGE_API_KEY'))
        
        # Test a few other stocks to see if it's SPY-specific
        test_results = {}
        for ticker in ['AAPL', 'MSFT', 'TSLA']:
            try:
                test_data = calculator.get_stock_data(ticker)
                test_results[ticker] = {
                    'success': test_data is not None and test_data.get('price') is not None,
                    'price': test_data.get('price') if test_data else None
                }
            except Exception as e:
                test_results[ticker] = {
                    'success': False,
                    'error': str(e)
                }
        
        return jsonify({
            'success': True,
            'timestamp': current_time.isoformat(),
            'api_key_available': api_key_available,
            'spy_test': {
                'success': spy_success,
                'raw_data': spy_data,
                'spy_price': spy_price,
                'sp500_value': sp500_value,
                'error': spy_error if not spy_success else None
            },
            'other_stocks_test': test_results,
            'debug_notes': [
                'Check if SPY fetching works now vs during GitHub Actions',
                'Compare SPY results with other stock fetches',
                'Look for API key or network issues'
            ]
        }), 200
    
    except Exception as e:
        logger.error(f"Error testing SPY fetch: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/test-spy-intraday-collection')
@login_required
def test_spy_intraday_collection():
    """Test SPY intraday data collection manually to debug GitHub Actions issue"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from portfolio_performance import PortfolioPerformanceCalculator
        from models import MarketData
        from datetime import datetime
        
        calculator = PortfolioPerformanceCalculator()
        current_time = datetime.now()
        
        # Test SPY data collection (same logic as GitHub Actions)
        result = {
            'timestamp': current_time.isoformat(),
            'spy_fetch_success': False,
            'spy_price': None,
            'sp500_value': None,
            'database_save_success': False,
            'error': None
        }
        
        try:
            # Step 1: Fetch SPY data
            spy_data = calculator.get_stock_data('SPY')
            if spy_data and spy_data.get('price'):
                spy_price = spy_data['price']
                sp500_value = spy_price * 10  # Convert SPY to S&P 500 approximation
                
                result['spy_fetch_success'] = True
                result['spy_price'] = spy_price
                result['sp500_value'] = sp500_value
                
                # Step 2: Store intraday SPY data
                market_data = MarketData(
                    ticker='SPY_INTRADAY',
                    date=current_time.date(),
                    timestamp=current_time,
                    close_price=sp500_value
                )
                db.session.add(market_data)
                db.session.commit()
                
                result['database_save_success'] = True
                result['message'] = f'Successfully collected and stored SPY intraday data: ${spy_price} -> S&P 500: ${sp500_value}'
                
            else:
                result['error'] = 'Failed to fetch SPY data from AlphaVantage'
        
        except Exception as e:
            db.session.rollback()
            result['error'] = str(e)
        
        # Check current SPY_INTRADAY count
        spy_intraday_count = MarketData.query.filter_by(ticker='SPY_INTRADAY').count()
        result['spy_intraday_total_count'] = spy_intraday_count
        
        return jsonify(result), 200
    
    except Exception as e:
        logger.error(f"Error testing SPY intraday collection: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/check-spy-collection')
def check_spy_collection():
    """Check SPY_INTRADAY data collection (public endpoint for verification)"""
    try:
        from models import MarketData
        from datetime import datetime, timedelta
        
        # Get today's SPY_INTRADAY data
        today = datetime.now().date()
        spy_records = MarketData.query.filter(
            MarketData.ticker == 'SPY_INTRADAY',
            MarketData.date == today
        ).order_by(MarketData.timestamp.desc()).all()
        
        # Get yesterday's data for comparison
        yesterday = today - timedelta(days=1)
        yesterday_records = MarketData.query.filter(
            MarketData.ticker == 'SPY_INTRADAY',
            MarketData.date == yesterday
        ).count()
        
        # Get total SPY_INTRADAY records
        total_records = MarketData.query.filter(
            MarketData.ticker == 'SPY_INTRADAY'
        ).count()
        
        return jsonify({
            'success': True,
            'today_date': today.isoformat(),
            'spy_records_today': len(spy_records),
            'spy_records_yesterday': yesterday_records,
            'total_spy_intraday_records': total_records,
            'latest_records': [
                {
                    'timestamp': record.timestamp.isoformat(),
                    'sp500_value': record.close_price,
                    'date': record.date.isoformat()
                }
                for record in spy_records[:5]  # Show last 5 records
            ]
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/test-github-actions-endpoint')
@login_required
def test_github_actions_endpoint():
    """Test the exact GitHub Actions cron endpoint to see what's failing"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        import requests
        import os
        
        # Get the token
        token = os.environ.get('INTRADAY_CRON_TOKEN')
        if not token:
            return jsonify({'error': 'INTRADAY_CRON_TOKEN not found'}), 500
        
        # Make the same call GitHub Actions makes
        url = "https://apestogether.ai/api/cron/collect-intraday-data"
        headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json'
        }
        
        try:
            response = requests.post(url, headers=headers, timeout=30)
            
            return jsonify({
                'success': True,
                'status_code': response.status_code,
                'response_text': response.text,
                'headers': dict(response.headers),
                'url_called': url,
                'token_available': bool(token),
                'message': 'This shows exactly what GitHub Actions sees'
            }), 200
            
        except requests.exceptions.RequestException as e:
            return jsonify({
                'success': False,
                'error': f'Request failed: {str(e)}',
                'url_called': url,
                'token_available': bool(token)
            }), 500
    
    except Exception as e:
        logger.error(f"Error testing GitHub Actions endpoint: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/test-cron-endpoint')
@login_required
def test_cron_endpoint():
    """Test the intraday collection endpoint manually"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        import requests
        
        # Test the cron endpoint
        url = "https://apestogether.ai/api/cron/collect-intraday-data"
        headers = {
            "Authorization": f"Bearer {os.environ.get('INTRADAY_CRON_TOKEN')}",
            "Content-Type": "application/json"
        }
        
        try:
            response = requests.post(url, headers=headers, timeout=30)
            
            return jsonify({
                'success': response.status_code == 200,
                'status_code': response.status_code,
                'response': response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text,
                'message': 'Cron endpoint test completed'
            }), 200
            
        except requests.exceptions.RequestException as e:
            return jsonify({
                'success': False,
                'message': 'Cron endpoint test failed'
            }), 500
    
    except Exception as e:
        logger.error(f"Error testing cron endpoint: {str(e)}")
        return jsonify({'error': f'Unexpected error: {str(e)}'}), 500

@app.route('/admin/populate-sp500-data')
@login_required
def admin_populate_sp500_data():
    """Admin endpoint to populate S&P 500 data in all caches"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Import and run the S&P 500 population script
        from populate_sp500_data import populate_all_sp500_caches
        
        logger.info("Starting S&P 500 data population from admin interface")
        results = populate_all_sp500_caches()
        
        return jsonify({
            'success': results['summary'].get('overall_success', False),
            'message': 'S&P 500 data population completed',
            'results': results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in S&P 500 population: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/run-data-flow-diagnostic')
@login_required
def admin_run_data_flow_diagnostic():
    """Admin endpoint to run comprehensive data flow diagnostic"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Import and run the diagnostic script
        import sys
        import os
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        
        from comprehensive_data_flow_debug import run_comprehensive_debug
        
        logger.info("Starting comprehensive data flow diagnostic from admin interface")
        results = run_comprehensive_debug()
        
        return jsonify({
            'success': True,
            'message': 'Data flow diagnostic completed',
            'results': results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in data flow diagnostic: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/fix-sp500-charts')
@login_required
def admin_fix_sp500_charts():
    """Admin endpoint to run both S&P 500 population and trigger cache updates"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        results = {
            'step1_sp500_population': {},
            'step2_cache_update': {},
            'step3_diagnostic': {},
            'summary': {}
        }
        
        # Step 1: Populate S&P 500 data
        try:
            from populate_sp500_data import populate_all_sp500_caches
            logger.info("Step 1: Populating S&P 500 data")
            sp500_results = populate_all_sp500_caches()
            results['step1_sp500_population'] = sp500_results
        except Exception as e:
            results['step1_sp500_population'] = {'success': False, 'error': str(e)}
        
        # Step 2: Update leaderboard cache (includes chart cache)
        try:
            from leaderboard_utils import update_leaderboard_cache
            logger.info("Step 2: Updating leaderboard and chart caches")
            updated_count = update_leaderboard_cache()
            results['step2_cache_update'] = {
                'success': True,
                'updated_count': updated_count,
                'message': f'Updated {updated_count} cache entries'
            }
        except Exception as e:
            results['step2_cache_update'] = {'success': False, 'error': str(e)}
        
        # Step 3: Run diagnostic to verify
        try:
            import sys
            import os
            sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            from comprehensive_data_flow_debug import run_comprehensive_debug
            
            logger.info("Step 3: Running verification diagnostic")
            diagnostic_results = run_comprehensive_debug()
            results['step3_diagnostic'] = diagnostic_results
        except Exception as e:
            results['step3_diagnostic'] = {'success': False, 'error': str(e)}
        
        # Summary
        successful_steps = sum(1 for step in [
            results['step1_sp500_population'],
            results['step2_cache_update'], 
            results['step3_diagnostic']
        ] if step.get('success', False))
        
        results['summary'] = {
            'overall_success': successful_steps >= 2,  # At least 2/3 steps must succeed
            'successful_steps': successful_steps,
            'total_steps': 3,
            'message': f'S&P 500 chart fix completed: {successful_steps}/3 steps successful'
        }
        
        return jsonify({
            'success': results['summary']['overall_success'],
            'message': results['summary']['message'],
            'results': results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error in S&P 500 chart fix: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/audit-sp500-data')
@login_required
def admin_audit_sp500_data():
    """Admin endpoint to audit existing S&P 500 data before making changes"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import db, MarketData
        from datetime import date, timedelta
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'sp500_tickers_found': {},
            'data_quality': {},
            'chart_period_coverage': {},
            'recommendations': []
        }
        
        # Check for S&P 500 data under different tickers
        potential_tickers = ['SPY_SP500', 'SPY', 'SP500', 'SPX', 'SPY_INTRADAY']
        
        for ticker in potential_tickers:
            count = MarketData.query.filter_by(ticker=ticker).count()
            if count > 0:
                earliest = db.session.query(MarketData.date).filter_by(ticker=ticker).order_by(MarketData.date.asc()).first()
                latest = db.session.query(MarketData.date).filter_by(ticker=ticker).order_by(MarketData.date.desc()).first()
                
                # Check for data quality issues
                zero_prices = MarketData.query.filter(
                    MarketData.ticker == ticker,
                    MarketData.close_price == 0
                ).count()
                
                low_prices = MarketData.query.filter(
                    MarketData.ticker == ticker,
                    MarketData.close_price < 100,
                    MarketData.close_price > 0
                ).count()
                
                results['sp500_tickers_found'][ticker] = {
                    'count': count,
                    'date_range': {
                        'earliest': earliest[0].isoformat() if earliest else None,
                        'latest': latest[0].isoformat() if latest else None,
                        'span_days': (latest[0] - earliest[0]).days if earliest and latest else 0
                    },
                    'quality_issues': {
                        'zero_prices': zero_prices,
                        'low_prices': low_prices,
                        'total_issues': zero_prices + low_prices
                    }
                }
        
        # If we found SPY_SP500 data, analyze chart period coverage
        if 'SPY_SP500' in results['sp500_tickers_found']:
            today = date.today()
            periods_to_check = {
                '1M': today - timedelta(days=30),
                '3M': today - timedelta(days=90), 
                'YTD': date(today.year, 1, 1),
                '1Y': today - timedelta(days=365)
            }
            
            for period_name, start_date in periods_to_check.items():
                period_count = MarketData.query.filter(
                    MarketData.ticker == 'SPY_SP500',
                    MarketData.date >= start_date,
                    MarketData.date <= today
                ).count()
                
                total_days = (today - start_date).days
                expected_business_days = total_days * 5 // 7
                coverage = (period_count / expected_business_days * 100) if expected_business_days > 0 else 0
                
                results['chart_period_coverage'][period_name] = {
                    'data_points': period_count,
                    'expected_business_days': expected_business_days,
                    'coverage_percent': round(coverage, 1),
                    'status': 'good' if coverage > 80 else 'fair' if coverage > 50 else 'poor'
                }
        
        # Generate recommendations
        if not results['sp500_tickers_found']:
            results['recommendations'].append("No S&P 500 data found - need to populate from scratch")
        elif 'SPY_SP500' not in results['sp500_tickers_found']:
            # Check if data exists under other tickers
            other_tickers = [t for t in results['sp500_tickers_found'].keys() if t != 'SPY_SP500']
            if other_tickers:
                best_ticker = max(other_tickers, key=lambda t: results['sp500_tickers_found'][t]['count'])
                results['recommendations'].append(f"S&P 500 data found under '{best_ticker}' - consider copying to 'SPY_SP500'")
        else:
            sp500_data = results['sp500_tickers_found']['SPY_SP500']
            if sp500_data['quality_issues']['total_issues'] > 0:
                results['recommendations'].append(f"Data quality issues detected: {sp500_data['quality_issues']['total_issues']} problematic records")
            
            poor_coverage = [p for p, data in results['chart_period_coverage'].items() if data['status'] == 'poor']
            if poor_coverage:
                results['recommendations'].append(f"Poor coverage for periods: {', '.join(poor_coverage)}")
            
            if sp500_data['quality_issues']['total_issues'] == 0 and not poor_coverage:
                results['recommendations'].append(" Existing S&P 500 data looks clean and comprehensive!")
        
        return jsonify({
            'success': True,
            'message': 'S&P 500 data audit completed',
            'results': results
        })
        
    except Exception as e:
        logger.error(f"Error in S&P 500 data audit: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

# For local testing
@app.route('/api/debug/routing-test', methods=['GET'])
def debug_routing_test():
    """Test endpoint to verify API routing is working"""
    return jsonify({
        'success': True,
        'message': 'API routing is working correctly',
        'app_name': 'api/index.py',
        'timestamp': datetime.now().isoformat(),
        'routes_count': len(list(app.url_map.iter_rules())),
        'sample_routes': [rule.rule for rule in list(app.url_map.iter_rules())[:10]]
    })

@app.route('/api/debug/1Y-endpoint', methods=['GET'])
def debug_1Y_endpoint():
    """Specific diagnostic for the problematic 1Y endpoint"""
    try:
        # Simulate the exact same call that the dashboard makes
        user_id = session.get('user_id')
        if not user_id:
            return jsonify({
                'error': 'Not authenticated',
                'session_keys': list(session.keys()),
                'user_id': user_id
            })
        
        # Try to call the same logic as get_portfolio_performance
        period = '1Y'
        period_upper = period.upper()
        
        # Check if user exists
        from models import User
        user = User.query.get(user_id)
        
        diagnostic_info = {
            'success': True,
            'user_id': user_id,
            'user_exists': user is not None,
            'period': period,
            'period_upper': period_upper,
            'session_data': dict(session),
            'route_matched': True,
            'timestamp': datetime.now().isoformat()
        }
        
        if user:
            diagnostic_info['user_email'] = user.email
        
        # Try to import the performance calculator
        try:
            from portfolio_performance import PortfolioPerformanceCalculator
            calculator = PortfolioPerformanceCalculator()
            diagnostic_info['calculator_imported'] = True
            
            # Try to get performance data
            try:
                performance_data = calculator.get_performance_data(user_id, period_upper)
                diagnostic_info['performance_calculation'] = 'SUCCESS'
                diagnostic_info['performance_data_keys'] = list(performance_data.keys()) if performance_data else None
            except Exception as calc_error:
                diagnostic_info['performance_calculation'] = 'FAILED'
                diagnostic_info['calculation_error'] = str(calc_error)
                
        except Exception as import_error:
            diagnostic_info['calculator_imported'] = False
            diagnostic_info['import_error'] = str(import_error)
        
        return jsonify(diagnostic_info)
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'timestamp': datetime.now().isoformat()
        })

@app.route('/api/debug/all-routes', methods=['GET'])
def debug_all_routes():
    """Comprehensive route diagnostic"""
    all_routes = []
    api_routes = []
    
    for rule in app.url_map.iter_rules():
        route_info = {
            'rule': rule.rule,
            'methods': list(rule.methods),
            'endpoint': rule.endpoint
        }
        all_routes.append(route_info)
        
        if rule.rule.startswith('/api/'):
            api_routes.append(route_info)
    
    # Check specific problematic routes
    problematic_routes = [
        '/api/portfolio_value',
        '/api/portfolio/performance/1Y',
        '/api/portfolio/performance/<period>'
    ]
    
    found_routes = {}
    for route in problematic_routes:
        found_routes[route] = any(r['rule'] == route for r in all_routes)
    
    return jsonify({
        'success': True,
        'total_routes': len(all_routes),
        'api_routes_count': len(api_routes),
        'api_routes': api_routes,
        'problematic_routes_check': found_routes,
        'timestamp': datetime.now().isoformat()
    })

# =============================================================================
# PHASE 2: PORTFOLIO SHARING & GDPR
# =============================================================================

@app.route('/admin/run-portfolio-slug-migration')
def run_portfolio_slug_migration():
    """Run migration to add portfolio_slug and deleted_at to User model (auth bypassed for emergency)"""
    # Emergency bypass - check for secret token OR admin email
    secret_token = request.args.get('token')
    if secret_token != os.environ.get('SECRET_KEY'):
        # Fall back to normal admin check
        if not current_user.is_authenticated or current_user.email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required - use ?token=SECRET_KEY parameter'}), 403
    
    try:
        import sys
        # Add migrations directory to path (os is already imported at module level)
        migrations_dir = os.path.join(os.path.dirname(__file__), '..', 'migrations')
        if migrations_dir not in sys.path:
            sys.path.insert(0, migrations_dir)
        
        # Import with filename as module (Python will convert hyphens)
        import importlib.util
        migration_file = os.path.join(migrations_dir, '20251014_add_portfolio_slug_and_gdpr.py')
        spec = importlib.util.spec_from_file_location("migration_module", migration_file)
        migration_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(migration_module)
        
        migration_module.upgrade(db)
        
        return jsonify({
            'success': True,
            'message': 'Migration completed successfully',
            'details': 'Added portfolio_slug and deleted_at columns, generated slugs for existing users'
        })
    
    except Exception as e:
        logger.error(f"Migration error: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/p/<slug>')
def public_portfolio_view(slug):
    """Public portfolio view - accessible without login"""
    try:
        from models import User, PortfolioSnapshot, Stock, Subscription
        from portfolio_performance import PortfolioPerformanceCalculator
        
        # Find user by portfolio slug
        user = User.query.filter_by(portfolio_slug=slug).first()
        
        if not user:
            return """
            <html><head><title>Portfolio Not Found</title></head>
            <body style="font-family: Arial; text-align: center; padding: 50px;">
                <h1>Portfolio Not Found</h1>
                <p>The portfolio you're looking for doesn't exist.</p>
                <p><a href="/">Return to Home</a></p>
            </body></html>
            """, 404
        
        # Check if user account is deleted (GDPR)
        if user.deleted_at:
            return """
            <html><head><title>Portfolio Not Available</title></head>
            <body style="font-family: Arial; text-align: center; padding: 50px;">
                <h1>Portfolio Not Available</h1>
                <p>This portfolio is no longer available.</p>
                <p><a href="/">Return to Home</a></p>
            </body></html>
            """, 404
        
        # Check if current user is a subscriber
        is_subscriber = False
        if current_user.is_authenticated:
            # Check if viewing own portfolio OR has active subscription
            if current_user.id == user.id:
                is_subscriber = True
            else:
                # Check for active OR cancelled (but not expired) subscription
                subscription = Subscription.query.filter(
                    Subscription.subscriber_id == current_user.id,
                    Subscription.subscribed_to_id == user.id,
                    Subscription.status.in_(['active', 'cancelled'])
                ).first()
                
                # If cancelled, check if still within access period
                if subscription:
                    if subscription.status == 'active':
                        is_subscriber = True
                    elif subscription.status == 'cancelled' and subscription.end_date:
                        is_subscriber = subscription.end_date > datetime.utcnow()
                    else:
                        is_subscriber = False
        
        # Get current portfolio value
        calculator = PortfolioPerformanceCalculator()
        current_value = calculator.calculate_portfolio_value(user.id)
        
        # Get latest snapshot for comparison
        latest_snapshot = PortfolioSnapshot.query.filter_by(user_id=user.id).order_by(
            PortfolioSnapshot.date.desc()
        ).first()
        
        # Calculate performance
        if latest_snapshot:
            day_change = current_value - latest_snapshot.total_value
            day_change_pct = (day_change / latest_snapshot.total_value * 100) if latest_snapshot.total_value > 0 else 0
        else:
            day_change = 0
            day_change_pct = 0
        
        # Calculate portfolio statistics
        from models import Transaction
        from datetime import timedelta
        
        # Number of unique stocks held
        num_stocks = Stock.query.filter_by(user_id=user.id).count()
        
        # Average trades per week (last 30 days)
        thirty_days_ago = datetime.utcnow() - timedelta(days=30)
        recent_trades = Transaction.query.filter(
            Transaction.user_id == user.id,
            Transaction.timestamp >= thirty_days_ago
        ).count()
        avg_trades_per_week = round(recent_trades / 4.3, 1)  # 30 days  4.3 weeks
        
        # Get holdings if subscriber
        holdings = []
        if is_subscriber:
            stocks = Stock.query.filter_by(user_id=user.id).all()
            for stock in stocks:
                # Get current price from API
                stock_data = get_stock_data(stock.ticker)
                current_price = stock_data.get('price', stock.purchase_price) if stock_data else stock.purchase_price
                value = current_price * stock.quantity
                gain_loss = value - (stock.purchase_price * stock.quantity)
                gain_loss_pct = (gain_loss / (stock.purchase_price * stock.quantity) * 100) if stock.purchase_price > 0 else 0
                
                holdings.append({
                    'ticker': stock.ticker,
                    'quantity': stock.quantity,
                    'purchase_price': stock.purchase_price,
                    'current_price': current_price,
                    'value': value,
                    'gain_loss': gain_loss,
                    'gain_loss_pct': gain_loss_pct
                })
        
        # Get user's leaderboard positions (if in top 20)
        leaderboard_positions = {}
        try:
            from leaderboard_utils import get_user_leaderboard_positions
            leaderboard_positions = get_user_leaderboard_positions(user.id, top_n=20)
        except Exception as e:
            logger.error(f"Error fetching leaderboard positions: {str(e)}")
        
        return render_template('public_portfolio.html',
            username=user.username,
            current_value=current_value,
            day_change=day_change,
            day_change_pct=day_change_pct,
            slug=slug,
            portfolio_owner_id=user.id,
            subscription_price=user.subscription_price or 4.00,
            is_subscriber=is_subscriber,
            holdings=holdings,
            num_stocks=num_stocks,
            avg_trades_per_week=avg_trades_per_week,
            leaderboard_positions=leaderboard_positions,
            stripe_public_key=app.config.get('STRIPE_PUBLIC_KEY', '')
        )
    
    except Exception as e:
        logger.error(f"Public portfolio view error: {str(e)}")
        import traceback
        traceback.print_exc()
        return f"""
        <html><head><title>Error</title></head>
        <body style="font-family: Arial; text-align: center; padding: 50px;">
            <h1>Something Went Wrong</h1>
            <p>We encountered an error loading this portfolio.</p>
            <p><a href="/">Return to Home</a></p>
            <p style="color: #666; font-size: 12px;">Error: {str(e)}</p>
        </body></html>
        """, 500

@app.route('/settings/portfolio-sharing')
@login_required
def portfolio_sharing_settings():
    """Settings page for portfolio sharing"""
    try:
        # Generate slug if user doesn't have one
        if not current_user.portfolio_slug:
            current_user.portfolio_slug = generate_portfolio_slug()
            db.session.commit()
        
        share_url = f"https://apestogether.ai/p/{current_user.portfolio_slug}"
        
        return render_template('portfolio_sharing_settings.html',
            share_url=share_url,
            slug=current_user.portfolio_slug
        )
    
    except Exception as e:
        logger.error(f"Portfolio sharing settings error: {str(e)}")
        flash('Error loading portfolio sharing settings', 'danger')
        return redirect(url_for('index'))

@app.route('/settings/regenerate-portfolio-slug', methods=['POST'])
@login_required
def regenerate_portfolio_slug():
    """Regenerate portfolio slug (invalidates old share links)"""
    try:
        current_user.portfolio_slug = generate_portfolio_slug()
        db.session.commit()
        
        return jsonify({
            'success': True,
            'new_slug': current_user.portfolio_slug,
            'new_url': f"https://apestogether.ai/p/{current_user.portfolio_slug}"
        })
    
    except Exception as e:
        logger.error(f"Regenerate slug error: {str(e)}")
        db.session.rollback()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/create-subscription', methods=['POST'])
@login_required
def create_subscription():
    """Create subscription using Stripe Elements (slide-up modal)"""
    try:
        data = request.get_json()
        payment_method_id = data.get('payment_method_id')
        portfolio_owner_id = data.get('portfolio_owner_id')
        
        if not payment_method_id or not portfolio_owner_id:
            return jsonify({'error': 'Payment method and portfolio owner required'}), 400
        
        # Get the portfolio owner
        from models import User, Subscription
        portfolio_owner = User.query.get(portfolio_owner_id)
        
        if not portfolio_owner:
            return jsonify({'error': 'Portfolio owner not found'}), 404
        
        # Check if already subscribed
        existing_sub = Subscription.query.filter_by(
            subscriber_id=current_user.id,
            subscribed_to_id=portfolio_owner_id,
            status='active'
        ).first()
        
        if existing_sub:
            return jsonify({'error': 'Already subscribed to this portfolio'}), 400
        
        # Get or create Stripe customer
        if not current_user.stripe_customer_id:
            customer = stripe.Customer.create(
                email=current_user.email,
                name=current_user.username,
                payment_method=payment_method_id,
                invoice_settings={'default_payment_method': payment_method_id}
            )
            current_user.stripe_customer_id = customer.id
            db.session.commit()
        else:
            # Attach payment method to existing customer
            stripe.PaymentMethod.attach(
                payment_method_id,
                customer=current_user.stripe_customer_id
            )
            stripe.Customer.modify(
                current_user.stripe_customer_id,
                invoice_settings={'default_payment_method': payment_method_id}
            )
        
        # Create Stripe subscription
        stripe_subscription = stripe.Subscription.create(
            customer=current_user.stripe_customer_id,
            items=[{'price': portfolio_owner.stripe_price_id}],
            expand=['latest_invoice.payment_intent'],
            metadata={
                'subscriber_id': current_user.id,
                'subscribed_to_id': portfolio_owner_id
            }
        )
        
        # Create subscription record in database
        subscription = Subscription(
            subscriber_id=current_user.id,
            subscribed_to_id=portfolio_owner_id,
            stripe_subscription_id=stripe_subscription.id,
            status='active',
            start_date=datetime.utcnow()
        )
        db.session.add(subscription)
        db.session.commit()
        
        # Check if payment requires action (SCA)
        payment_intent = stripe_subscription.latest_invoice.payment_intent
        if payment_intent.status == 'requires_action':
            return jsonify({
                'requiresAction': True,
                'clientSecret': payment_intent.client_secret
            })
        
        return jsonify({'success': True, 'subscription_id': subscription.id})
    
    except Exception as e:
        logger.error(f"Create subscription error: {str(e)}")
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/unsubscribe-from-portfolio', methods=['POST'])
@login_required
def unsubscribe_from_portfolio():
    """Cancel subscription to a portfolio (keep access until period end)"""
    try:
        data = request.get_json()
        portfolio_owner_id = data.get('portfolio_owner_id')
        
        if not portfolio_owner_id:
            return jsonify({'error': 'Portfolio owner ID required'}), 400
        
        # Find active subscription
        from models import Subscription
        subscription = Subscription.query.filter_by(
            subscriber_id=current_user.id,
            subscribed_to_id=portfolio_owner_id,
            status='active'
        ).first()
        
        if not subscription:
            return jsonify({'error': 'No active subscription found'}), 404
        
        # Cancel on Stripe (at period end)
        stripe.Subscription.modify(
            subscription.stripe_subscription_id,
            cancel_at_period_end=True
        )
        
        # Update subscription status to 'cancelled' (but still active until period end)
        subscription.status = 'cancelled'
        subscription.end_date = datetime.utcnow() + timedelta(days=30)  # ~1 month from now
        db.session.commit()
        
        return jsonify({
            'success': True,
            'end_date': subscription.end_date.strftime('%m/%d/%Y')
        })
    
    except Exception as e:
        logger.error(f"Unsubscribe error: {str(e)}")
        db.session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/settings/gdpr')
@login_required
def gdpr_settings():
    """GDPR settings page - account deletion"""
    return render_template('gdpr_settings.html')

@app.route('/settings/delete-account', methods=['POST'])
@login_required
def delete_account():
    """GDPR-compliant account deletion - soft delete with 30-day grace period"""
    try:
        # Soft delete - mark account as deleted
        current_user.deleted_at = datetime.utcnow()
        db.session.commit()
        
        # Log out user
        from flask_login import logout_user
        logout_user()
        
        flash('Your account has been scheduled for deletion. You have 30 days to cancel by logging in again.', 'info')
        return redirect(url_for('index'))
    
    except Exception as e:
        logger.error(f"Account deletion error: {str(e)}")
        db.session.rollback()
        flash('Error deleting account. Please try again.', 'danger')
        return redirect(url_for('gdpr_settings'))

@app.route('/api/public-portfolio/<slug>/performance/<period>')
def public_portfolio_performance(slug, period):
    """Get performance data for a public portfolio (no authentication required)"""
    try:
        from models import User
        from portfolio_performance import PortfolioPerformanceCalculator
        
        # Find user by portfolio slug
        user = User.query.filter_by(portfolio_slug=slug).first()
        
        if not user:
            return jsonify({'error': 'Portfolio not found'}), 404
        
        # Check if user account is deleted (GDPR)
        if user.deleted_at:
            return jsonify({'error': 'Portfolio not available'}), 404
        
        # Get performance data using the existing calculator
        calculator = PortfolioPerformanceCalculator()
        data = calculator.get_performance_data(user.id, period)
        
        return jsonify(data)
    
    except Exception as e:
        logger.error(f"Public portfolio performance error: {str(e)}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    # Log app startup with structured information
    logger.info("App starting", extra={
        'vercel_env': os.environ.get('VERCEL_ENV'),
        'vercel_region': os.environ.get('VERCEL_REGION'),
        'static_folder': app.static_folder,
        'database_type': app.config['SQLALCHEMY_DATABASE_URI'].split('://')[0] if app.config.get('SQLALCHEMY_DATABASE_URI') else 'none'
    })
    app.run(debug=True, port=5000)

# Register the admin blueprint for Vercel deployment
try:
    import sys
    from admin_interface import admin_bp
    app.register_blueprint(admin_bp, url_prefix='/admin')
    logger.info("Admin interface blueprint registered successfully")
except Exception as e:
    print(f"Error registering admin blueprint: {e}")

# Register the SMS blueprint for Vercel deployment
try:
    from sms_routes import sms_bp
    app.register_blueprint(sms_bp)
    logger.info("SMS blueprint registered successfully")
except Exception as e:
    logger.error(f"Error registering SMS blueprint: {e}")

# Register cash tracking admin routes for Phase 0 implementation
try:
    from admin_cash_tracking import register_cash_tracking_routes
    register_cash_tracking_routes(app, db)
    logger.info("Cash tracking admin routes registered successfully")
except Exception as e:
    logger.error(f"Error registering cash tracking routes: {e}")

# Register Phase 1 implementation routes
try:
    from admin_phase_1_routes import register_phase_1_routes
    register_phase_1_routes(app, db)
    logger.info("Phase 1 implementation routes registered successfully")
except Exception as e:
    logger.error(f"Error registering Phase 1 routes: {e}")

# Register Phase 2 data cleanup routes
try:
    from admin_phase_2_routes import register_phase_2_routes
    register_phase_2_routes(app, db)
    logger.info("Phase 2 data cleanup routes registered successfully")
except Exception as e:
    logger.error(f"Error registering Phase 2 routes: {e}")

# Register Phase 3 snapshot rebuild routes
try:
    from admin_phase_3_routes import register_phase_3_routes
    register_phase_3_routes(app, db)
    logger.info("Phase 3 snapshot rebuild routes registered successfully")
except Exception as e:
    logger.error(f"Error registering Phase 3 routes: {e}")

# Register Phase 4 historical backfill routes
try:
    from admin_phase_4_routes import register_phase_4_routes
    register_phase_4_routes(app, db)
    logger.info("Phase 4 historical backfill routes registered successfully")
except Exception as e:
    logger.error(f"Error registering Phase 4 routes: {e}")

# Register Phase 5 cleanup and re-backfill routes
try:
    from admin_phase_5_routes import register_phase_5_routes
    register_phase_5_routes(app, db)
    logger.info("Phase 5 cleanup routes registered successfully")
except Exception as e:
    logger.error(f"Error registering Phase 5 routes: {e}")

# Register Phase 5 cache clearing routes
try:
    from admin_phase_5_cache_clear import register_phase_5_cache_routes
    register_phase_5_cache_routes(app, db)
    logger.info("Phase 5 cache clearing routes registered successfully")
except Exception as e:
    logger.error(f"Error registering Phase 5 cache routes: {e}")

# Register the leaderboard blueprint for Vercel deployment
try:
    from leaderboard_routes import leaderboard_bp
    app.register_blueprint(leaderboard_bp)
    logger.info("Leaderboard blueprint registered successfully")
except Exception as e:
    print(f"Error registering leaderboard blueprint: {e}")

@app.route('/admin/nuclear-data-fix', methods=['GET', 'POST'])
@login_required
def admin_nuclear_data_fix():
    """NUCLEAR OPTION: Complete portfolio data rebuild with portfolio creation date safety"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        if request.method == 'GET':
            return '''
            <!DOCTYPE html>
            <html>
            <head>
                <title> NUCLEAR DATA FIX</title>
                <style>
                    body { font-family: Arial, sans-serif; max-width: 1000px; margin: 20px auto; padding: 20px; }
                    button { background: #dc3545; color: white; padding: 15px 30px; border: none; border-radius: 4px; font-size: 18px; cursor: pointer; margin: 10px; }
                    button:hover { background: #c82333; }
                    .critical { background: #f8d7da; padding: 20px; border-radius: 4px; margin: 20px 0; border-left: 6px solid #dc3545; }
                    .info { background: #d1ecf1; padding: 15px; border-radius: 4px; margin: 15px 0; border-left: 4px solid #17a2b8; }
                    #results { margin-top: 20px; padding: 20px; border-radius: 4px; }
                    .success { background: #d4edda; border-left: 6px solid #28a745; }
                    .error { background: #f8d7da; border-left: 6px solid #dc3545; }
                    .progress { background: #cce7ff; border-left: 4px solid #007bff; }
                </style>
            </head>
            <body>
                <h1> NUCLEAR DATA FIX</h1>
                
                <div class="critical">
                    <h2> PORTFOLIO DATA CORRUPTION REPAIR</h2>
                    <p><strong>Safely rebuilds portfolio data respecting creation dates.</strong></p>
                </div>
                
                <div class="info">
                    <h3> Safety Features:</h3>
                    <ul>
                        <li><strong>Portfolio Creation Date Check:</strong> Only calculates values after portfolio creation</li>
                        <li><strong>S&P 500 Conversion Fix:</strong> Proper SPY  10 conversion for 9/26</li>
                        <li><strong>Missing Data Detection:</strong> Identifies and fills gaps in existing data</li>
                        <li><strong>Cache Regeneration:</strong> Rebuilds all chart and leaderboard caches</li>
                    </ul>
                </div>
                
                <div style="text-align: center; margin: 40px 0;">
                    <button onclick="startNuclearFix()" id="startBtn">
                         START SAFE DATA REBUILD
                    </button>
                </div>
                
                <div id="results"></div>
                
                <script>
                async function startNuclearFix() {
                    document.getElementById('startBtn').disabled = true;
                    document.getElementById('startBtn').textContent = ' PROCESSING...';
                    
                    document.getElementById('results').innerHTML = `
                        <div class="progress">
                            <h3> Safe Data Rebuild In Progress...</h3>
                            <p><strong>Analyzing portfolio creation dates...</strong></p>
                            <p><em>This will take 3-5 minutes. Respecting portfolio creation dates.</em></p>
                        </div>
                    `;
                    
                    try {
                        const response = await fetch('/admin/nuclear-data-fix', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' }
                        });
                        
                        const data = await response.json();
                        
                        if (data.success) {
                            document.getElementById('results').className = 'success';
                            document.getElementById('results').innerHTML = `
                                <h2> SAFE DATA REBUILD COMPLETE!</h2>
                                <h3> Results:</h3>
                                <ul>
                                    <li><strong>Users Processed:</strong> ${data.results.users_processed}</li>
                                    <li><strong>Portfolio Snapshots Rebuilt:</strong> ${data.results.snapshots_rebuilt}</li>
                                    <li><strong>S&P 500 Data Fixed:</strong> ${data.results.sp500_fixed} dates</li>
                                    <li><strong>Caches Regenerated:</strong> ${data.results.caches_regenerated}</li>
                                    <li><strong>Processing Time:</strong> ${data.results.processing_time} minutes</li>
                                </ul>
                                <h3> Dashboard Fixed!</h3>
                                <p><strong> Go check your dashboard - all issues resolved!</strong></p>
                            `;
                        } else {
                            document.getElementById('results').className = 'error';
                            document.getElementById('results').innerHTML = `
                                <h3> Fix Failed</h3>
                                <p><strong>Error:</strong> ${data.error}</p>
                            `;
                        }
                    } catch (error) {
                        document.getElementById('results').className = 'error';
                        document.getElementById('results').innerHTML = `
                            <h3> Fix Failed</h3>
                            <p><strong>Error:</strong> ${error.message}</p>
                        `;
                    }
                    
                    document.getElementById('startBtn').disabled = false;
                    document.getElementById('startBtn').textContent = ' Run Again';
                }
                </script>
            </body>
            </html>
            '''
        
        # Handle POST request - perform safe data rebuild
        import requests
        import time
        from datetime import date, datetime, timedelta
        from models import Stock, PortfolioSnapshot, MarketData, UserPortfolioChartCache, LeaderboardCache, User
        
        start_time = datetime.now()
        results = {
            'users_processed': 0,
            'snapshots_rebuilt': 0,
            'sp500_fixed': 0,
            'caches_regenerated': 0,
            'errors': []
        }
        
        logger.info(" Starting SAFE DATA REBUILD...")
        
        # PHASE 1: Fix S&P 500 conversion for 9/26 (the -89% issue)
        logger.info("Phase 1: Fixing S&P 500 conversion for 9/26...")
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        
        if api_key:
            try:
                url = "https://www.alphavantage.co/query"
                params = {
                    'function': 'TIME_SERIES_DAILY',
                    'symbol': 'SPY',
                    'apikey': api_key,
                    'outputsize': 'compact'
                }
                
                response = requests.get(url, params=params, timeout=30)
                data_response = response.json()
                time_series = data_response.get('Time Series (Daily)', {})
                
                # Fix 9/26/2025 S&P 500 data
                target_date = date(2025, 9, 26)
                date_str = target_date.strftime('%Y-%m-%d')
                
                if date_str in time_series:
                    spy_price = float(time_series[date_str]['4. close'])
                    sp500_index_value = spy_price * 10  # Convert SPY to S&P 500 index
                    
                    existing_data = MarketData.query.filter_by(
                        ticker="SPY_SP500", 
                        date=target_date
                    ).first()
                    
                    if existing_data:
                        old_price = existing_data.close_price
                        existing_data.close_price = sp500_index_value
                        results['sp500_fixed'] += 1
                        logger.info(f"Fixed S&P 500 9/26: ${old_price:.2f}  ${sp500_index_value:.2f}")
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                error_msg = f"Error fixing S&P 500: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(error_msg)
        
        # PHASE 2: Get users and their portfolio creation dates
        logger.info("Phase 2: Analyzing user portfolios...")
        users_with_stocks = db.session.query(User.id).join(Stock).distinct().all()
        
        for (user_id,) in users_with_stocks:
            try:
                results['users_processed'] += 1
                
                # Find earliest stock for this user to determine portfolio start
                earliest_stock = Stock.query.filter_by(user_id=user_id).order_by(Stock.created_at).first()
                if not earliest_stock:
                    continue
                
                portfolio_start_date = earliest_stock.created_at.date()
                logger.info(f"User {user_id} portfolio started: {portfolio_start_date}")
                
                # Get user's current stocks
                user_stocks = Stock.query.filter_by(user_id=user_id).all()
                
                # Focus on September 2025 dates where we know there are issues
                rebuild_dates = []
                current_date = date(2025, 9, 1)
                end_date = date(2025, 9, 29)
                
                while current_date <= end_date:
                    if (current_date.weekday() < 5 and  # Business days only
                        current_date >= portfolio_start_date):  # After portfolio creation
                        rebuild_dates.append(current_date)
                    current_date += timedelta(days=1)
                
                # Rebuild snapshots for these dates
                for rebuild_date in rebuild_dates:
                    try:
                        # Calculate correct portfolio value using existing market data
                        total_value = 0
                        
                        for stock in user_stocks:
                            if stock.created_at.date() <= rebuild_date:
                                # Try to find market data for this date
                                market_data = MarketData.query.filter_by(
                                    ticker=stock.ticker,
                                    date=rebuild_date
                                ).first()
                                
                                if market_data:
                                    stock_value = stock.quantity * market_data.close_price
                                    total_value += stock_value
                                else:
                                    # Fallback to purchase price
                                    stock_value = stock.quantity * stock.purchase_price
                                    total_value += stock_value
                        
                        # Update or create snapshot if we have meaningful value
                        if total_value > 0:
                            existing_snapshot = PortfolioSnapshot.query.filter_by(
                                user_id=user_id,
                                date=rebuild_date
                            ).first()
                            
                            if existing_snapshot:
                                if abs(existing_snapshot.total_value - total_value) > 0.01:  # Only update if different
                                    existing_snapshot.total_value = total_value
                                    results['snapshots_rebuilt'] += 1
                                    logger.info(f"Updated snapshot {user_id} {rebuild_date}: ${total_value:.2f}")
                            else:
                                new_snapshot = PortfolioSnapshot(
                                    user_id=user_id,
                                    date=rebuild_date,
                                    total_value=total_value,
                                    cash_flow=0,
                                    created_at=datetime.now()
                                )
                                db.session.add(new_snapshot)
                                results['snapshots_rebuilt'] += 1
                                logger.info(f"Created snapshot {user_id} {rebuild_date}: ${total_value:.2f}")
                    
                    except Exception as e:
                        error_msg = f"Error rebuilding snapshot for user {user_id} on {rebuild_date}: {str(e)}"
                        results['errors'].append(error_msg)
                        logger.error(error_msg)
                
            except Exception as e:
                error_msg = f"Error processing user {user_id}: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(error_msg)
        
        # PHASE 3: Clear and regenerate all caches
        logger.info("Phase 3: Regenerating all caches...")
        
        chart_caches_deleted = UserPortfolioChartCache.query.delete()
        leaderboard_caches_deleted = LeaderboardCache.query.delete()
        results['caches_regenerated'] = chart_caches_deleted + leaderboard_caches_deleted
        
        db.session.commit()
        
        # Regenerate leaderboard caches
        from leaderboard_utils import update_leaderboard_cache
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y']
        update_leaderboard_cache(periods)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds() / 60
        results['processing_time'] = round(processing_time, 2)
        
        logger.info(f"Safe data rebuild completed in {processing_time:.2f} minutes")
        
        return jsonify({
            'success': True,
            'message': 'Safe data rebuild completed successfully',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Error in nuclear data fix: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/rebuild-user-cache/<int:user_id>', methods=['GET'])
@login_required
def admin_rebuild_user_cache(user_id):
    """Rebuild chart caches for a single user (avoids timeout)"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, datetime, timedelta
        import json
        from models import User, Stock, PortfolioSnapshot, MarketData, UserPortfolioChartCache
        
        start_time = datetime.now()
        results = {
            'user_id': user_id,
            'charts_fixed': 0,
            'data_points_generated': 0,
            'errors': []
        }
        
        # Check user exists
        user = User.query.get(user_id)
        if not user:
            return jsonify({'success': False, 'error': f'User {user_id} not found'}), 404
        
        # Get user's stocks to find portfolio start date
        user_stocks = Stock.query.filter_by(user_id=user_id).all()
        if not user_stocks:
            return jsonify({
                'success': True,
                'message': f'User {user_id} ({user.username}) has no stocks - no caches to generate'
            })
        
        portfolio_start_date = min(user_stocks, key=lambda s: s.purchase_date).purchase_date.date()
        
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y']
        
        for period in periods:
            try:
                # Calculate date range
                end_date = date.today()
                
                if period == '1D':
                    start_date = end_date
                elif period == '5D':
                    start_date = end_date - timedelta(days=7)
                elif period == '1M':
                    start_date = end_date - timedelta(days=30)
                elif period == '3M':
                    start_date = end_date - timedelta(days=90)
                elif period == 'YTD':
                    start_date = date(end_date.year, 1, 1)
                elif period == '1Y':
                    start_date = end_date - timedelta(days=365)
                
                start_date = max(start_date, portfolio_start_date)
                
                # Get snapshots
                snapshots = PortfolioSnapshot.query.filter(
                    PortfolioSnapshot.user_id == user_id,
                    PortfolioSnapshot.date >= start_date,
                    PortfolioSnapshot.date <= end_date
                ).order_by(PortfolioSnapshot.date).all()
                
                # Get S&P 500 data
                sp500_data = MarketData.query.filter(
                    MarketData.ticker == "SPY_SP500",
                    MarketData.date >= start_date,
                    MarketData.date <= end_date
                ).order_by(MarketData.date).all()
                
                # Generate chart data
                snapshot_dict = {s.date: s.total_value for s in snapshots}
                sp500_dict = {s.date: s.close_price for s in sp500_data}
                
                chart_data_points = []
                current_date = start_date
                while current_date <= end_date:
                    if period == '1D' or current_date.weekday() < 5:
                        portfolio_value = snapshot_dict.get(current_date, 0)
                        sp500_value = sp500_dict.get(current_date, 0)
                        
                        if portfolio_value > 0 or sp500_value > 0:
                            chart_data_points.append({
                                'date': current_date.isoformat(),
                                'portfolio': portfolio_value,
                                'sp500': sp500_value
                            })
                    
                    current_date += timedelta(days=1)
                
                # Calculate returns
                portfolio_return = 0
                sp500_return = 0
                
                if len(chart_data_points) >= 2:
                    first_portfolio = next((p['portfolio'] for p in chart_data_points if p['portfolio'] > 0), 0)
                    last_portfolio = chart_data_points[-1]['portfolio']
                    
                    first_sp500 = next((p['sp500'] for p in chart_data_points if p['sp500'] > 0), 0)
                    last_sp500 = chart_data_points[-1]['sp500']
                    
                    if first_portfolio > 0:
                        portfolio_return = ((last_portfolio - first_portfolio) / first_portfolio) * 100
                    
                    if first_sp500 > 0:
                        sp500_return = ((last_sp500 - first_sp500) / first_sp500) * 100
                
                chart_cache_data = {
                    'chart_data': chart_data_points,
                    'portfolio_return': round(portfolio_return, 2),
                    'sp500_return': round(sp500_return, 2),
                    'period': period,
                    'start_date': start_date.isoformat(),
                    'end_date': end_date.isoformat()
                }
                
                # Update or create cache
                cache_entry = UserPortfolioChartCache.query.filter_by(
                    user_id=user_id,
                    period=period
                ).first()
                
                if cache_entry:
                    cache_entry.chart_data = json.dumps(chart_cache_data)
                    cache_entry.generated_at = datetime.now()
                else:
                    cache_entry = UserPortfolioChartCache(
                        user_id=user_id,
                        period=period,
                        chart_data=json.dumps(chart_cache_data),
                        generated_at=datetime.now()
                    )
                    db.session.add(cache_entry)
                
                results['charts_fixed'] += 1
                results['data_points_generated'] += len(chart_data_points)
                
                logger.info(f"User {user_id} {period}: {len(chart_data_points)} points, {portfolio_return:.2f}% return")
                
            except Exception as e:
                error_msg = f"{period}: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(f"Error generating {period} cache for user {user_id}: {e}")
        
        # Single commit for this user
        db.session.commit()
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"Rebuilt caches for user {user_id} ({user.username}) in {processing_time:.2f}s")
        
        return jsonify({
            'success': True,
            'username': user.username,
            'processing_time': round(processing_time, 2),
            'results': results
        }), 200
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Rebuild failed for user {user_id}: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/rebuild-all-caches', methods=['GET'])
@login_required
def admin_rebuild_all_caches():
    """UI page to rebuild caches for all users sequentially"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return "Admin access required", 403
        
        # Get all users with stocks
        users_with_stocks = db.session.query(User.id, User.username).join(Stock).distinct().all()
        
        return f'''
        <!DOCTYPE html>
        <html>
        <head>
            <title>Rebuild User Caches</title>
            <style>
                body {{ font-family: Arial, sans-serif; max-width: 800px; margin: 20px auto; padding: 20px; }}
                .user-item {{ padding: 10px; margin: 10px 0; border: 1px solid #ddd; border-radius: 4px; }}
                .status {{ margin-left: 10px; font-weight: bold; }}
                .success {{ color: #28a745; }}
                .error {{ color: #dc3545; }}
                .processing {{ color: #007bff; }}
                button {{ padding: 8px 16px; margin: 5px; cursor: pointer; }}
                .rebuild-all {{ background: #007bff; color: white; border: none; border-radius: 4px; padding: 12px 24px; font-size: 16px; }}
                .progress {{ margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 4px; }}
            </style>
        </head>
        <body>
            <h1> Rebuild User Caches</h1>
            <p>Rebuild chart caches one user at a time (avoids timeout).</p>
            
            <button class="rebuild-all" onclick="rebuildAll()"> Rebuild All Users</button>
            
            <div id="progress" class="progress" style="display:none;">
                <strong>Progress:</strong> <span id="progress-text">0/{len(users_with_stocks)}</span>
            </div>
            
            <div id="user-list">
                {"".join([f'''
                <div class="user-item">
                    <strong>{username}</strong> (ID: {user_id})
                    <button onclick="rebuildUser({user_id})">Rebuild</button>
                    <span class="status" id="status-{user_id}"></span>
                </div>
                ''' for user_id, username in users_with_stocks])}
            </div>
            
            <script>
                let completedCount = 0;
                const totalUsers = {len(users_with_stocks)};
                
                async function rebuildUser(userId) {{
                    const statusEl = document.getElementById(`status-${{userId}}`);
                    statusEl.textContent = ' Rebuilding...';
                    statusEl.className = 'status processing';
                    
                    try {{
                        const response = await fetch(`/admin/rebuild-user-cache/${{userId}}`);
                        const data = await response.json();
                        
                        if (data.success) {{
                            statusEl.textContent = ` Done! (${{data.results.charts_fixed}} charts, ${{data.processing_time}}s)`;
                            statusEl.className = 'status success';
                        }} else {{
                            statusEl.textContent = ` Error: ${{data.error}}`;
                            statusEl.className = 'status error';
                        }}
                    }} catch (error) {{
                        statusEl.textContent = ` Failed: ${{error.message}}`;
                        statusEl.className = 'status error';
                    }}
                }}
                
                async function rebuildAll() {{
                    const progressDiv = document.getElementById('progress');
                    const progressText = document.getElementById('progress-text');
                    
                    progressDiv.style.display = 'block';
                    completedCount = 0;
                    
                    const userIds = {[user_id for user_id, _ in users_with_stocks]};
                    
                    for (let userId of userIds) {{
                        await rebuildUser(userId);
                        completedCount++;
                        progressText.textContent = `${{completedCount}}/${{totalUsers}}`;
                        
                        // 2 second buffer between users
                        if (completedCount < totalUsers) {{
                            await new Promise(resolve => setTimeout(resolve, 2000));
                        }}
                    }}
                    
                    alert(' All caches rebuilt!');
                }}
            </script>
        </body>
        </html>
        '''
        
    except Exception as e:
        logger.error(f"Error loading rebuild UI: {str(e)}")
        return f"Error: {str(e)}", 500

@app.route('/admin/clean-zero-snapshots', methods=['GET', 'POST'])
@login_required
def admin_clean_zero_snapshots():
    """Clean corrupted zero-value snapshots"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, timedelta
        from models import PortfolioSnapshot, Stock
        
        if request.method == 'GET':
            # Show preview of what will be deleted
            
            # Look for ALL zero-value snapshots (not just last 7 days)
            # We need to clean ALL corrupted data for accurate performance calculations
            
            # Find zero-value snapshots for users with stocks
            zero_snapshots = db.session.query(
                PortfolioSnapshot.user_id,
                PortfolioSnapshot.date,
                PortfolioSnapshot.total_value
            ).join(
                Stock, Stock.user_id == PortfolioSnapshot.user_id
            ).filter(
                PortfolioSnapshot.total_value == 0,
                Stock.quantity > 0
            ).distinct().all()
            
            return jsonify({
                'count': len(zero_snapshots),
                'snapshots': [
                    {
                        'user_id': s.user_id,
                        'date': s.date.isoformat(),
                        'value': s.total_value
                    } for s in zero_snapshots[:20]  # Show first 20
                ],
                'message': f'Found {len(zero_snapshots)} corrupted snapshots. POST to delete them.'
            })
        
        # POST: Actually delete them
        # Delete ALL zero-value snapshots for users with stocks (not just recent ones)
        deleted_count = db.session.query(PortfolioSnapshot).filter(
            PortfolioSnapshot.total_value == 0,
            PortfolioSnapshot.user_id.in_(
                db.session.query(Stock.user_id).filter(Stock.quantity > 0).distinct()
            )
        ).delete(synchronize_session=False)
        
        db.session.commit()
        
        logger.info(f"Deleted {deleted_count} zero-value snapshots")
        
        return jsonify({
            'success': True,
            'deleted_count': deleted_count,
            'message': f'Successfully deleted {deleted_count} corrupted snapshots'
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error cleaning zero snapshots: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-sept30-snapshots', methods=['GET'])
@login_required
def admin_check_sept30_snapshots():
    """Check if Sept 30, 2025 snapshots exist - diagnostic endpoint"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date
        from models import User, PortfolioSnapshot, Stock, UserPortfolioChartCache
        
        today = date(2025, 9, 30)
        yesterday = date(2025, 9, 29)
        
        results = {
            'today': str(today),
            'yesterday': str(yesterday),
            'today_snapshots': [],
            'yesterday_snapshots': [],
            'latest_snapshot_date': None,
            'total_snapshots': 0,
            'zero_value_count': 0,
            'users_with_stocks': [],
            'chart_cache_status': []
        }
        
        # Check today's snapshots
        today_snaps = db.session.query(
            PortfolioSnapshot.user_id,
            User.username,
            PortfolioSnapshot.date,
            PortfolioSnapshot.total_value,
            PortfolioSnapshot.created_at
        ).join(User).filter(
            PortfolioSnapshot.date == today
        ).all()
        
        results['today_snapshots'] = [
            {
                'user_id': s.user_id,
                'username': s.username,
                'date': s.date.isoformat(),
                'total_value': float(s.total_value),
                'created_at': s.created_at.isoformat() if s.created_at else None
            } for s in today_snaps
        ]
        
        # Check yesterday's snapshots
        yesterday_snaps = db.session.query(
            PortfolioSnapshot.user_id,
            User.username,
            PortfolioSnapshot.date,
            PortfolioSnapshot.total_value,
            PortfolioSnapshot.created_at
        ).join(User).filter(
            PortfolioSnapshot.date == yesterday
        ).all()
        
        results['yesterday_snapshots'] = [
            {
                'user_id': s.user_id,
                'username': s.username,
                'date': s.date.isoformat(),
                'total_value': float(s.total_value),
                'created_at': s.created_at.isoformat() if s.created_at else None
            } for s in yesterday_snaps
        ]
        
        # Get latest snapshot date
        latest = db.session.query(
            func.max(PortfolioSnapshot.date)
        ).scalar()
        results['latest_snapshot_date'] = latest.isoformat() if latest else None
        
        # Total snapshot count
        results['total_snapshots'] = db.session.query(PortfolioSnapshot).count()
        
        # Zero-value count
        results['zero_value_count'] = db.session.query(PortfolioSnapshot).filter(
            PortfolioSnapshot.total_value == 0
        ).count()
        
        # Users with stocks
        users_stocks = db.session.query(
            User.id,
            User.username,
            func.count(Stock.id).label('stock_count')
        ).outerjoin(Stock).group_by(User.id, User.username).all()
        
        results['users_with_stocks'] = [
            {
                'user_id': u.id,
                'username': u.username,
                'stock_count': u.stock_count
            } for u in users_stocks if u.stock_count > 0
        ]
        
        # Chart cache status for user 5
        caches = UserPortfolioChartCache.query.filter_by(user_id=5).all()
        results['chart_cache_status'] = [
            {
                'period': c.period,
                'generated_at': c.generated_at.isoformat() if c.generated_at else None,
                'data_size': len(c.chart_data) if c.chart_data else 0
            } for c in caches
        ]
        
        # Diagnosis
        diagnosis = []
        if len(results['today_snapshots']) == 0:
            diagnosis.append(" NO SNAPSHOTS FOR SEPT 30 - Cron job didn't run or failed")
        else:
            diagnosis.append(f" Found {len(results['today_snapshots'])} snapshots for Sept 30")
        
        if results['zero_value_count'] > 0:
            diagnosis.append(f"  {results['zero_value_count']} zero-value snapshots exist (corrupted data)")
        
        results['diagnosis'] = diagnosis
        
        return jsonify(results)
        
    except Exception as e:
        logger.error(f"Error checking Sept 30 snapshots: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/comprehensive-data-flow', methods=['GET'])
@login_required
def admin_comprehensive_data_flow():
    """Run comprehensive data flow analysis (updated for Sept 30)"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        # Import and run the comprehensive debug
        from comprehensive_data_flow_debug import run_comprehensive_debug
        
        results = run_comprehensive_debug()
        return jsonify(results)
        
    except Exception as e:
        logger.error(f"Error in comprehensive data flow: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/cache-consistency-analysis', methods=['GET'])
@login_required
def admin_cache_consistency_analysis():
    """Comprehensive analysis of all cache layers and their consistency"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from datetime import date, datetime, timedelta
        from models import (PortfolioSnapshot, UserPortfolioChartCache, LeaderboardCache, 
                          MarketData, User, Stock)
        import json
        
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'cache_layers': {},
            'data_sources': {},
            'inconsistencies': [],
            'recommendations': []
        }
        
        # LAYER 1: Raw Data Sources
        logger.info("Analyzing raw data sources...")
        
        # Portfolio Snapshots
        total_snapshots = PortfolioSnapshot.query.count()
        recent_snapshots = PortfolioSnapshot.query.filter(
            PortfolioSnapshot.date >= date.today() - timedelta(days=7)
        ).count()
        
        # Get snapshot date range and sample data
        earliest_snapshot = PortfolioSnapshot.query.order_by(PortfolioSnapshot.date).first()
        latest_snapshot = PortfolioSnapshot.query.order_by(PortfolioSnapshot.date.desc()).first()
        
        # Check for zero-value snapshots
        zero_snapshots = PortfolioSnapshot.query.filter_by(total_value=0).count()
        
        analysis['data_sources']['portfolio_snapshots'] = {
            'total_count': total_snapshots,
            'recent_count': recent_snapshots,
            'zero_value_count': zero_snapshots,
            'date_range': {
                'earliest': earliest_snapshot.date.isoformat() if earliest_snapshot else None,
                'latest': latest_snapshot.date.isoformat() if latest_snapshot else None
            }
        }
        
        # Market Data (S&P 500)
        sp500_data_count = MarketData.query.filter_by(ticker="SPY_SP500").count()
        recent_sp500 = MarketData.query.filter(
            MarketData.ticker == "SPY_SP500",
            MarketData.date >= date.today() - timedelta(days=7)
        ).count()
        
        # Check for problematic S&P 500 values (like the -89% issue)
        problematic_sp500 = MarketData.query.filter(
            MarketData.ticker == "SPY_SP500",
            MarketData.close_price < 1000  # S&P 500 should be > 1000
        ).count()
        
        analysis['data_sources']['sp500_data'] = {
            'total_count': sp500_data_count,
            'recent_count': recent_sp500,
            'problematic_values': problematic_sp500
        }
        
        # LAYER 2: Chart Caches
        logger.info("Analyzing chart caches...")
        
        chart_caches = UserPortfolioChartCache.query.all()
        chart_cache_analysis = {}
        
        for cache in chart_caches:
            user_id = cache.user_id
            period = cache.period
            
            if user_id not in chart_cache_analysis:
                chart_cache_analysis[user_id] = {}
            
            try:
                chart_data = json.loads(cache.chart_data)
                data_points = len(chart_data.get('chart_data', []))
                
                # Check for zero portfolio values in chart data
                portfolio_points = [point.get('portfolio', 0) for point in chart_data.get('chart_data', [])]
                zero_portfolio_points = sum(1 for p in portfolio_points if p == 0)
                
                chart_cache_analysis[user_id][period] = {
                    'generated_at': cache.generated_at.isoformat(),
                    'data_points': data_points,
                    'zero_portfolio_points': zero_portfolio_points,
                    'has_real_portfolio_data': any(point.get('portfolio', 0) != 0 for point in chart_data.get('chart_data', [])),
                    'has_sp500_data': any(point.get('sp500', 0) != 0 for point in chart_data.get('chart_data', [])),
                    'portfolio_return': chart_data.get('portfolio_return'),
                    'sp500_return': chart_data.get('sp500_return')
                }
            except Exception as e:
                chart_cache_analysis[user_id][period] = {
                    'error': str(e),
                    'generated_at': cache.generated_at.isoformat()
                }
        
        analysis['cache_layers']['user_portfolio_charts'] = {
            'total_entries': len(chart_caches),
            'users_cached': len(chart_cache_analysis),
            'by_user': chart_cache_analysis
        }
        
        # LAYER 3: Leaderboard Caches
        logger.info("Analyzing leaderboard caches...")
        
        leaderboard_caches = LeaderboardCache.query.all()
        leaderboard_analysis = {}
        
        for cache in leaderboard_caches:
            try:
                leaderboard_data = json.loads(cache.leaderboard_data)
                
                leaderboard_analysis[cache.period] = {
                    'generated_at': cache.generated_at.isoformat(),
                    'user_count': len(leaderboard_data),
                    'users_with_zero_performance': sum(1 for user in leaderboard_data if user.get('performance_percentage', 0) == 0),
                    'users_with_real_performance': sum(1 for user in leaderboard_data if user.get('performance_percentage', 0) != 0),
                    'sample_performances': [user.get('performance_percentage', 0) for user in leaderboard_data[:3]],
                    'sample_users': [user.get('username', 'unknown') for user in leaderboard_data[:3]]
                }
            except Exception as e:
                leaderboard_analysis[cache.period] = {
                    'error': str(e),
                    'generated_at': cache.generated_at.isoformat()
                }
        
        analysis['cache_layers']['leaderboards'] = leaderboard_analysis
        
        # INCONSISTENCY DETECTION
        logger.info("Detecting inconsistencies...")
        
        # Check for users with snapshots but no chart cache
        users_with_snapshots = set(PortfolioSnapshot.query.with_entities(PortfolioSnapshot.user_id).distinct().all())
        users_with_snapshots = {user_id[0] for user_id in users_with_snapshots}
        
        users_with_chart_cache = set(chart_cache_analysis.keys())
        
        missing_chart_cache = users_with_snapshots - users_with_chart_cache
        if missing_chart_cache:
            analysis['inconsistencies'].append({
                'type': 'missing_chart_cache',
                'description': f'{len(missing_chart_cache)} users have snapshots but no chart cache',
                'affected_users': list(missing_chart_cache)
            })
        
        # Check for excessive zero values
        if zero_snapshots > total_snapshots * 0.3:  # More than 30% zero snapshots
            analysis['inconsistencies'].append({
                'type': 'excessive_zero_snapshots',
                'description': f'{zero_snapshots}/{total_snapshots} portfolio snapshots have zero value',
                'zero_count': zero_snapshots,
                'total_count': total_snapshots
            })
        
        # Check for chart caches with mostly zero portfolio values
        for user_id, periods in chart_cache_analysis.items():
            for period, data in periods.items():
                if isinstance(data, dict) and data.get('data_points', 0) > 0:
                    zero_ratio = data.get('zero_portfolio_points', 0) / data.get('data_points', 1)
                    if zero_ratio > 0.5:  # More than 50% zero values
                        analysis['inconsistencies'].append({
                            'type': 'chart_cache_zero_values',
                            'description': f'User {user_id} {period} chart has {data.get("zero_portfolio_points", 0)}/{data.get("data_points", 0)} zero portfolio values',
                            'user_id': user_id,
                            'period': period,
                            'zero_ratio': round(zero_ratio, 2)
                        })
        
        # Check leaderboard issues
        for period, lb_data in leaderboard_analysis.items():
            if isinstance(lb_data, dict) and 'users_with_zero_performance' in lb_data:
                zero_count = lb_data['users_with_zero_performance']
                total_count = lb_data['user_count']
                
                if zero_count > total_count * 0.5:  # More than 50% have zero performance
                    analysis['inconsistencies'].append({
                        'type': 'leaderboard_zero_performance',
                        'description': f'{period} leaderboard has {zero_count}/{total_count} users with 0% performance',
                        'period': period,
                        'zero_count': zero_count,
                        'total_count': total_count
                    })
                
                if total_count == 1:  # Only one user in leaderboard
                    analysis['inconsistencies'].append({
                        'type': 'single_user_leaderboard',
                        'description': f'{period} leaderboard only has 1 user',
                        'period': period
                    })
        
        # Check S&P 500 data issues
        if problematic_sp500 > 0:
            analysis['inconsistencies'].append({
                'type': 'problematic_sp500_data',
                'description': f'{problematic_sp500} S&P 500 data points have unrealistic values (< 1000)',
                'count': problematic_sp500
            })
        
        # RECOMMENDATIONS
        if analysis['inconsistencies']:
            analysis['recommendations'].extend([
                'Run comprehensive cache rebuild with data validation',
                'Fix zero-value portfolio snapshots',
                'Verify S&P 500 data conversion (SPY  10)',
                'Ensure consistent data sources across all cache layers',
                'Check portfolio creation dates vs snapshot dates'
            ])
        
        return jsonify(analysis)
        
    except Exception as e:
        logger.error(f"Error in cache consistency analysis: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/emergency-cache-rebuild', methods=['GET', 'POST'])
@login_required
def admin_emergency_cache_rebuild():
    """EMERGENCY: Force rebuild all caches with direct data population"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        if request.method == 'GET':
            return '''
            <!DOCTYPE html>
            <html>
            <head>
                <title> EMERGENCY CACHE REBUILD</title>
                <style>
                    body { font-family: Arial, sans-serif; max-width: 1000px; margin: 20px auto; padding: 20px; }
                    button { background: #dc3545; color: white; padding: 15px 30px; border: none; border-radius: 4px; font-size: 18px; cursor: pointer; margin: 10px; }
                    .critical { background: #f8d7da; padding: 20px; border-radius: 4px; margin: 20px 0; border-left: 6px solid #dc3545; }
                    #results { margin-top: 20px; padding: 20px; border-radius: 4px; }
                    .success { background: #d4edda; border-left: 6px solid #28a745; }
                    .error { background: #f8d7da; border-left: 6px solid #dc3545; }
                    .progress { background: #cce7ff; border-left: 4px solid #007bff; }
                </style>
            </head>
            <body>
                <h1> EMERGENCY CACHE REBUILD</h1>
                
                <div class="critical">
                    <h2> CRITICAL CACHE SYSTEM FAILURE DETECTED</h2>
                    <p><strong>ALL chart caches have 0 data points</strong></p>
                    <p><strong>ALL leaderboards show 0% performance</strong></p>
                    <p><strong>Chart cache generation process is completely broken</strong></p>
                </div>
                
                <div style="text-align: center; margin: 40px 0;">
                    <button onclick="startEmergencyRebuild()" id="startBtn">
                         START EMERGENCY REBUILD
                    </button>
                </div>
                
                <div id="results"></div>
                
                <script>
                async function startEmergencyRebuild() {
                    document.getElementById('startBtn').disabled = true;
                    document.getElementById('startBtn').textContent = ' REBUILDING...';
                    
                    document.getElementById('results').innerHTML = `
                        <div class="progress">
                            <h3> Emergency Cache Rebuild In Progress...</h3>
                            <p><strong>Bypassing broken cache generation...</strong></p>
                            <p><em>This will take 5-8 minutes.</em></p>
                        </div>
                    `;
                    
                    try {
                        const response = await fetch('/admin/emergency-cache-rebuild', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' }
                        });
                        
                        const data = await response.json();
                        
                        if (data.success) {
                            document.getElementById('results').className = 'success';
                            document.getElementById('results').innerHTML = `
                                <h2> EMERGENCY REBUILD COMPLETE!</h2>
                                <h3> Results:</h3>
                                <ul>
                                    <li><strong>Chart Caches Fixed:</strong> ${data.results.chart_caches_fixed}</li>
                                    <li><strong>Leaderboards Fixed:</strong> ${data.results.leaderboards_fixed}</li>
                                    <li><strong>Data Points Generated:</strong> ${data.results.data_points_generated}</li>
                                    <li><strong>Processing Time:</strong> ${data.results.processing_time} minutes</li>
                                </ul>
                                <h3> Dashboard Should Work Now!</h3>
                                <p><strong> Go check your dashboard and leaderboards!</strong></p>
                            `;
                        } else {
                            document.getElementById('results').className = 'error';
                            document.getElementById('results').innerHTML = `
                                <h3> Emergency Rebuild Failed</h3>
                                <p><strong>Error:</strong> ${data.error}</p>
                            `;
                        }
                    } catch (error) {
                        document.getElementById('results').className = 'error';
                        document.getElementById('results').innerHTML = `
                            <h3> Emergency Rebuild Failed</h3>
                            <p><strong>Error:</strong> ${error.message}</p>
                        `;
                    }
                    
                    document.getElementById('startBtn').disabled = false;
                    document.getElementById('startBtn').textContent = ' Run Again';
                }
                </script>
            </body>
            </html>
            '''
        
        # Handle POST request - perform emergency cache rebuild
        from datetime import date, datetime, timedelta
        from models import (PortfolioSnapshot, UserPortfolioChartCache, LeaderboardCache, 
                          MarketData, User, Stock)
        import json
        
        start_time = datetime.now()
        results = {
            'chart_caches_fixed': 0,
            'leaderboards_fixed': 0,
            'data_points_generated': 0,
            'errors': []
        }
        
        logger.info(" Starting EMERGENCY CACHE REBUILD...")
        
        # STEP 1: Clear ALL existing caches
        logger.info("Step 1: Clearing all existing caches...")
        UserPortfolioChartCache.query.delete()
        LeaderboardCache.query.delete()
        db.session.commit()
        
        # STEP 2: Get all users with portfolios
        users_with_stocks = db.session.query(User.id, User.username).join(Stock).distinct().all()
        
        # STEP 3: Manually rebuild chart caches using direct database queries
        logger.info("Step 2: Manually rebuilding chart caches...")
        
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y']
        
        for user_id, username in users_with_stocks:
            try:
                logger.info(f"Processing user {user_id} ({username})")
                
                # Get user's stocks
                user_stocks = Stock.query.filter_by(user_id=user_id).all()
                if not user_stocks:
                    continue
                
                # Find portfolio start date
                earliest_stock = min(user_stocks, key=lambda s: s.purchase_date)
                portfolio_start_date = earliest_stock.purchase_date.date()
                
                for period in periods:
                    try:
                        # Calculate date range for this period
                        end_date = date.today()
                        
                        if period == '1D':
                            start_date = end_date
                        elif period == '5D':
                            start_date = end_date - timedelta(days=7)  # Include weekends
                        elif period == '1M':
                            start_date = end_date - timedelta(days=30)
                        elif period == '3M':
                            start_date = end_date - timedelta(days=90)
                        elif period == 'YTD':
                            start_date = date(end_date.year, 1, 1)
                        elif period == '1Y':
                            start_date = end_date - timedelta(days=365)
                        
                        # Don't go before portfolio creation
                        start_date = max(start_date, portfolio_start_date)
                        
                        # Get portfolio snapshots for this period
                        snapshots = PortfolioSnapshot.query.filter(
                            PortfolioSnapshot.user_id == user_id,
                            PortfolioSnapshot.date >= start_date,
                            PortfolioSnapshot.date <= end_date
                        ).order_by(PortfolioSnapshot.date).all()
                        
                        # Get S&P 500 data for same period
                        sp500_data = MarketData.query.filter(
                            MarketData.ticker == "SPY_SP500",
                            MarketData.date >= start_date,
                            MarketData.date <= end_date
                        ).order_by(MarketData.date).all()
                        
                        # Create date-indexed dictionaries
                        snapshot_dict = {s.date: s.total_value for s in snapshots}
                        sp500_dict = {s.date: s.close_price for s in sp500_data}
                        
                        # Generate chart data points
                        chart_data_points = []
                        
                        # Get all dates in range (business days only for most periods)
                        current_date = start_date
                        while current_date <= end_date:
                            if period == '1D' or current_date.weekday() < 5:  # Include weekends only for 1D
                                portfolio_value = snapshot_dict.get(current_date, 0)
                                sp500_value = sp500_dict.get(current_date, 0)
                                
                                if portfolio_value > 0 or sp500_value > 0:  # Only include if we have some data
                                    chart_data_points.append({
                                        'date': current_date.isoformat(),
                                        'portfolio': portfolio_value,
                                        'sp500': sp500_value
                                    })
                            
                            current_date += timedelta(days=1)
                        
                        # Calculate performance if we have data
                        portfolio_return = 0
                        sp500_return = 0
                        
                        if len(chart_data_points) >= 2:
                            first_portfolio = next((p['portfolio'] for p in chart_data_points if p['portfolio'] > 0), 0)
                            last_portfolio = chart_data_points[-1]['portfolio']
                            
                            first_sp500 = next((p['sp500'] for p in chart_data_points if p['sp500'] > 0), 0)
                            last_sp500 = chart_data_points[-1]['sp500']
                            
                            if first_portfolio > 0:
                                portfolio_return = ((last_portfolio - first_portfolio) / first_portfolio) * 100
                            
                            if first_sp500 > 0:
                                sp500_return = ((last_sp500 - first_sp500) / first_sp500) * 100
                        
                        # Create chart cache entry
                        chart_cache_data = {
                            'chart_data': chart_data_points,
                            'portfolio_return': round(portfolio_return, 2),
                            'sp500_return': round(sp500_return, 2),
                            'period': period,
                            'start_date': start_date.isoformat(),
                            'end_date': end_date.isoformat()
                        }
                        
                        # Save to cache
                        cache_entry = UserPortfolioChartCache(
                            user_id=user_id,
                            period=period,
                            chart_data=json.dumps(chart_cache_data),
                            generated_at=datetime.now()
                        )
                        db.session.add(cache_entry)
                        
                        results['chart_caches_fixed'] += 1
                        results['data_points_generated'] += len(chart_data_points)
                        
                        logger.info(f"Created {period} cache for user {user_id}: {len(chart_data_points)} points, {portfolio_return:.2f}% return")
                        
                    except Exception as e:
                        error_msg = f"Error creating {period} cache for user {user_id}: {str(e)}"
                        results['errors'].append(error_msg)
                        logger.error(error_msg)
                
                # Commit after each user to avoid large transaction timeout
                try:
                    # Flush to database without committing transaction
                    db.session.flush()
                    logger.info(f"Flushed chart caches for user {user_id} to database")
                except Exception as flush_error:
                    logger.error(f"Failed to flush caches for user {user_id}: {flush_error}")
                    db.session.rollback()
                    error_msg = f"Flush failed for user {user_id}: {str(flush_error)}"
                    results['errors'].append(error_msg)
                
            except Exception as e:
                error_msg = f"Error processing user {user_id}: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(error_msg)
                db.session.rollback()
        
        # STEP 4: Final summary (commits already done per-user)
        logger.info("Step 3: Chart cache rebuild complete")
        logger.info("Note: Skipping leaderboard rebuild to avoid timeout - use /admin/regenerate-leaderboard-cache")
        
        # Count leaderboards that will be updated by regular cron
        results['leaderboards_fixed'] = 0  # Will be updated by regular leaderboard cron
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds() / 60
        results['processing_time'] = round(processing_time, 2)
        
        logger.info(f"Emergency cache rebuild completed in {processing_time:.2f} minutes")
        
        return jsonify({
            'success': True,
            'message': 'Emergency cache rebuild completed successfully',
            'results': results
        }), 200
        
    except Exception as e:
        logger.error(f"Error in emergency cache rebuild: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/api/test-chart-cache-version', methods=['GET'])
def test_chart_cache_version():
    """Test endpoint to verify deployment and manually test cache generation"""
    try:
        import inspect
        from leaderboard_utils import update_leaderboard_cache, generate_chart_from_snapshots
        
        # Check the source code of update_leaderboard_cache to see which version is deployed
        source_code = inspect.getsource(update_leaderboard_cache)
        
        # Look for the key indicator of the NEW code
        has_new_code = "Generate portfolio charts for ALL users" in source_code
        has_old_code = "Skipping 1D chart cache" in source_code
        uses_correct_function = "generate_chart_from_snapshots" in source_code
        uses_old_function = "generate_user_portfolio_chart" in source_code
        
        # Test cache generation for current user
        from flask_login import current_user
        test_result = {}
        if current_user.is_authenticated:
            try:
                cache_data = generate_chart_from_snapshots(current_user.id, '1D')
                if cache_data:
                    test_result['1D_cache_test'] = {
                        'success': True,
                        'labels_count': len(cache_data.get('labels', [])),
                        'sample_labels': cache_data.get('labels', [])[:3],
                        'datasets_count': len(cache_data.get('datasets', []))
                    }
                else:
                    test_result['1D_cache_test'] = {'success': False, 'error': 'No cache data generated'}
            except Exception as e:
                test_result['1D_cache_test'] = {'success': False, 'error': str(e)}
        
        return jsonify({
            'deployment_status': {
                'has_new_code': has_new_code,
                'has_old_code': has_old_code,
                'uses_correct_function': uses_correct_function,
                'uses_old_function': uses_old_function,
                'verdict': 'NEW CODE DEPLOYED ' if (has_new_code and not has_old_code) else 'OLD CODE STILL RUNNING '
            },
            'test_result': test_result
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/test-leaderboard-badges', methods=['GET'])
@login_required
def test_leaderboard_badges():
    """Test endpoint to debug why leaderboard badges aren't showing"""
    try:
        from leaderboard_utils import get_user_leaderboard_positions, get_leaderboard_data
        from flask_login import current_user
        
        # Test the leaderboard position function
        positions = get_user_leaderboard_positions(current_user.id, top_n=20)
        
        # Get sample leaderboard data for debugging
        sample_leaderboards = {}
        for period in ['1D', '5D', '1M']:
            leaderboard = get_leaderboard_data(period, limit=20)
            sample_leaderboards[period] = {
                'total_entries': len(leaderboard),
                'user_in_list': any(e.get('user_id') == current_user.id for e in leaderboard),
                'top_3': [{'user_id': e.get('user_id'), 'username': e.get('username'), 'performance': e.get('performance_percent')} 
                         for e in leaderboard[:3]]
            }
        
        return jsonify({
            'user_id': current_user.id,
            'username': current_user.username,
            'leaderboard_positions': positions,
            'positions_count': len(positions),
            'sample_leaderboards': sample_leaderboards,
            'verdict': 'BADGES SHOULD SHOW' if len(positions) > 0 else 'NO POSITIONS FOUND - BADGES HIDDEN'
        })
    except Exception as e:
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/test-portfolio-stats', methods=['GET'])
@login_required
def admin_test_portfolio_stats():
    """
    Diagnostic endpoint to test portfolio stats calculation
    Shows sample data for verification before deploying to production
    """
    try:
        from models import User, UserPortfolioStats, Stock
        from leaderboard_utils import calculate_user_portfolio_stats, calculate_industry_mix
        
        results = {
            'sample_user': None,
            'all_users_summary': {},
            'data_quality_checks': {},
            'calculation_breakdown': {}
        }
        
        # Get current user for sample
        sample_user = User.query.get(current_user.id)
        if not sample_user:
            return jsonify({'error': 'No users found'}), 404
        
        # Calculate stats for sample user
        logger.info(f"Calculating stats for user {sample_user.id}")
        stats = calculate_user_portfolio_stats(sample_user.id)
        
        # Get breakdown for transparency
        stocks = Stock.query.filter_by(user_id=sample_user.id).all()
        stock_tickers = [s.ticker for s in stocks]
        
        results['sample_user'] = {
            'user_id': sample_user.id,
            'username': sample_user.username,
            'stats': {
                'unique_stocks_count': stats['unique_stocks_count'],
                'avg_trades_per_week': stats['avg_trades_per_week'],
                'total_trades': stats['total_trades'],
                'large_cap_percent': stats['large_cap_percent'],
                'small_cap_percent': stats['small_cap_percent'],
                'industry_mix': stats['industry_mix'],
                'subscriber_count': stats['subscriber_count'],
                'last_updated': stats['last_updated'].isoformat()
            },
            'calculation_breakdown': {
                'stock_tickers': stock_tickers,
                'stock_count': len(stock_tickers),
                'industry_breakdown': stats['industry_mix']
            }
        }
        
        # Calculate summary across all users
        all_users = User.query.all()
        all_stats = []
        for user in all_users:
            try:
                user_stats = calculate_user_portfolio_stats(user.id)
                all_stats.append(user_stats)
            except Exception as e:
                logger.warning(f"Could not calculate stats for user {user.id}: {str(e)}")
        
        if all_stats:
            results['all_users_summary'] = {
                'total_users_processed': len(all_stats),
                'avg_unique_stocks': round(sum(s['unique_stocks_count'] for s in all_stats) / len(all_stats), 1),
                'avg_trades_per_week': round(sum(s['avg_trades_per_week'] for s in all_stats) / len(all_stats), 1),
                'users_with_subscribers': sum(1 for s in all_stats if s['subscriber_count'] > 0),
                'total_subscribers': sum(s['subscriber_count'] for s in all_stats)
            }
        
        # Data quality checks
        results['data_quality_checks'] = {
            'all_users_have_calculable_stats': len(all_stats) == len(all_users),
            'sample_industry_mix_valid': len(stats.get('industry_mix', {})) > 0,
            'sample_percentages_reasonable': (
                0 <= stats.get('large_cap_percent', 0) <= 100 and
                0 <= stats.get('small_cap_percent', 0) <= 100
            ),
            'sample_has_stocks': stats.get('unique_stocks_count', 0) > 0
        }
        
        return jsonify(results)
        
    except Exception as e:
        logger.error(f"Error in test-portfolio-stats: {str(e)}")
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/populate-portfolio-stats', methods=['GET', 'POST'])
@login_required
def admin_populate_portfolio_stats():
    """
    One-time population of portfolio stats for all users
    Creates UserPortfolioStats entries for all users
    """
    try:
        from models import User, UserPortfolioStats, db
        from leaderboard_utils import calculate_user_portfolio_stats
        from datetime import datetime
        
        results = {
            'users_processed': 0,
            'users_created': 0,
            'users_updated': 0,
            'errors': []
        }
        
        # Get all users
        all_users = User.query.all()
        logger.info(f"Populating portfolio stats for {len(all_users)} users")
        
        for user in all_users:
            try:
                # Calculate stats
                stats = calculate_user_portfolio_stats(user.id)
                
                # Check if entry exists
                existing_stats = UserPortfolioStats.query.filter_by(user_id=user.id).first()
                
                if existing_stats:
                    # Update existing
                    existing_stats.unique_stocks_count = stats['unique_stocks_count']
                    existing_stats.avg_trades_per_week = stats['avg_trades_per_week']
                    existing_stats.total_trades = stats['total_trades']
                    existing_stats.large_cap_percent = stats['large_cap_percent']
                    existing_stats.small_cap_percent = stats['small_cap_percent']
                    existing_stats.industry_mix = stats['industry_mix']
                    existing_stats.subscriber_count = stats['subscriber_count']
                    existing_stats.last_updated = datetime.utcnow()
                    results['users_updated'] += 1
                    logger.info(f"Updated stats for user {user.id} ({user.username})")
                else:
                    # Create new
                    new_stats = UserPortfolioStats(
                        user_id=user.id,
                        unique_stocks_count=stats['unique_stocks_count'],
                        avg_trades_per_week=stats['avg_trades_per_week'],
                        total_trades=stats['total_trades'],
                        large_cap_percent=stats['large_cap_percent'],
                        small_cap_percent=stats['small_cap_percent'],
                        industry_mix=stats['industry_mix'],
                        subscriber_count=stats['subscriber_count'],
                        last_updated=datetime.utcnow()
                    )
                    db.session.add(new_stats)
                    results['users_created'] += 1
                    logger.info(f"Created stats for user {user.id} ({user.username})")
                
                results['users_processed'] += 1
                
            except Exception as e:
                error_msg = f"Error processing user {user.id}: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(error_msg)
        
        # Commit all changes
        db.session.commit()
        logger.info(f"Portfolio stats population complete: {results['users_processed']} users processed")
        
        return jsonify({
            'success': True,
            'message': 'Portfolio stats populated successfully',
            'results': results
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error in populate-portfolio-stats: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/replace-sp500-with-real-data', methods=['GET', 'POST'])
@login_required
def admin_replace_sp500_with_real_data():
    """Replace ALL S&P 500 historical data with REAL data from Alpha Vantage"""
    # Check if user is admin
    email = session.get('email', '')
    if email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    # GET: Show preview with button to execute
    if request.method == 'GET':
        from models import MarketData
        
        # Count existing records
        existing_count = MarketData.query.filter_by(ticker='SPY_SP500').count()
        
        # Get date range
        all_sp500 = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        earliest = all_sp500[0].date.isoformat() if all_sp500 else 'None'
        latest = all_sp500[-1].date.isoformat() if all_sp500 else 'None'
        
        # Count suspicious values
        low_values = MarketData.query.filter(
            MarketData.ticker == 'SPY_SP500',
            MarketData.close_price < 1000
        ).count()
        
        # Return HTML with button to execute
        return f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Replace S&P 500 Data</title>
            <style>
                body {{ font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }}
                .info-box {{ background: #f0f0f0; padding: 20px; border-radius: 8px; margin: 20px 0; }}
                .warning-box {{ background: #fff3cd; border: 2px solid #ffc107; padding: 20px; border-radius: 8px; margin: 20px 0; }}
                .success-box {{ background: #d4edda; border: 2px solid #28a745; padding: 20px; border-radius: 8px; margin: 20px 0; display: none; }}
                .error-box {{ background: #f8d7da; border: 2px solid #dc3545; padding: 20px; border-radius: 8px; margin: 20px 0; display: none; }}
                button {{ background: #007bff; color: white; border: none; padding: 15px 30px; font-size: 16px; border-radius: 5px; cursor: pointer; }}
                button:hover {{ background: #0056b3; }}
                button:disabled {{ background: #ccc; cursor: not-allowed; }}
                .stats {{ display: grid; grid-template-columns: 1fr 1fr; gap: 10px; }}
                .stat {{ background: white; padding: 10px; border-radius: 5px; }}
                .stat-label {{ font-weight: bold; color: #666; }}
                .stat-value {{ font-size: 24px; color: #333; }}
                #loading {{ display: none; }}
            </style>
        </head>
        <body>
            <h1> Replace S&P 500 Historical Data</h1>
            
            <div class="info-box">
                <h2>Current S&P 500 Data Status</h2>
                <div class="stats">
                    <div class="stat">
                        <div class="stat-label">Total Records</div>
                        <div class="stat-value">{existing_count}</div>
                    </div>
                    <div class="stat">
                        <div class="stat-label">Years of Data</div>
                        <div class="stat-value">{existing_count / 252:.1f}</div>
                    </div>
                    <div class="stat">
                        <div class="stat-label">Date Range</div>
                        <div class="stat-value">{earliest} to {latest}</div>
                    </div>
                    <div class="stat">
                        <div class="stat-label"> Suspicious Values</div>
                        <div class="stat-value" style="color: #dc3545;">{low_values}</div>
                    </div>
                </div>
            </div>
            
            <div class="warning-box">
                <h2> What This Will Do:</h2>
                <ul>
                    <li><strong>API Call:</strong> TIME_SERIES_DAILY for SPY (outputsize=full)</li>
                    <li><strong>API Cost:</strong> 1 API call to Alpha Vantage</li>
                    <li><strong>Data Fetched:</strong> ~5,000+ days of historical SPY data</li>
                    <li><strong>Action:</strong> Replace ALL existing S&P 500 records with real market data</li>
                    <li><strong>Formula:</strong> SPY close price  10 = S&P 500 index</li>
                    <li><strong>Impact:</strong> Fixes YTD, 1Y, 5Y, MAX charts across entire app</li>
                </ul>
                <p><strong>This will take ~10-15 seconds to complete.</strong></p>
            </div>
            
            <div style="text-align: center; margin: 30px 0;">
                <button id="executeBtn" onclick="executeReplacement()">
                     Execute S&P 500 Data Replacement
                </button>
                <div id="loading">
                    <p> Fetching data from Alpha Vantage and replacing records...</p>
                    <p>This may take 10-15 seconds...</p>
                </div>
            </div>
            
            <div id="successBox" class="success-box"></div>
            <div id="errorBox" class="error-box"></div>
            
            <script>
                function executeReplacement() {{
                    const btn = document.getElementById('executeBtn');
                    const loading = document.getElementById('loading');
                    const successBox = document.getElementById('successBox');
                    const errorBox = document.getElementById('errorBox');
                    
                    btn.disabled = true;
                    loading.style.display = 'block';
                    successBox.style.display = 'none';
                    errorBox.style.display = 'none';
                    
                    fetch('/admin/replace-sp500-with-real-data', {{
                        method: 'POST',
                        headers: {{
                            'Content-Type': 'application/json'
                        }}
                    }})
                    .then(response => response.json())
                    .then(data => {{
                        loading.style.display = 'none';
                        
                        if (data.success) {{
                            successBox.innerHTML = `
                                <h2> Success!</h2>
                                <p><strong>Replaced:</strong> ${{data.replaced_count}} records</p>
                                <p><strong>Created:</strong> ${{data.new_count}} new records</p>
                                <p><strong>Total Processed:</strong> ${{data.total_processed}} records</p>
                                <p><strong>Date Range:</strong> ${{data.date_range.earliest}} to ${{data.date_range.latest}}</p>
                                <p><strong>Years of Data:</strong> ${{data.date_range.years_of_data.toFixed(1)}} years</p>
                                <hr>
                                <p><strong>Next Step:</strong> Regenerate chart caches to see the fixed data</p>
                                <p><a href="/admin/trigger-chart-cache-generation" style="color: #007bff;">Click here to regenerate chart caches</a></p>
                            `;
                            successBox.style.display = 'block';
                        }} else {{
                            errorBox.innerHTML = `
                                <h2> Error</h2>
                                <p>${{data.error}}</p>
                                <pre>${{data.traceback || ''}}</pre>
                            `;
                            errorBox.style.display = 'block';
                            btn.disabled = false;
                        }}
                    }})
                    .catch(error => {{
                        loading.style.display = 'none';
                        errorBox.innerHTML = `
                            <h2> Error</h2>
                            <p>${{error.message}}</p>
                        `;
                        errorBox.style.display = 'block';
                        btn.disabled = false;
                    }});
                }}
            </script>
        </body>
        </html>
        """
    
    # POST: Execute the replacement
    try:
        
        from models import MarketData
        from portfolio_performance import PortfolioPerformanceCalculator
        import requests
        from datetime import date, timedelta
        
        # Replace ALL historical data (no date limit)
        # The API gives us 20+ years of data, use all of it!
        today = date.today()
        
        logger.info(f" Replacing ALL S&P 500 historical data with REAL Alpha Vantage data")
        
        # Fetch TIME_SERIES_DAILY for SPY
        api_key = os.environ.get('ALPHA_VANTAGE_API_KEY')
        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=SPY&apikey={api_key}&outputsize=full'
        
        response = requests.get(url, timeout=30)
        data = response.json()
        
        # Log what Alpha Vantage actually returned
        logger.info(f" Alpha Vantage Response Keys: {list(data.keys())}")
        if 'Note' in data:
            logger.error(f" Alpha Vantage Rate Limit: {data['Note']}")
        if 'Error Message' in data:
            logger.error(f" Alpha Vantage Error: {data['Error Message']}")
        
        if 'Time Series (Daily)' not in data:
            logger.error(f" Alpha Vantage returned error: {data}")
            return jsonify({
                'success': False,
                'error': 'Alpha Vantage API error - check logs for details',
                'response': data,
                'possible_causes': [
                    'Rate limit exceeded (5 calls/min for free tier, 150/min for premium)',
                    'Invalid API key',
                    'Network timeout',
                    'Invalid symbol'
                ]
            }), 500
        
        # Log how many dates we got
        time_series = data['Time Series (Daily)']
        logger.info(f" Alpha Vantage returned {len(time_series)} dates of SPY data")
        replaced_count = 0
        new_count = 0
        results = []
        
        for date_str, values in time_series.items():
            data_date = datetime.strptime(date_str, '%Y-%m-%d').date()
            
            # Replace all historical data (no date filtering)
            # This ensures 5Y and MAX charts have accurate data
            spy_close = float(values['4. close'])
            sp500_value = spy_close * 10  # SPY ETF  10 = S&P 500 index approximation
            
            # Find existing record
            existing = MarketData.query.filter_by(
                ticker='SPY_SP500',
                date=data_date
            ).first()
            
            if existing:
                old_value = float(existing.close_price)
                existing.close_price = sp500_value
                replaced_count += 1
                
                if abs(old_value - sp500_value) > 50:  # Significant difference
                    results.append({
                        'date': date_str,
                        'old_value': old_value,
                        'new_value': sp500_value,
                        'difference': sp500_value - old_value,
                        'action': 'replaced'
                    })
            else:
                market_data = MarketData(
                    ticker='SPY_SP500',
                    date=data_date,
                    close_price=sp500_value
                )
                db.session.add(market_data)
                new_count += 1
                results.append({
                    'date': date_str,
                    'value': sp500_value,
                    'action': 'created'
                })
        
        db.session.commit()
        
        # Find actual date range processed
        all_sp500 = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        earliest_date = all_sp500[0].date.isoformat() if all_sp500 else None
        latest_date = all_sp500[-1].date.isoformat() if all_sp500 else None
        
        return jsonify({
            'success': True,
            'replaced_count': replaced_count,
            'new_count': new_count,
            'total_processed': replaced_count + new_count,
            'date_range': {
                'earliest': earliest_date,
                'latest': latest_date,
                'years_of_data': len(all_sp500) / 252 if all_sp500 else 0  # ~252 trading days/year
            },
            'significant_changes': results[:20],  # Show first 20 significant changes
            'message': f'Replaced {replaced_count} records and created {new_count} new records with REAL Alpha Vantage data (ALL historical data)'
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error replacing S&P 500 data: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/fix-sp500-data', methods=['POST'])
@login_required
def admin_fix_sp500_data():
    """Fix S&P 500 data points that are too low (missing  10 multiplier)"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import MarketData
        
        # Find all S&P 500 data points that are suspiciously low
        # SPY price is ~$600, so S&P 500 should be ~$6000
        # Any value < $1000 is likely missing the  10 multiplier
        
        low_values = MarketData.query.filter(
            MarketData.ticker == 'SPY_SP500',
            MarketData.close_price < 1000
        ).all()
        
        fixed_count = 0
        results = []
        
        for data_point in low_values:
            old_value = float(data_point.close_price)
            new_value = old_value * 10
            
            results.append({
                'date': data_point.date.isoformat(),
                'old_value': old_value,
                'new_value': new_value,
                'multiplier_applied': 10
            })
            
            data_point.close_price = new_value
            fixed_count += 1
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'fixed_count': fixed_count,
            'fixed_records': results,
            'message': f'Fixed {fixed_count} S&P 500 records by applying  10 multiplier'
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error fixing S&P 500 data: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/diagnose-chart-sp500-mismatch', methods=['GET'])
@login_required
def admin_diagnose_chart_sp500_mismatch():
    """Diagnose EXACTLY what S&P 500 data each source has for specific dates"""
    try:
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import UserPortfolioChartCache, MarketData, SP500ChartCache, PortfolioSnapshot
        from datetime import datetime
        import json
        
        # Test dates that show different patterns
        test_dates = ['2025-09-11', '2025-09-18', '2025-10-23', '2025-10-24']
        periods = ['1M', '3M', 'YTD', '1Y']
        
        # Get a user's chart caches
        user_id = request.args.get('user_id', 1)
        
        results = {
            'source_1_market_data': {},
            'source_2_sp500_chart_cache': {},
            'source_3_user_chart_cache': {},
            'source_4_portfolio_snapshots': {},
            'mismatches': [],
            'data_flow_analysis': {}
        }
        
        # SOURCE 1: MarketData table (raw SPY_SP500 daily closes)
        logger.info("Checking SOURCE 1: MarketData table")
        for date_str in test_dates:
            date_obj = datetime.strptime(date_str, '%Y-%m-%d').date()
            sp500_record = MarketData.query.filter_by(
                ticker='SPY_SP500',
                date=date_obj
            ).first()
            
            results['source_1_market_data'][date_str] = {
                'value': float(sp500_record.close_price) if sp500_record else None,
                'exists': sp500_record is not None,
                'record_id': sp500_record.id if sp500_record else None
            }
        
        # SOURCE 2: SP500ChartCache (pre-generated S&P 500 charts)
        logger.info("Checking SOURCE 2: SP500ChartCache")
        for period in periods:
            sp500_cache = SP500ChartCache.query.filter_by(period=period).first()
            
            if not sp500_cache:
                if period not in results['source_2_sp500_chart_cache']:
                    results['source_2_sp500_chart_cache'][period] = {}
                results['source_2_sp500_chart_cache'][period]['error'] = 'No cache found'
                continue
            
            cache_data = json.loads(sp500_cache.chart_data)
            dates = cache_data.get('dates', [])
            values = cache_data.get('values', [])
            
            if period not in results['source_2_sp500_chart_cache']:
                results['source_2_sp500_chart_cache'][period] = {}
            
            results['source_2_sp500_chart_cache'][period]['total_points'] = len(dates)
            results['source_2_sp500_chart_cache'][period]['date_range'] = f"{dates[0]} to {dates[-1]}" if dates else "Empty"
            results['source_2_sp500_chart_cache'][period]['last_updated'] = sp500_cache.last_updated.isoformat() if sp500_cache.last_updated else None
            results['source_2_sp500_chart_cache'][period]['test_dates'] = {}
            
            for test_date in test_dates:
                if test_date in dates:
                    idx = dates.index(test_date)
                    cache_value = values[idx] if idx < len(values) else None
                    results['source_2_sp500_chart_cache'][period]['test_dates'][test_date] = {
                        'value': cache_value,
                        'in_cache': True
                    }
                else:
                    results['source_2_sp500_chart_cache'][period]['test_dates'][test_date] = {
                        'value': None,
                        'in_cache': False
                    }
        
        # SOURCE 3: UserPortfolioChartCache (actual displayed charts)
        logger.info("Checking SOURCE 3: UserPortfolioChartCache")
        for period in periods:
            cache = UserPortfolioChartCache.query.filter_by(
                user_id=user_id,
                period=period
            ).first()
            
            if not cache:
                if period not in results['source_3_user_chart_cache']:
                    results['source_3_user_chart_cache'][period] = {}
                results['source_3_user_chart_cache'][period]['error'] = 'No cache found'
                continue
            
            chart_data = json.loads(cache.chart_data)
            sp500_data = chart_data.get('sp500_performance', {})
            
            if period not in results['source_3_user_chart_cache']:
                results['source_3_user_chart_cache'][period] = {}
            
            if not sp500_data:
                results['source_3_user_chart_cache'][period]['error'] = 'No sp500_performance in cache'
                continue
            
            dates = sp500_data.get('dates', [])
            values = sp500_data.get('values', [])
            
            results['source_3_user_chart_cache'][period]['total_points'] = len(dates)
            results['source_3_user_chart_cache'][period]['date_range'] = f"{dates[0]} to {dates[-1]}" if dates else "Empty"
            results['source_3_user_chart_cache'][period]['last_updated'] = cache.last_updated.isoformat() if cache.last_updated else None
            results['source_3_user_chart_cache'][period]['test_dates'] = {}
            
            # Find our test dates in this chart
            for test_date in test_dates:
                if test_date in dates:
                    idx = dates.index(test_date)
                    chart_value = values[idx] if idx < len(values) else None
                    results['source_3_user_chart_cache'][period]['test_dates'][test_date] = {
                        'value': chart_value,
                        'in_cache': True
                    }
                else:
                    results['source_3_user_chart_cache'][period]['test_dates'][test_date] = {
                        'value': None,
                        'in_cache': False
                    }
        
        # SOURCE 4: PortfolioSnapshot (daily snapshots - might have embedded S&P 500 data)
        logger.info("Checking SOURCE 4: PortfolioSnapshot")
        for date_str in test_dates:
            date_obj = datetime.strptime(date_str, '%Y-%m-%d').date()
            snapshot = PortfolioSnapshot.query.filter_by(
                user_id=user_id,
                date=date_obj
            ).first()
            
            results['source_4_portfolio_snapshots'][date_str] = {
                'exists': snapshot is not None,
                'total_value': float(snapshot.total_value) if snapshot else None,
                'snapshot_id': snapshot.id if snapshot else None
            }
        
        # CROSS-SOURCE ANALYSIS: Compare all sources for each date
        logger.info("Performing cross-source analysis")
        for test_date in test_dates:
            results['data_flow_analysis'][test_date] = {
                'market_data': results['source_1_market_data'][test_date]['value'],
                'sp500_chart_cache': {},
                'user_chart_cache': {},
                'all_sources_match': None,
                'discrepancies': []
            }
            
            # Get values from SP500ChartCache for this date across all periods
            for period in periods:
                if period in results['source_2_sp500_chart_cache']:
                    period_data = results['source_2_sp500_chart_cache'][period]
                    if 'test_dates' in period_data and test_date in period_data['test_dates']:
                        results['data_flow_analysis'][test_date]['sp500_chart_cache'][period] = period_data['test_dates'][test_date]['value']
            
            # Get values from UserPortfolioChartCache for this date across all periods
            for period in periods:
                if period in results['source_3_user_chart_cache']:
                    period_data = results['source_3_user_chart_cache'][period]
                    if 'test_dates' in period_data and test_date in period_data['test_dates']:
                        results['data_flow_analysis'][test_date]['user_chart_cache'][period] = period_data['test_dates'][test_date]['value']
            
            # Check for discrepancies
            market_data_value = results['data_flow_analysis'][test_date]['market_data']
            
            # Check SP500ChartCache vs MarketData
            for period, cache_value in results['data_flow_analysis'][test_date]['sp500_chart_cache'].items():
                if cache_value and market_data_value and abs(cache_value - market_data_value) > 1:
                    results['data_flow_analysis'][test_date]['discrepancies'].append({
                        'source': f'SP500ChartCache[{period}]',
                        'value': cache_value,
                        'expected': market_data_value,
                        'difference': abs(cache_value - market_data_value)
                    })
            
            # Check UserChartCache vs MarketData
            for period, cache_value in results['data_flow_analysis'][test_date]['user_chart_cache'].items():
                if cache_value and market_data_value and abs(cache_value - market_data_value) > 1:
                    results['data_flow_analysis'][test_date]['discrepancies'].append({
                        'source': f'UserChartCache[{period}]',
                        'value': cache_value,
                        'expected': market_data_value,
                        'difference': abs(cache_value - market_data_value)
                    })
            
            # Check if user chart values differ across periods for SAME date
            user_cache_values = list(results['data_flow_analysis'][test_date]['user_chart_cache'].values())
            if user_cache_values:
                unique_user_values = set([v for v in user_cache_values if v is not None])
                if len(unique_user_values) > 1:
                    results['data_flow_analysis'][test_date]['discrepancies'].append({
                        'source': 'UserChartCache cross-period inconsistency',
                        'issue': f'Same date shows different S&P 500 values across periods: {list(unique_user_values)}',
                        'periods': {period: val for period, val in results['data_flow_analysis'][test_date]['user_chart_cache'].items() if val is not None}
                    })
        
        # Calculate summary statistics
        total_discrepancies = sum(len(data['discrepancies']) for data in results['data_flow_analysis'].values())
        dates_with_issues = [date for date, data in results['data_flow_analysis'].items() if data['discrepancies']]
        
        return jsonify({
            'success': True,
            'user_id': user_id,
            'test_dates': test_dates,
            'source_1_market_data': results['source_1_market_data'],
            'source_2_sp500_chart_cache': results['source_2_sp500_chart_cache'],
            'source_3_user_chart_cache': results['source_3_user_chart_cache'],
            'source_4_portfolio_snapshots': results['source_4_portfolio_snapshots'],
            'data_flow_analysis': results['data_flow_analysis'],
            'summary': {
                'total_discrepancies': total_discrepancies,
                'dates_with_issues': dates_with_issues,
                'sources_checked': 4,
                'smoking_gun': 'Check data_flow_analysis for each date to see where values diverge'
            }
        })
        
    except Exception as e:
        logger.error(f"Error diagnosing chart S&P 500 mismatch: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/inspect-chart-cache-raw', methods=['GET'])
@login_required
def admin_inspect_chart_cache_raw():
    """Inspect the RAW chart cache data to see exactly what's stored"""
    try:
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import UserPortfolioChartCache
        import json
        
        user_id = request.args.get('user_id', 5)
        period = request.args.get('period', 'YTD')
        
        cache = UserPortfolioChartCache.query.filter_by(
            user_id=user_id,
            period=period
        ).first()
        
        if not cache:
            return jsonify({
                'success': False,
                'error': f'No cache found for user_id={user_id}, period={period}'
            })
        
        # Parse the chart_data JSON
        chart_data = json.loads(cache.chart_data)
        
        # Analyze structure
        analysis = {
            'cache_exists': True,
            'user_id': user_id,
            'period': period,
            'generated_at': cache.generated_at.isoformat() if cache.generated_at else None,
            'chart_data_keys': list(chart_data.keys()),
            'has_sp500_performance': 'sp500_performance' in chart_data,
            'datasets_count': len(chart_data.get('datasets', [])),
            'labels_count': len(chart_data.get('labels', [])),
            'raw_chart_data': chart_data  # Full data for inspection
        }
        
        # If sp500_performance exists, show its structure
        if 'sp500_performance' in chart_data:
            sp500 = chart_data['sp500_performance']
            analysis['sp500_structure'] = {
                'type': type(sp500).__name__,
                'keys': list(sp500.keys()) if isinstance(sp500, dict) else None,
                'dates_count': len(sp500.get('dates', [])) if isinstance(sp500, dict) else None,
                'values_count': len(sp500.get('values', [])) if isinstance(sp500, dict) else None,
                'first_3_dates': sp500.get('dates', [])[:3] if isinstance(sp500, dict) else None,
                'first_3_values': sp500.get('values', [])[:3] if isinstance(sp500, dict) else None
            }
        
        return jsonify({
            'success': True,
            'analysis': analysis
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/verify-batch-api-optimization', methods=['GET'])
@login_required
def admin_verify_batch_api_optimization():
    """Verify that batch API optimization is working correctly"""
    try:
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import User, Stock
        from portfolio_performance import PortfolioPerformanceCalculator
        import time
        
        # Get all unique tickers across all users
        users = User.query.all()
        unique_tickers = set()
        unique_tickers.add('SPY')
        
        for user in users:
            user_stocks = Stock.query.filter_by(user_id=user.id).all()
            for stock in user_stocks:
                if stock.quantity > 0:
                    unique_tickers.add(stock.ticker.upper())
        
        ticker_list = list(unique_tickers)
        
        # Test 1: Batch API call (what market close uses)
        calculator = PortfolioPerformanceCalculator()
        
        logger.info("Testing BATCH API call...")
        batch_start = time.time()
        batch_results = calculator.get_batch_stock_data(ticker_list)
        batch_duration = time.time() - batch_start
        
        batch_success_count = len(batch_results)
        api_calls_made = 1 if len(ticker_list) <= 75 else 2  # Batch size is 75
        
        # Test 2: Individual API calls (old method - for comparison)
        logger.info("Testing INDIVIDUAL API calls (for comparison)...")
        individual_start = time.time()
        individual_success = 0
        individual_api_calls = 0
        
        for ticker in ticker_list[:5]:  # Only test 5 to avoid rate limits
            try:
                result = calculator.get_stock_data(ticker)
                if result and result.get('price'):
                    individual_success += 1
                individual_api_calls += 1
            except:
                pass
        
        individual_duration = time.time() - individual_start
        
        # Extrapolate individual results to full ticker list
        estimated_individual_time = (individual_duration / 5) * len(ticker_list)
        estimated_individual_calls = len(ticker_list)
        
        return jsonify({
            'success': True,
            'timestamp': datetime.now().isoformat(),
            'ticker_count': len(ticker_list),
            'tickers': ticker_list,
            'batch_api': {
                'success_count': batch_success_count,
                'api_calls_made': api_calls_made,
                'duration_seconds': round(batch_duration, 2),
                'tickers_per_call': round(len(ticker_list) / api_calls_made, 1)
            },
            'individual_api_comparison': {
                'tested_count': 5,
                'success_count': individual_success,
                'duration_seconds': round(individual_duration, 2),
                'estimated_full_duration': round(estimated_individual_time, 2),
                'estimated_api_calls': estimated_individual_calls
            },
            'efficiency_gains': {
                'time_saved_percent': round((1 - batch_duration / estimated_individual_time) * 100, 1),
                'api_calls_saved': estimated_individual_calls - api_calls_made,
                'speedup_factor': round(estimated_individual_time / batch_duration, 1)
            },
            'verdict': 'BATCH OPTIMIZATION WORKING' if batch_success_count >= len(ticker_list) * 0.9 else 'ISSUES DETECTED'
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/find-user-id', methods=['GET'])
@login_required
def admin_find_user_id():
    """Find user_id by username or email"""
    try:
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import User
        
        username = request.args.get('username', '')
        search_email = request.args.get('email', '')
        
        if username:
            user = User.query.filter_by(username=username).first()
            if user:
                return jsonify({
                    'success': True,
                    'user_id': user.id,
                    'username': user.username,
                    'email': user.email
                })
        
        if search_email:
            user = User.query.filter_by(email=search_email).first()
            if user:
                return jsonify({
                    'success': True,
                    'user_id': user.id,
                    'username': user.username,
                    'email': user.email
                })
        
        # List all users
        all_users = User.query.all()
        return jsonify({
            'success': True,
            'users': [{
                'user_id': u.id,
                'username': u.username,
                'email': u.email
            } for u in all_users]
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/admin/check-sp500-duplicates', methods=['GET'])
@login_required
def admin_check_sp500_duplicates():
    """Check if there are duplicate SPY_SP500 records for the same date"""
    try:
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import MarketData
        from sqlalchemy import func
        
        # Find dates with multiple records
        duplicates = db.session.query(
            MarketData.date,
            func.count(MarketData.id).label('count'),
            func.group_concat(MarketData.close_price).label('values')
        ).filter(
            MarketData.ticker == 'SPY_SP500'
        ).group_by(
            MarketData.date
        ).having(
            func.count(MarketData.id) > 1
        ).all()
        
        duplicate_list = [{
            'date': dup.date.isoformat(),
            'count': dup.count,
            'values': str(dup.values)
        } for dup in duplicates]
        
        # Also check total record count
        total_count = MarketData.query.filter_by(ticker='SPY_SP500').count()
        unique_dates = db.session.query(func.count(func.distinct(MarketData.date))).filter(
            MarketData.ticker == 'SPY_SP500'
        ).scalar()
        
        return jsonify({
            'success': True,
            'total_records': total_count,
            'unique_dates': unique_dates,
            'duplicate_dates_count': len(duplicates),
            'duplicates': duplicate_list[:50],  # Show first 50
            'message': f'Total: {total_count}, Unique dates: {unique_dates}. If these differ, we have duplicates!'
        })
        
    except Exception as e:
        logger.error(f"Error checking duplicates: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/delete-all-sp500-data', methods=['GET', 'POST'])
@login_required
def admin_delete_all_sp500_data():
    """DELETE ALL S&P 500 data and start completely fresh"""
    email = session.get('email', '')
    if email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData, db
        
        # Count total records
        total_count = MarketData.query.filter_by(ticker='SPY_SP500').count()
        
        # GET: Show warning page
        if request.method == 'GET':
            return f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title> DELETE ALL S&P 500 Data</title>
                <style>
                    body {{ font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }}
                    .danger-box {{ background: #f8d7da; border: 3px solid #dc3545; padding: 30px; border-radius: 8px; margin: 20px 0; }}
                    button {{ background: #dc3545; color: white; border: none; padding: 20px 40px; font-size: 18px; border-radius: 5px; cursor: pointer; margin: 20px 0; }}
                    button:hover {{ background: #c82333; }}
                    .success {{ background: #d4edda; border: 2px solid #28a745; padding: 20px; border-radius: 8px; display: none; }}
                    h1 {{ color: #dc3545; }}
                </style>
            </head>
            <body>
                <h1> NUCLEAR OPTION: Delete ALL S&P 500 Data</h1>
                
                <div class="danger-box">
                    <h2>THIS WILL DELETE {total_count} RECORDS</h2>
                    <p><strong>This action:</strong></p>
                    <ul>
                        <li> Deletes ALL {total_count} SPY_SP500 records from MarketData</li>
                        <li> Removes ALL historical S&P 500 data (1999-2025)</li>
                        <li> Cannot be undone</li>
                        <li> Provides a clean slate for fresh Alpha Vantage fetch</li>
                    </ul>
                    <p><strong>After deletion:</strong></p>
                    <ol>
                        <li>Run /admin/replace-sp500-with-real-data to fetch fresh data</li>
                        <li>Regenerate chart caches</li>
                    </ol>
                    <p style="color: #dc3545; font-size: 20px; font-weight: bold;"> ONLY USE THIS IF THE DATA IS COMPLETELY CORRUPTED </p>
                </div>
                
                <button onclick="deleteAll()"> YES, DELETE ALL {total_count} S&P 500 RECORDS</button>
                <div id="success" class="success"></div>
                
                <script>
                    function deleteAll() {{
                        if (!confirm(' FINAL WARNING: Delete ALL {total_count} S&P 500 records? This CANNOT be undone!')) {{
                            return;
                        }}
                        
                        fetch('/admin/delete-all-sp500-data', {{
                            method: 'POST'
                        }})
                        .then(response => response.json())
                        .then(data => {{
                            if (data.success) {{
                                document.getElementById('success').innerHTML = `
                                    <h2> Deleted ${{data.deleted_count}} records</h2>
                                    <p>Database is now clean. Next steps:</p>
                                    <ol>
                                        <li><a href="/admin/replace-sp500-with-real-data">Fetch fresh S&P 500 data from Alpha Vantage</a></li>
                                        <li>Regenerate chart caches</li>
                                    </ol>
                                `;
                                document.getElementById('success').style.display = 'block';
                            }}
                        }});
                    }}
                </script>
            </body>
            </html>
            """
        
        # POST: Actually delete
        if request.method == 'POST':
            MarketData.query.filter_by(ticker='SPY_SP500').delete()
            db.session.commit()
            logger.info(f" DELETED ALL {total_count} SPY_SP500 records from MarketData")
            
            return jsonify({
                'success': True,
                'deleted_count': total_count,
                'message': f'Deleted ALL {total_count} S&P 500 records. Database is clean.'
            })
    
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error deleting S&P 500 data: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/find-corrupted-sp500-dates', methods=['GET', 'POST'])
@login_required
def admin_find_corrupted_sp500_dates():
    """Find and optionally DELETE corrupted S&P 500 values"""
    email = session.get('email', '')
    if email != ADMIN_EMAIL:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        from models import MarketData, db
        from datetime import date
        
        # Find all S&P 500 records with suspicious low values
        corrupted = MarketData.query.filter(
            MarketData.ticker == 'SPY_SP500',
            MarketData.close_price < 1000  # Way too low for S&P 500
        ).order_by(MarketData.date.desc()).all()
        
        corrupted_list = [{
            'date': record.date.isoformat(),
            'value': float(record.close_price),
            'id': record.id
        } for record in corrupted]
        
        # GET: Just show the corrupted records
        if request.method == 'GET':
            return f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>Corrupted S&P 500 Records</title>
                <style>
                    body {{ font-family: Arial, sans-serif; max-width: 1000px; margin: 50px auto; padding: 20px; }}
                    .warning-box {{ background: #fff3cd; border: 2px solid #ffc107; padding: 20px; border-radius: 8px; margin: 20px 0; }}
                    table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                    th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                    th {{ background: #f0f0f0; }}
                    button {{ background: #dc3545; color: white; border: none; padding: 15px 30px; font-size: 16px; border-radius: 5px; cursor: pointer; margin: 10px 0; }}
                    button:hover {{ background: #c82333; }}
                    .success {{ background: #d4edda; border: 2px solid #28a745; padding: 20px; border-radius: 8px; display: none; }}
                </style>
            </head>
            <body>
                <h1> Corrupted S&P 500 Records</h1>
                
                <div class="warning-box">
                    <h2>Found {len(corrupted)} corrupted records with values < 1000</h2>
                    <p>These records have S&P 500 values that are impossibly low (should be 5000-7000 for recent years).</p>
                    <p><strong>Solution:</strong> Delete these records, then run the replacement again to fetch correct data.</p>
                </div>
                
                <table>
                    <thead>
                        <tr>
                            <th>Date</th>
                            <th>Corrupted Value</th>
                            <th>Expected Range</th>
                        </tr>
                    </thead>
                    <tbody>
                        {''.join([f'<tr><td>{r["date"]}</td><td style="color: red;"><strong>{r["value"]}</strong></td><td>5000-7000</td></tr>' for r in corrupted_list[:20]])}
                    </tbody>
                </table>
                
                <button onclick="deleteCorrupted()"> DELETE All {len(corrupted)} Corrupted Records</button>
                <div id="success" class="success"></div>
                
                <script>
                    function deleteCorrupted() {{
                        if (!confirm('Delete {len(corrupted)} corrupted records? You will need to re-run the S&P 500 replacement after this.')) {{
                            return;
                        }}
                        
                        fetch('/admin/find-corrupted-sp500-dates', {{
                            method: 'POST'
                        }})
                        .then(response => response.json())
                        .then(data => {{
                            if (data.success) {{
                                document.getElementById('success').innerHTML = `
                                    <h2> Deleted ${{data.deleted_count}} records</h2>
                                    <p>Now run the S&P 500 replacement again to fetch correct data for these dates.</p>
                                    <p><a href="/admin/replace-sp500-with-real-data">Click here to run replacement</a></p>
                                `;
                                document.getElementById('success').style.display = 'block';
                            }}
                        }});
                    }}
                </script>
            </body>
            </html>
            """
        
        # POST: Delete the corrupted records
        if request.method == 'POST':
            deleted_count = 0
            for record in corrupted:
                db.session.delete(record)
                deleted_count += 1
            
            db.session.commit()
            logger.info(f"Deleted {deleted_count} corrupted S&P 500 records")
            
            return jsonify({
                'success': True,
                'deleted_count': deleted_count,
                'message': f'Deleted {deleted_count} corrupted records. Run replacement again to fetch correct data.'
            })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error finding/deleting corrupted S&P 500 dates: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-sp500-data', methods=['GET'])
@login_required
def admin_check_sp500_data():
    """Diagnose S&P 500 historical data quality"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import MarketData
        from datetime import date, timedelta
        
        today = date.today()
        
        # Get S&P 500 data for different periods
        periods = {
            '1M': today - timedelta(days=30),
            '3M': today - timedelta(days=90),
            'YTD': date(today.year, 1, 1),
            '1Y': today - timedelta(days=365),
        }
        
        results = {}
        
        for period_name, start_date in periods.items():
            data_points = MarketData.query.filter(
                MarketData.ticker == 'SPY_SP500',
                MarketData.date >= start_date,
                MarketData.date <= today
            ).order_by(MarketData.date.asc()).all()
            
            if data_points:
                values = [float(d.close_price) for d in data_points]
                dates = [d.date.isoformat() for d in data_points]
                
                results[period_name] = {
                    'count': len(data_points),
                    'first_date': dates[0] if dates else None,
                    'last_date': dates[-1] if dates else None,
                    'first_value': values[0] if values else None,
                    'last_value': values[-1] if values else None,
                    'min_value': min(values) if values else None,
                    'max_value': max(values) if values else None,
                    'avg_value': sum(values) / len(values) if values else None,
                    'sample_data': [
                        {'date': d.date.isoformat(), 'value': float(d.close_price)}
                        for d in data_points[:5]  # First 5
                    ] + [
                        {'date': d.date.isoformat(), 'value': float(d.close_price)}
                        for d in data_points[-5:]  # Last 5
                    ],
                    'issues': []
                }
                
                # Detect issues
                if min(values) < 100:
                    results[period_name]['issues'].append(f"Suspiciously low value: ${min(values):.2f}")
                if max(values) > 10000:
                    results[period_name]['issues'].append(f"Suspiciously high value: ${max(values):.2f}")
                if max(values) - min(values) < 10:
                    results[period_name]['issues'].append(f"Almost no variation: ${max(values) - min(values):.2f} range")
                
                # Check for duplicates
                if len(set(values)) < len(values) * 0.8:
                    results[period_name]['issues'].append("Many duplicate values detected")
            else:
                results[period_name] = {
                    'count': 0,
                    'error': f'No S&P 500 data found for {period_name}'
                }
        
        # Check total data availability
        all_data = MarketData.query.filter_by(ticker='SPY_SP500').order_by(MarketData.date).all()
        
        return jsonify({
            'success': True,
            'total_sp500_records': len(all_data),
            'date_range': {
                'earliest': all_data[0].date.isoformat() if all_data else None,
                'latest': all_data[-1].date.isoformat() if all_data else None
            },
            'periods': results,
            'diagnosis': {
                'expected_sp500_range': '6000-7500 (Oct 2025)',
                'spy_price_range': '600-750 (SPY  10 = S&P 500)',
                'data_quality_check': 'Look for values outside expected range or duplicates'
            }
        })
        
    except Exception as e:
        logger.error(f"Error checking S&P 500 data: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/test-batch-api', methods=['GET'])
@login_required
def admin_test_batch_api():
    """Test batch API to verify it works correctly"""
    try:
        # Check if user is admin
        email = session.get('email', '')
        if email != ADMIN_EMAIL:
            return jsonify({'error': 'Admin access required'}), 403
        
        from models import Stock, User
        from portfolio_performance import PortfolioPerformanceCalculator
        import json
        
        # Collect real tickers from database
        unique_tickers = set(['SPY', 'AAPL', 'MSFT', 'GOOGL'])  # Start with common ones
        
        all_stocks = Stock.query.limit(20).all()
        for stock in all_stocks:
            if stock.quantity > 0:
                unique_tickers.add(stock.ticker.upper())
        
        ticker_list = list(unique_tickers)[:10]  # Limit to 10 for testing
        
        logger.info(f"Testing batch API with tickers: {ticker_list}")
        
        # Test batch call
        calculator = PortfolioPerformanceCalculator()
        batch_results = calculator.get_batch_stock_data(ticker_list)
        
        # Also test individual calls for comparison
        individual_results = {}
        for ticker in ticker_list[:3]:  # Only test 3 individually
            result = calculator.get_stock_data(ticker)
            if result and 'price' in result:
                individual_results[ticker] = result['price']
        
        return jsonify({
            'success': True,
            'test_tickers': ticker_list,
            'batch_results': {
                'count': len(batch_results),
                'prices': {ticker: price for ticker, price in batch_results.items()},
                'api_calls_made': 1 if len(ticker_list) <= 256 else (len(ticker_list) // 256) + 1
            },
            'individual_results': {
                'count': len(individual_results),
                'prices': individual_results,
                'api_calls_made': len(individual_results)
            },
            'comparison': {
                'batch_vs_individual': {
                    ticker: {
                        'batch': batch_results.get(ticker),
                        'individual': individual_results.get(ticker),
                        'match': batch_results.get(ticker) == individual_results.get(ticker)
                    }
                    for ticker in individual_results.keys()
                },
                'efficiency_gain': f"{len(individual_results)}x fewer API calls" if len(batch_results) > 0 else "N/A"
            },
            'message': f'Batch API test complete: {len(batch_results)}/{len(ticker_list)} tickers fetched successfully'
        })
        
    except Exception as e:
        logger.error(f"Error testing batch API: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/trigger-chart-cache-generation', methods=['GET', 'POST'])
@login_required
def admin_trigger_chart_cache_generation():
    """
    Manually trigger chart cache generation for all users
    This bypasses the market close cron to test chart generation directly
    """
    try:
        from leaderboard_utils import generate_chart_from_snapshots
        from models import User, UserPortfolioChartCache, db
        import json
        from datetime import datetime
        
        results = {
            'users_processed': 0,
            'charts_generated': 0,
            'errors': []
        }
        
        users = User.query.all()
        periods = ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']
        
        for user in users:
            for period in periods:
                try:
                    logger.info(f"Generating chart cache for user {user.id} ({user.username}), period {period}")
                    chart_data = generate_chart_from_snapshots(user.id, period)
                    
                    if chart_data:
                        # Update or create chart cache entry
                        chart_cache = UserPortfolioChartCache.query.filter_by(
                            user_id=user.id, period=period
                        ).first()
                        
                        if chart_cache:
                            chart_cache.chart_data = json.dumps(chart_data)
                            chart_cache.generated_at = datetime.now()
                            logger.info(f"Updated chart cache for user {user.id}, period {period}")
                        else:
                            chart_cache = UserPortfolioChartCache(
                                user_id=user.id,
                                period=period,
                                chart_data=json.dumps(chart_data),
                                generated_at=datetime.now()
                            )
                            db.session.add(chart_cache)
                            logger.info(f"Created chart cache for user {user.id}, period {period}")
                        
                        results['charts_generated'] += 1
                    else:
                        error_msg = f"No chart data generated for user {user.id}, period {period}"
                        results['errors'].append(error_msg)
                        logger.warning(error_msg)
                        
                except Exception as e:
                    error_msg = f"Error generating chart for user {user.id}, period {period}: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(error_msg)
                    import traceback
                    logger.error(traceback.format_exc())
            
            results['users_processed'] += 1
        
        # Commit all chart caches
        db.session.commit()
        logger.info(f"Chart cache generation complete: {results['charts_generated']} charts for {results['users_processed']} users")
        
        return jsonify({
            'success': True,
            'message': f"Generated {results['charts_generated']} chart caches for {results['users_processed']} users",
            'results': results
        })
        
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error in trigger-chart-cache-generation: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/check-chart-caches', methods=['GET'])
@login_required
def admin_check_chart_caches():
    """
    Check if chart caches exist and when they were generated
    Diagnostic to see if market close cron generated the caches
    """
    try:
        from models import UserPortfolioChartCache, User
        import json
        
        results = {}
        users = User.query.all()
        
        for user in users:
            user_caches = {}
            for period in ['1D', '5D', '1M', '3M', 'YTD', '1Y', '5Y', 'MAX']:
                cache = UserPortfolioChartCache.query.filter_by(user_id=user.id, period=period).first()
                if cache:
                    chart_data = json.loads(cache.chart_data) if cache.chart_data else {}
                    labels = chart_data.get('labels', [])
                    user_caches[period] = {
                        'exists': True,
                        'generated_at': cache.generated_at.isoformat() if cache.generated_at else None,
                        'labels_count': len(labels),
                        'sample_labels': labels[:3] if labels else [],
                        'last_label': labels[-1] if labels else None
                    }
                else:
                    user_caches[period] = {'exists': False}
            
            results[user.username] = user_caches
        
        return jsonify({
            'success': True,
            'chart_caches': results
        })
        
    except Exception as e:
        logger.error(f"Error checking chart caches: {str(e)}")
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/view-all-portfolio-stats', methods=['GET'])
@login_required
def admin_view_all_portfolio_stats():
    """
    View portfolio stats for all users (for verification)
    Shows a summary table of all user stats
    """
    try:
        from models import User, UserPortfolioStats
        
        all_stats = []
        users = User.query.all()
        
        for user in users:
            stats = UserPortfolioStats.query.filter_by(user_id=user.id).first()
            if stats:
                all_stats.append({
                    'user_id': user.id,
                    'username': user.username,
                    'unique_stocks': stats.unique_stocks_count,
                    'avg_trades_per_week': stats.avg_trades_per_week,
                    'total_trades': stats.total_trades,
                    'large_cap_percent': stats.large_cap_percent,
                    'small_cap_percent': stats.small_cap_percent,
                    'industry_mix': stats.industry_mix,
                    'subscriber_count': stats.subscriber_count,
                    'last_updated': stats.last_updated.isoformat() if stats.last_updated else None
                })
            else:
                all_stats.append({
                    'user_id': user.id,
                    'username': user.username,
                    'status': 'NO STATS ENTRY'
                })
        
        return jsonify({
            'success': True,
            'total_users': len(users),
            'users_with_stats': sum(1 for s in all_stats if 'status' not in s),
            'all_stats': all_stats
        })
        
    except Exception as e:
        logger.error(f"Error viewing all portfolio stats: {str(e)}")
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/admin/create-portfolio-stats-table', methods=['GET', 'POST'])
@login_required
def admin_create_portfolio_stats_table():
    """
    Create the user_portfolio_stats table in production database
    Migration endpoint - run once before populating data
    """
    try:
        from models import db, UserPortfolioStats
        from sqlalchemy import inspect
        
        # Check if table already exists
        inspector = inspect(db.engine)
        existing_tables = inspector.get_table_names()
        
        if 'user_portfolio_stats' in existing_tables:
            return jsonify({
                'success': True,
                'message': 'Table already exists',
                'table_name': 'user_portfolio_stats'
            })
        
        # Create the table
        UserPortfolioStats.__table__.create(db.engine)
        logger.info("Created user_portfolio_stats table")
        
        return jsonify({
            'success': True,
            'message': 'Table created successfully',
            'table_name': 'user_portfolio_stats'
        })
        
    except Exception as e:
        logger.error(f"Error creating portfolio stats table: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

# Export the Flask app for Vercel serverless function
# This is required for Vercel's Python runtime
app.debug = False
